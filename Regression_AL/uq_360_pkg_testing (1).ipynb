{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use this cell as the first notebook cell to add src/ folder to the PATH and enable import of the package.\n",
    "# import sys,os\n",
    "# sys.path.append(os.path.abspath(\"../src\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(os.path.abspath(\"../src\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-1.11.0-cp38-cp38-manylinux1_x86_64.whl (750.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 750.6 MB 15 kB/s  eta 0:00:013\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading torchvision-0.12.0-cp38-cp38-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 21.0 MB 3.2 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting torchaudio\n",
      "  Downloading torchaudio-0.11.0-cp38-cp38-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 55.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (9.1.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.22.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.0.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (1.26.7)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "Successfully installed torch-1.11.0 torchaudio-0.11.0 torchvision-0.12.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install torch torchvision torchaudio \n",
    "#--extra-index-url\n",
    "#https://download.pytorch.org/whl/cpu\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting uq360\n",
      "  Downloading uq360-0.2-py3-none-any.whl (172 kB)\n",
      "\u001b[K     |████████████████████████████████| 172 kB 680 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy<1.21,>=1.16.5\n",
      "  Downloading numpy-1.20.3-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.4 MB 387 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting gpytorch>=1.3.0\n",
      "  Downloading gpytorch-1.6.0.tar.gz (310 kB)\n",
      "\u001b[K     |████████████████████████████████| 310 kB 61.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting autograd>=1.3\n",
      "  Downloading autograd-1.4-py3-none-any.whl (48 kB)\n",
      "\u001b[K     |████████████████████████████████| 48 kB 7.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting umap-learn==0.3.10\n",
      "  Downloading umap-learn-0.3.10.tar.gz (40 kB)\n",
      "\u001b[K     |████████████████████████████████| 40 kB 16.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gitpython==2.1.15\n",
      "  Downloading GitPython-2.1.15-py2.py3-none-any.whl (452 kB)\n",
      "\u001b[K     |████████████████████████████████| 452 kB 50.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-hub\n",
      "  Downloading tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n",
      "\u001b[K     |████████████████████████████████| 108 kB 453 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from uq360) (1.11.0)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.8/dist-packages (from uq360) (1.4.2)\n",
      "Collecting botorch>=0.3.2\n",
      "  Downloading botorch-0.6.4-py3-none-any.whl (363 kB)\n",
      "\u001b[K     |████████████████████████████████| 363 kB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm==4.42.0\n",
      "  Downloading tqdm-4.42.0-py2.py3-none-any.whl (59 kB)\n",
      "\u001b[K     |████████████████████████████████| 59 kB 17.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (from uq360) (2.6.0+nv)\n",
      "Collecting scikit-learn<1.0,>=0.22\n",
      "  Downloading scikit_learn-0.24.2-cp38-cp38-manylinux2010_x86_64.whl (24.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.9 MB 537 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from uq360) (1.8.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from uq360) (2.27.1)\n",
      "Requirement already satisfied: matplotlib>=3.2 in /usr/local/lib/python3.8/dist-packages (from uq360) (3.5.2)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from uq360) (0.8.9)\n",
      "Collecting gitdb2<3,>=2\n",
      "  Downloading gitdb2-2.0.6-py2.py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 7.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numba>=0.37 in /usr/local/lib/python3.8/dist-packages (from umap-learn==0.3.10->uq360) (0.55.1)\n",
      "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.8/dist-packages (from autograd>=1.3->uq360) (0.18.2)\n",
      "Collecting pyro-ppl==1.8.0\n",
      "  Downloading pyro_ppl-1.8.0-py3-none-any.whl (713 kB)\n",
      "\u001b[K     |████████████████████████████████| 713 kB 541 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multipledispatch\n",
      "  Downloading multipledispatch-0.6.0-py3-none-any.whl (11 kB)\n",
      "Collecting pyro-api>=0.1.1\n",
      "  Downloading pyro_api-0.1.2-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from pyro-ppl==1.8.0->botorch>=0.3.2->uq360) (3.3.0)\n",
      "Collecting smmap2>=2.0.0\n",
      "  Downloading smmap2-3.0.1-py3-none-any.whl (1.1 kB)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2->uq360) (9.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2->uq360) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2->uq360) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2->uq360) (21.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2->uq360) (2.4.7)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2->uq360) (4.33.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2->uq360) (1.4.2)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /usr/local/lib/python3.8/dist-packages (from numba>=0.37->umap-learn==0.3.10->uq360) (0.38.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba>=0.37->umap-learn==0.3.10->uq360) (58.1.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.24.0->uq360) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2->uq360) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn<1.0,>=0.22->uq360) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn<1.0,>=0.22->uq360) (1.1.0)\n",
      "Requirement already satisfied: smmap>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from smmap2>=2.0.0->gitdb2<3,>=2->gitpython==2.1.15->uq360) (5.0.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->uq360) (3.7.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->uq360) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->uq360) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->uq360) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from requests->uq360) (2.0.6)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow->uq360) (1.1.2)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->uq360) (1.12)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow->uq360) (2.6.0)\n",
      "Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->uq360) (5.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->uq360) (1.12.1)\n",
      "Requirement already satisfied: absl-py==0.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->uq360) (0.12.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.8/dist-packages (from tensorflow->uq360) (0.37.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->uq360) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow->uq360) (3.17.3)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow->uq360) (1.6.3)\n",
      "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->uq360) (0.4.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow->uq360) (0.2.0)\n",
      "Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow->uq360) (2.6.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->uq360) (3.1.0)\n",
      "Collecting numpy<1.21,>=1.16.5\n",
      "  Downloading numpy-1.19.5-cp38-cp38-manylinux2010_x86_64.whl (14.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.9 MB 2.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow->uq360) (2.6.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->uq360) (1.39.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow->uq360) (1.35.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow->uq360) (2.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow->uq360) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow->uq360) (1.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow->uq360) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow->uq360) (0.6.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow->uq360) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow->uq360) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow->uq360) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow->uq360) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow->uq360) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow->uq360) (3.1.1)\n",
      "Building wheels for collected packages: umap-learn, gpytorch\n",
      "  Building wheel for umap-learn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for umap-learn: filename=umap_learn-0.3.10-py3-none-any.whl size=38883 sha256=f1770f1de8b2f01876e4e8990790f621f7800e4a8091af79caa55185d02c24e6\n",
      "  Stored in directory: /root/.cache/pip/wheels/d8/5b/bb/addda229a9f418417d287055c1daf4acf4c6b074e2d38dfcfa\n",
      "  Building wheel for gpytorch (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gpytorch: filename=gpytorch-1.6.0-py2.py3-none-any.whl size=509894 sha256=9949cdd85c8a44c6bd937323b0be1bdb05bcada7816e84139a35be8add4533ce\n",
      "  Stored in directory: /root/.cache/pip/wheels/fb/fe/ac/09e58c64743aa0b4244554d81558cd6119a3d1bdee5ddf600b\n",
      "Successfully built umap-learn gpytorch\n",
      "Installing collected packages: numpy, tqdm, smmap2, pyro-api, scikit-learn, pyro-ppl, multipledispatch, gpytorch, gitdb2, umap-learn, tensorflow-hub, gitpython, botorch, autograd, uq360\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.22.3\n",
      "    Uninstalling numpy-1.22.3:\n",
      "      Successfully uninstalled numpy-1.22.3\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.64.0\n",
      "    Uninstalling tqdm-4.64.0:\n",
      "      Successfully uninstalled tqdm-4.64.0\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.0.2\n",
      "    Uninstalling scikit-learn-1.0.2:\n",
      "      Successfully uninstalled scikit-learn-1.0.2\n",
      "  Attempting uninstall: umap-learn\n",
      "    Found existing installation: umap-learn 0.5.3\n",
      "    Uninstalling umap-learn-0.5.3:\n",
      "      Successfully uninstalled umap-learn-0.5.3\n",
      "  Attempting uninstall: gitpython\n",
      "    Found existing installation: GitPython 3.1.27\n",
      "    Uninstalling GitPython-3.1.27:\n",
      "      Successfully uninstalled GitPython-3.1.27\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ailab-active-learning 0.3.5 requires numpy~=1.22, but you have numpy 1.19.5 which is incompatible.\u001b[0m\n",
      "Successfully installed autograd-1.4 botorch-0.6.4 gitdb2-2.0.6 gitpython-2.1.15 gpytorch-1.6.0 multipledispatch-0.6.0 numpy-1.19.5 pyro-api-0.1.2 pyro-ppl-1.8.0 scikit-learn-0.24.2 smmap2-3.0.1 tensorflow-hub-0.12.0 tqdm-4.42.0 umap-learn-0.3.10 uq360-0.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install uq360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ailab_active_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ailab_active_learning.uq_regression import ActiveLearningRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "alr = ActiveLearningRegression()\n",
    "data = alr.read_data('/data/MGP/TestPoints_100k_NEW.xlsx',\n",
    "                     'ResFeasible')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Tin [K]    Pin [kPa]      N [rpm]  Total Consumed power   Pout [kPA]  \\\n",
      "0  322.017877  1867.320507  4220.190859            298.334466  2584.822359   \n",
      "1  300.789449  2261.871734  2041.082876             60.837886  2475.185517   \n",
      "\n",
      "   Qin [m3/s]  \n",
      "0    0.099404  \n",
      "1    0.049342  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/ailab_active_learning/uq_regression.py:115: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Diff_Pressure'] = df['Pout [kPA]'] - df['Pin [kPa]']\n"
     ]
    }
   ],
   "source": [
    "data_sel = alr.select_features_from_columns(data)\n",
    "print(data_sel.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_samples = 15000\n",
    "data_scaled, y_labels, x_data = alr.data_scaling(data_sel, no_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ailab_active_learning.Regressor import Data, Regressor, ALRegressor, BaselineRegressor, RegressionExperiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_al = Data()\n",
    "data_al.set_data(x_data=x_data, y_data=y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_hr, hr_kwargs, config_al = alr.config(data_al)\n",
    "alregegre = ALRegressor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.6900098398327827\n",
      "Epoch: 1, loss = 0.6297798231244087\n",
      "Epoch: 2, loss = 0.5763267278671265\n",
      "Epoch: 3, loss = 0.5264978818595409\n",
      "Epoch: 4, loss = 0.4778805822134018\n",
      "Epoch: 5, loss = 0.4291093535721302\n",
      "Epoch: 6, loss = 0.3792320266366005\n",
      "Epoch: 7, loss = 0.32721710205078125\n",
      "Epoch: 8, loss = 0.2720448412001133\n",
      "Epoch: 9, loss = 0.21332143992185593\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.7205189391970634\n",
      "Epoch: 1, loss = 0.6760700717568398\n",
      "Epoch: 2, loss = 0.6336546912789345\n",
      "Epoch: 3, loss = 0.5917950719594955\n",
      "Epoch: 4, loss = 0.5492900609970093\n",
      "Epoch: 5, loss = 0.5050941184163094\n",
      "Epoch: 6, loss = 0.45842183381319046\n",
      "Epoch: 7, loss = 0.40870705991983414\n",
      "Epoch: 8, loss = 0.3554152473807335\n",
      "Epoch: 9, loss = 0.29809724912047386\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.7463112026453018\n",
      "Epoch: 1, loss = 0.675407201051712\n",
      "Epoch: 2, loss = 0.6157076880335808\n",
      "Epoch: 3, loss = 0.5642448961734772\n",
      "Epoch: 4, loss = 0.5155215859413147\n",
      "Epoch: 5, loss = 0.46570125967264175\n",
      "Epoch: 6, loss = 0.4142416976392269\n",
      "Epoch: 7, loss = 0.3610517792403698\n",
      "Epoch: 8, loss = 0.3053370863199234\n",
      "Epoch: 9, loss = 0.24665250442922115\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.7292361035943031\n",
      "Epoch: 1, loss = 0.6915154829621315\n",
      "Epoch: 2, loss = 0.654953770339489\n",
      "Epoch: 3, loss = 0.618928574025631\n",
      "Epoch: 4, loss = 0.582860141992569\n",
      "Epoch: 5, loss = 0.5458435639739037\n",
      "Epoch: 6, loss = 0.5071188136935234\n",
      "Epoch: 7, loss = 0.46588171273469925\n",
      "Epoch: 8, loss = 0.42155594751238823\n",
      "Epoch: 9, loss = 0.3737437166273594\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.7772487103939056\n",
      "Epoch: 1, loss = 0.7295982614159584\n",
      "Epoch: 2, loss = 0.6868643462657928\n",
      "Epoch: 3, loss = 0.6464198604226112\n",
      "Epoch: 4, loss = 0.606136254966259\n",
      "Epoch: 5, loss = 0.5650766491889954\n",
      "Epoch: 6, loss = 0.5226215682923794\n",
      "Epoch: 7, loss = 0.47810717672109604\n",
      "Epoch: 8, loss = 0.4310101792216301\n",
      "Epoch: 9, loss = 0.38091396540403366\n",
      "\n",
      "T = 0\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.1852701488468382\n",
      "Epoch: 1, loss = 0.12588748108181688\n",
      "Epoch: 2, loss = 0.06496601055065791\n",
      "Epoch: 3, loss = 0.0010701728363831861\n",
      "Epoch: 4, loss = -0.066200473656257\n",
      "Epoch: 5, loss = -0.13733586337831286\n",
      "Epoch: 6, loss = -0.212424303094546\n",
      "Epoch: 7, loss = -0.2916414861877759\n",
      "Epoch: 8, loss = -0.37480975190798443\n",
      "Epoch: 9, loss = -0.4620493039902713\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.2454201297627555\n",
      "Epoch: 1, loss = 0.18616814414660135\n",
      "Epoch: 2, loss = 0.12426932983928259\n",
      "Epoch: 3, loss = 0.05892786766505903\n",
      "Epoch: 4, loss = -0.010717721862925425\n",
      "Epoch: 5, loss = -0.08485390504615174\n",
      "Epoch: 6, loss = -0.16401789337396622\n",
      "Epoch: 7, loss = -0.24807096355491215\n",
      "Epoch: 8, loss = -0.3373888201183743\n",
      "Epoch: 9, loss = -0.4318321016099718\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.19601448045836553\n",
      "Epoch: 1, loss = 0.13399470349152884\n",
      "Epoch: 2, loss = 0.07043421982477109\n",
      "Epoch: 3, loss = 0.0037598376058869884\n",
      "Epoch: 4, loss = -0.06664550842510329\n",
      "Epoch: 5, loss = -0.14076717218591106\n",
      "Epoch: 6, loss = -0.21917472862535048\n",
      "Epoch: 7, loss = -0.30186283091704047\n",
      "Epoch: 8, loss = -0.3888208170731862\n",
      "Epoch: 9, loss = -0.48007389240794707\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.33917766478326583\n",
      "Epoch: 1, loss = 0.28905846344100106\n",
      "Epoch: 2, loss = 0.23693925970130494\n",
      "Epoch: 3, loss = 0.18137459291352165\n",
      "Epoch: 4, loss = 0.12171336056457627\n",
      "Epoch: 5, loss = 0.05740513714651267\n",
      "Epoch: 6, loss = -0.012019469092289604\n",
      "Epoch: 7, loss = -0.08676880598068237\n",
      "Epoch: 8, loss = -0.1671576045660509\n",
      "Epoch: 9, loss = -0.2530163899064064\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.34587621357705856\n",
      "Epoch: 1, loss = 0.29300882253381944\n",
      "Epoch: 2, loss = 0.23874305023087397\n",
      "Epoch: 3, loss = 0.1816360420650906\n",
      "Epoch: 4, loss = 0.12130063399672508\n",
      "Epoch: 5, loss = 0.05711018501056565\n",
      "Epoch: 6, loss = -0.011333244956201978\n",
      "Epoch: 7, loss = -0.08447490715318255\n",
      "Epoch: 8, loss = -0.16242965062459308\n",
      "Epoch: 9, loss = -0.24563428159389233\n",
      "\n",
      "T = 1\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = -0.49691003561019903\n",
      "Epoch: 1, loss = -0.5835204809904099\n",
      "Epoch: 2, loss = -0.661616362631321\n",
      "Epoch: 3, loss = -0.7470057129859925\n",
      "Epoch: 4, loss = -0.8378781348466874\n",
      "Epoch: 5, loss = -0.9173175871372222\n",
      "Epoch: 6, loss = -0.9513097822666169\n",
      "Epoch: 7, loss = -1.0295397162437439\n",
      "Epoch: 8, loss = -1.123544251918793\n",
      "Epoch: 9, loss = -1.2010964393615722\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = -0.49333220422267915\n",
      "Epoch: 1, loss = -0.5871796786785126\n",
      "Epoch: 2, loss = -0.6820146918296814\n",
      "Epoch: 3, loss = -0.781034404039383\n",
      "Epoch: 4, loss = -0.8822559595108033\n",
      "Epoch: 5, loss = -0.9848939061164856\n",
      "Epoch: 6, loss = -1.083253824710846\n",
      "Epoch: 7, loss = -1.1700492024421691\n",
      "Epoch: 8, loss = -1.2552579641342163\n",
      "Epoch: 9, loss = -1.3621961414813994\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = -0.536680780351162\n",
      "Epoch: 1, loss = -0.6214021801948547\n",
      "Epoch: 2, loss = -0.7140159428119659\n",
      "Epoch: 3, loss = -0.8047060251235962\n",
      "Epoch: 4, loss = -0.9002724587917329\n",
      "Epoch: 5, loss = -0.9908555448055268\n",
      "Epoch: 6, loss = -1.0342052817344667\n",
      "Epoch: 7, loss = -1.1411398112773894\n",
      "Epoch: 8, loss = -1.1868960261344912\n",
      "Epoch: 9, loss = -1.2655427336692808\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = -0.3047400897368789\n",
      "Epoch: 1, loss = -0.3901851624250412\n",
      "Epoch: 2, loss = -0.47323204576969147\n",
      "Epoch: 3, loss = -0.5616536080837249\n",
      "Epoch: 4, loss = -0.6424338042736053\n",
      "Epoch: 5, loss = -0.7272275060415269\n",
      "Epoch: 6, loss = -0.7869825243949891\n",
      "Epoch: 7, loss = -0.851037150621414\n",
      "Epoch: 8, loss = -0.8723839521408081\n",
      "Epoch: 9, loss = -0.9381417453289033\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = -0.29334552856162194\n",
      "Epoch: 1, loss = -0.3759297251701355\n",
      "Epoch: 2, loss = -0.46229486018419264\n",
      "Epoch: 3, loss = -0.5486639961600305\n",
      "Epoch: 4, loss = -0.6416098266839981\n",
      "Epoch: 5, loss = -0.7310040444135665\n",
      "Epoch: 6, loss = -0.8319953471422197\n",
      "Epoch: 7, loss = -0.9292315542697906\n",
      "Epoch: 8, loss = -1.0172035694122314\n",
      "Epoch: 9, loss = -1.1204913258552551\n",
      "\n",
      "T = 2\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = -1.1883710731159558\n",
      "Epoch: 1, loss = -1.253054597161033\n",
      "Epoch: 2, loss = -1.3338468995961275\n",
      "Epoch: 3, loss = -1.1951104781844397\n",
      "Epoch: 4, loss = -1.4061856053092263\n",
      "Epoch: 5, loss = -1.4284529360857878\n",
      "Epoch: 6, loss = -1.3849221576343884\n",
      "Epoch: 7, loss = -1.5151550390503623\n",
      "Epoch: 8, loss = -1.587811919775876\n",
      "Epoch: 9, loss = -1.6499918699264526\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = -1.36873614246195\n",
      "Epoch: 1, loss = -1.4447519562461162\n",
      "Epoch: 2, loss = -1.5122381828048013\n",
      "Epoch: 3, loss = -1.535512924194336\n",
      "Epoch: 4, loss = -1.6329231478951192\n",
      "Epoch: 5, loss = -1.6877346255562524\n",
      "Epoch: 6, loss = -1.7513675689697261\n",
      "Epoch: 7, loss = -1.807207172567194\n",
      "Epoch: 8, loss = -1.8031205697493118\n",
      "Epoch: 9, loss = -1.675846966830167\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = -1.3314373872496865\n",
      "Epoch: 1, loss = -1.2261051264676182\n",
      "Epoch: 2, loss = -1.370978664268147\n",
      "Epoch: 3, loss = -1.474051600152796\n",
      "Epoch: 4, loss = -1.5164485194466328\n",
      "Epoch: 5, loss = -1.5677925348281863\n",
      "Epoch: 6, loss = -1.5688332319259644\n",
      "Epoch: 7, loss = -1.693463672291149\n",
      "Epoch: 8, loss = -1.7274411916732788\n",
      "Epoch: 9, loss = -1.5643306428735908\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = -1.008260962637988\n",
      "Epoch: 1, loss = -1.1077802831476384\n",
      "Epoch: 2, loss = -1.1668387001210994\n",
      "Epoch: 3, loss = -1.088317800651897\n",
      "Epoch: 4, loss = -1.069780634208159\n",
      "Epoch: 5, loss = -1.1347068439830434\n",
      "Epoch: 6, loss = -1.31601406769319\n",
      "Epoch: 7, loss = -1.3700091026046062\n",
      "Epoch: 8, loss = -1.4209056171503933\n",
      "Epoch: 9, loss = -1.459593978795138\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = -1.1557388576594267\n",
      "Epoch: 1, loss = -1.2600351301106538\n",
      "Epoch: 2, loss = -1.3327226422049783\n",
      "Epoch: 3, loss = -1.4215097752484411\n",
      "Epoch: 4, loss = -1.4739813587882304\n",
      "Epoch: 5, loss = -1.46804649179632\n",
      "Epoch: 6, loss = -1.4685382734645496\n",
      "Epoch: 7, loss = -1.5297024900263008\n",
      "Epoch: 8, loss = -1.6340917023745452\n",
      "Epoch: 9, loss = -1.6790401610461148\n",
      "\n",
      "T = 3\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = -1.3684986432393393\n",
      "Epoch: 1, loss = -0.7857039620478948\n",
      "Epoch: 2, loss = -0.6997634048263233\n",
      "Epoch: 3, loss = -1.0950971419612565\n",
      "Epoch: 4, loss = -1.496397743622462\n",
      "Epoch: 5, loss = -1.625829060872396\n",
      "Epoch: 6, loss = -1.6620788474877677\n",
      "Epoch: 7, loss = -1.6985351045926411\n",
      "Epoch: 8, loss = -1.7342419425646465\n",
      "Epoch: 9, loss = -1.7731906473636627\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = -1.5939787924289703\n",
      "Epoch: 1, loss = -0.9752063167591891\n",
      "Epoch: 2, loss = -0.9008822080989677\n",
      "Epoch: 3, loss = -1.0427759538094201\n",
      "Epoch: 4, loss = -1.1824506570895512\n",
      "Epoch: 5, loss = -1.5466941297054293\n",
      "Epoch: 6, loss = -1.727596362431844\n",
      "Epoch: 7, loss = -1.775401065746943\n",
      "Epoch: 8, loss = -1.8101540505886078\n",
      "Epoch: 9, loss = -1.8390945295492807\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = -1.278268650174141\n",
      "Epoch: 1, loss = -1.43883482615153\n",
      "Epoch: 2, loss = -1.4366286446650822\n",
      "Epoch: 3, loss = -1.6416454116503398\n",
      "Epoch: 4, loss = -1.6504327058792112\n",
      "Epoch: 5, loss = -1.7558670540650687\n",
      "Epoch: 6, loss = -1.751410514116287\n",
      "Epoch: 7, loss = -1.810450166463852\n",
      "Epoch: 8, loss = -1.8178185025850933\n",
      "Epoch: 9, loss = -1.8260237872600553\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = -1.333141883214315\n",
      "Epoch: 1, loss = -1.3419809043407438\n",
      "Epoch: 2, loss = -1.3309197425842283\n",
      "Epoch: 3, loss = -1.4160249680280685\n",
      "Epoch: 4, loss = -1.4302825232346854\n",
      "Epoch: 5, loss = -1.5017475833495457\n",
      "Epoch: 6, loss = -1.5147806306680043\n",
      "Epoch: 7, loss = -1.5454822480678558\n",
      "Epoch: 8, loss = -1.5346518456935885\n",
      "Epoch: 9, loss = -1.5196797251701353\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = -1.7625764310359953\n",
      "Epoch: 1, loss = -1.708354522784551\n",
      "Epoch: 2, loss = -1.4798559943834944\n",
      "Epoch: 3, loss = -1.5713232656319935\n",
      "Epoch: 4, loss = -1.501861390968164\n",
      "Epoch: 5, loss = -1.3053258210420609\n",
      "Epoch: 6, loss = -1.3196345965067546\n",
      "Epoch: 7, loss = -1.4303813427686691\n",
      "Epoch: 8, loss = -1.632857898871104\n",
      "Epoch: 9, loss = -1.7087062299251554\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.8343117833137512\n",
      "Epoch: 1, loss = 0.7972749918699265\n",
      "Epoch: 2, loss = 0.7639063000679016\n",
      "Epoch: 3, loss = 0.7307401970028877\n",
      "Epoch: 4, loss = 0.6967381536960602\n",
      "Epoch: 5, loss = 0.6615797951817513\n",
      "Epoch: 6, loss = 0.624359242618084\n",
      "Epoch: 7, loss = 0.5844887346029282\n",
      "Epoch: 8, loss = 0.5414410643279552\n",
      "Epoch: 9, loss = 0.49476340040564537\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.7739237174391747\n",
      "Epoch: 1, loss = 0.7152590304613113\n",
      "Epoch: 2, loss = 0.6652219742536545\n",
      "Epoch: 3, loss = 0.6177457123994827\n",
      "Epoch: 4, loss = 0.5690100565552711\n",
      "Epoch: 5, loss = 0.5187578201293945\n",
      "Epoch: 6, loss = 0.4668896980583668\n",
      "Epoch: 7, loss = 0.4124934710562229\n",
      "Epoch: 8, loss = 0.3551187813282013\n",
      "Epoch: 9, loss = 0.29415640234947205\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.8077187612652779\n",
      "Epoch: 1, loss = 0.776576891541481\n",
      "Epoch: 2, loss = 0.7452234998345375\n",
      "Epoch: 3, loss = 0.7128882631659508\n",
      "Epoch: 4, loss = 0.6790819689631462\n",
      "Epoch: 5, loss = 0.6433211416006088\n",
      "Epoch: 6, loss = 0.6051415279507637\n",
      "Epoch: 7, loss = 0.5640516802668571\n",
      "Epoch: 8, loss = 0.5195331387221813\n",
      "Epoch: 9, loss = 0.47104930505156517\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.729969933629036\n",
      "Epoch: 1, loss = 0.6834419742226601\n",
      "Epoch: 2, loss = 0.6410914584994316\n",
      "Epoch: 3, loss = 0.5994448438286781\n",
      "Epoch: 4, loss = 0.5568822696805\n",
      "Epoch: 5, loss = 0.5131351090967655\n",
      "Epoch: 6, loss = 0.4676937386393547\n",
      "Epoch: 7, loss = 0.41985850036144257\n",
      "Epoch: 8, loss = 0.3689381368458271\n",
      "Epoch: 9, loss = 0.31456316262483597\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.7333459183573723\n",
      "Epoch: 1, loss = 0.6986559182405472\n",
      "Epoch: 2, loss = 0.6641641482710838\n",
      "Epoch: 3, loss = 0.6290376335382462\n",
      "Epoch: 4, loss = 0.5928840339183807\n",
      "Epoch: 5, loss = 0.5553633347153664\n",
      "Epoch: 6, loss = 0.5161524079740047\n",
      "Epoch: 7, loss = 0.4749310128390789\n",
      "Epoch: 8, loss = 0.4314114935696125\n",
      "Epoch: 9, loss = 0.3852224051952362\n",
      "\n",
      "T = 0\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.46823874115943914\n",
      "Epoch: 1, loss = 0.41521532535552985\n",
      "Epoch: 2, loss = 0.35933686047792435\n",
      "Epoch: 3, loss = 0.29917928874492644\n",
      "Epoch: 4, loss = 0.23419552445411684\n",
      "Epoch: 5, loss = 0.16366269960999488\n",
      "Epoch: 6, loss = 0.08717640172690153\n",
      "Epoch: 7, loss = 0.00450780726969242\n",
      "Epoch: 8, loss = -0.08411250710487365\n",
      "Epoch: 9, loss = -0.17760934382677077\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.25120454132556913\n",
      "Epoch: 1, loss = 0.18039051145315171\n",
      "Epoch: 2, loss = 0.1057032562792301\n",
      "Epoch: 3, loss = 0.026947995275259016\n",
      "Epoch: 4, loss = -0.05777274230495095\n",
      "Epoch: 5, loss = -0.14771042703650894\n",
      "Epoch: 6, loss = -0.2442696616053581\n",
      "Epoch: 7, loss = -0.3471338450908661\n",
      "Epoch: 8, loss = -0.4563414305448532\n",
      "Epoch: 9, loss = -0.5714629441499711\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.44262882173061363\n",
      "Epoch: 1, loss = 0.3883305609226227\n",
      "Epoch: 2, loss = 0.33137915879487995\n",
      "Epoch: 3, loss = 0.2704874023795128\n",
      "Epoch: 4, loss = 0.20519858859479428\n",
      "Epoch: 5, loss = 0.13486883230507374\n",
      "Epoch: 6, loss = 0.059249983914196495\n",
      "Epoch: 7, loss = -0.022180788032710556\n",
      "Epoch: 8, loss = -0.109542500320822\n",
      "Epoch: 9, loss = -0.20290488749742508\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.2848311737179756\n",
      "Epoch: 1, loss = 0.22245898842811584\n",
      "Epoch: 2, loss = 0.1577496126294136\n",
      "Epoch: 3, loss = 0.08853304106742144\n",
      "Epoch: 4, loss = 0.014748781919479374\n",
      "Epoch: 5, loss = -0.06445528715848924\n",
      "Epoch: 6, loss = -0.1485549253411591\n",
      "Epoch: 7, loss = -0.2379176672548056\n",
      "Epoch: 8, loss = -0.3320666268467903\n",
      "Epoch: 9, loss = -0.4302121117711067\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.35830900371074675\n",
      "Epoch: 1, loss = 0.3071371048688889\n",
      "Epoch: 2, loss = 0.2541207775473595\n",
      "Epoch: 3, loss = 0.1981174893677235\n",
      "Epoch: 4, loss = 0.13840986043214798\n",
      "Epoch: 5, loss = 0.07430336019024253\n",
      "Epoch: 6, loss = 0.005213743261992931\n",
      "Epoch: 7, loss = -0.06931910030543804\n",
      "Epoch: 8, loss = -0.14979416131973267\n",
      "Epoch: 9, loss = -0.23663613349199294\n",
      "\n",
      "T = 1\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = -0.20966100133955473\n",
      "Epoch: 1, loss = -0.29931069662173587\n",
      "Epoch: 2, loss = -0.39868698921054607\n",
      "Epoch: 3, loss = -0.4951253977293769\n",
      "Epoch: 4, loss = -0.587947458649675\n",
      "Epoch: 5, loss = -0.6454536008338133\n",
      "Epoch: 6, loss = -0.7461044440666835\n",
      "Epoch: 7, loss = -0.8374051426847775\n",
      "Epoch: 8, loss = -0.927889347076416\n",
      "Epoch: 9, loss = -0.9078160698215166\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = -0.6273515199621518\n",
      "Epoch: 1, loss = -0.7418528348207473\n",
      "Epoch: 2, loss = -0.8593516647815703\n",
      "Epoch: 3, loss = -0.9712564845879873\n",
      "Epoch: 4, loss = -1.081124986211459\n",
      "Epoch: 5, loss = -1.1935923546552658\n",
      "Epoch: 6, loss = -1.3075549950202308\n",
      "Epoch: 7, loss = -1.410721778869629\n",
      "Epoch: 8, loss = -1.4758029381434123\n",
      "Epoch: 9, loss = -1.434942662715912\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = -0.22873262781649828\n",
      "Epoch: 1, loss = -0.3236473786334197\n",
      "Epoch: 2, loss = -0.4222656482209762\n",
      "Epoch: 3, loss = -0.5235352284895877\n",
      "Epoch: 4, loss = -0.6259877358873684\n",
      "Epoch: 5, loss = -0.7245010485251745\n",
      "Epoch: 6, loss = -0.8164508193731308\n",
      "Epoch: 7, loss = -0.9131177092591921\n",
      "Epoch: 8, loss = -1.0122924819588663\n",
      "Epoch: 9, loss = -1.092688408990701\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = -0.46926498040556913\n",
      "Epoch: 1, loss = -0.5695454149196545\n",
      "Epoch: 2, loss = -0.6798042866090934\n",
      "Epoch: 3, loss = -0.7766171817978222\n",
      "Epoch: 4, loss = -0.8247865960001947\n",
      "Epoch: 5, loss = -0.8767868702610335\n",
      "Epoch: 6, loss = -0.9452556396524111\n",
      "Epoch: 7, loss = -1.050428698460261\n",
      "Epoch: 8, loss = -1.0061762978633246\n",
      "Epoch: 9, loss = -1.201141471664111\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = -0.2773582125858714\n",
      "Epoch: 1, loss = -0.3660016929109891\n",
      "Epoch: 2, loss = -0.4632412269711495\n",
      "Epoch: 3, loss = -0.562969463566939\n",
      "Epoch: 4, loss = -0.6641332730650901\n",
      "Epoch: 5, loss = -0.77235030879577\n",
      "Epoch: 6, loss = -0.8849172989527384\n",
      "Epoch: 7, loss = -0.9857953935861589\n",
      "Epoch: 8, loss = -1.0639157642920813\n",
      "Epoch: 9, loss = -1.1689953903357189\n",
      "\n",
      "T = 2\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = -1.0123641576085773\n",
      "Epoch: 1, loss = -0.9338062235287258\n",
      "Epoch: 2, loss = -1.0519661349909646\n",
      "Epoch: 3, loss = -1.1419057633195604\n",
      "Epoch: 4, loss = -1.1035306836877552\n",
      "Epoch: 5, loss = -1.2123180202075414\n",
      "Epoch: 6, loss = -1.077055654355458\n",
      "Epoch: 7, loss = -1.0976740366646223\n",
      "Epoch: 8, loss = -0.6247062172208514\n",
      "Epoch: 9, loss = -0.12189530687672753\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = -1.1364741548895834\n",
      "Epoch: 1, loss = -0.922542691230774\n",
      "Epoch: 2, loss = -0.48087695453848156\n",
      "Epoch: 3, loss = -0.6646920549018043\n",
      "Epoch: 4, loss = -1.095488318375179\n",
      "Epoch: 5, loss = -1.3654052657740454\n",
      "Epoch: 6, loss = -1.4839296170643397\n",
      "Epoch: 7, loss = -1.5359441467693875\n",
      "Epoch: 8, loss = -1.5834689140319826\n",
      "Epoch: 9, loss = -1.6317207217216492\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = -1.0968557000160215\n",
      "Epoch: 1, loss = -1.1636454675878798\n",
      "Epoch: 2, loss = -1.2769245590482439\n",
      "Epoch: 3, loss = -1.3543812930583954\n",
      "Epoch: 4, loss = -1.4438394180365972\n",
      "Epoch: 5, loss = -1.4076226864542283\n",
      "Epoch: 6, loss = -1.1586276143789291\n",
      "Epoch: 7, loss = -1.2926304851259502\n",
      "Epoch: 8, loss = -1.2554906053202493\n",
      "Epoch: 9, loss = -1.208458776984896\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = -1.151442608663014\n",
      "Epoch: 1, loss = -1.1386148929595945\n",
      "Epoch: 2, loss = -0.94794000685215\n",
      "Epoch: 3, loss = -1.0230933938707625\n",
      "Epoch: 4, loss = -0.9674055193151747\n",
      "Epoch: 5, loss = -0.9543821407215936\n",
      "Epoch: 6, loss = -1.029688251870019\n",
      "Epoch: 7, loss = -1.1653030599866594\n",
      "Epoch: 8, loss = -1.2975437172821587\n",
      "Epoch: 9, loss = -1.3669502351965224\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = -1.2340544845376695\n",
      "Epoch: 1, loss = -1.3056970153536116\n",
      "Epoch: 2, loss = -1.4050396851130895\n",
      "Epoch: 3, loss = -1.4627298925604137\n",
      "Epoch: 4, loss = -1.5476155706814356\n",
      "Epoch: 5, loss = -1.4692728604589194\n",
      "Epoch: 6, loss = -1.449520570891244\n",
      "Epoch: 7, loss = -1.5902362976755415\n",
      "Epoch: 8, loss = -1.6607024329049245\n",
      "Epoch: 9, loss = -1.6882088439805165\n",
      "\n",
      "T = 3\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = -1.1220029555261135\n",
      "Epoch: 1, loss = -1.2814105711877346\n",
      "Epoch: 2, loss = -1.2514662258327007\n",
      "Epoch: 3, loss = -1.0592047944664955\n",
      "Epoch: 4, loss = -0.8088926719501615\n",
      "Epoch: 5, loss = -0.9264704347588122\n",
      "Epoch: 6, loss = -1.2220314480364323\n",
      "Epoch: 7, loss = -1.069987677037716\n",
      "Epoch: 8, loss = -1.001883178949356\n",
      "Epoch: 9, loss = -1.1784124821424484\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = -1.572501502931118\n",
      "Epoch: 1, loss = -1.4465084187686443\n",
      "Epoch: 2, loss = -1.6623921021819115\n",
      "Epoch: 3, loss = -1.6815389096736908\n",
      "Epoch: 4, loss = -1.7263155430555344\n",
      "Epoch: 5, loss = -1.8223520293831825\n",
      "Epoch: 6, loss = -1.8744127750396729\n",
      "Epoch: 7, loss = -1.8519390299916267\n",
      "Epoch: 8, loss = -1.8488368690013885\n",
      "Epoch: 9, loss = -1.5114680044353008\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = -1.3804537281394005\n",
      "Epoch: 1, loss = -1.3049296028912067\n",
      "Epoch: 2, loss = -1.2387112863361835\n",
      "Epoch: 3, loss = -1.1688319072127342\n",
      "Epoch: 4, loss = -0.9457073099911213\n",
      "Epoch: 5, loss = -0.7895677154883742\n",
      "Epoch: 6, loss = -0.9132595732808113\n",
      "Epoch: 7, loss = -1.1209917031228542\n",
      "Epoch: 8, loss = -1.3337943479418755\n",
      "Epoch: 9, loss = -1.4398481994867325\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = -1.3670425079762936\n",
      "Epoch: 1, loss = -1.405815064907074\n",
      "Epoch: 2, loss = -1.3598719201982021\n",
      "Epoch: 3, loss = -1.206645518541336\n",
      "Epoch: 4, loss = -1.077537415549159\n",
      "Epoch: 5, loss = -0.9889168422669172\n",
      "Epoch: 6, loss = -1.13099623657763\n",
      "Epoch: 7, loss = -1.3120062351226807\n",
      "Epoch: 8, loss = -1.3721274733543396\n",
      "Epoch: 9, loss = -1.3142944276332855\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = -1.6052413880825043\n",
      "Epoch: 1, loss = -1.0138055123388767\n",
      "Epoch: 2, loss = -0.6694871820509434\n",
      "Epoch: 3, loss = -0.7229675799608231\n",
      "Epoch: 4, loss = -1.3065183013677597\n",
      "Epoch: 5, loss = -1.6339589804410934\n",
      "Epoch: 6, loss = -1.6823544055223465\n",
      "Epoch: 7, loss = -1.7309939116239548\n",
      "Epoch: 8, loss = -1.7349504306912422\n",
      "Epoch: 9, loss = -1.76035837829113\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.8343117833137512\n",
      "Epoch: 1, loss = 0.7972749918699265\n",
      "Epoch: 2, loss = 0.7639063000679016\n",
      "Epoch: 3, loss = 0.7307401970028877\n",
      "Epoch: 4, loss = 0.6967381536960602\n",
      "Epoch: 5, loss = 0.6615797951817513\n",
      "Epoch: 6, loss = 0.624359242618084\n",
      "Epoch: 7, loss = 0.5844887346029282\n",
      "Epoch: 8, loss = 0.5414410643279552\n",
      "Epoch: 9, loss = 0.49476340040564537\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.7739237174391747\n",
      "Epoch: 1, loss = 0.7152590304613113\n",
      "Epoch: 2, loss = 0.6652219742536545\n",
      "Epoch: 3, loss = 0.6177457123994827\n",
      "Epoch: 4, loss = 0.5690100565552711\n",
      "Epoch: 5, loss = 0.5187578201293945\n",
      "Epoch: 6, loss = 0.4668896980583668\n",
      "Epoch: 7, loss = 0.4124934710562229\n",
      "Epoch: 8, loss = 0.3551187813282013\n",
      "Epoch: 9, loss = 0.29415640234947205\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.8077187612652779\n",
      "Epoch: 1, loss = 0.776576891541481\n",
      "Epoch: 2, loss = 0.7452234998345375\n",
      "Epoch: 3, loss = 0.7128882631659508\n",
      "Epoch: 4, loss = 0.6790819689631462\n",
      "Epoch: 5, loss = 0.6433211416006088\n",
      "Epoch: 6, loss = 0.6051415279507637\n",
      "Epoch: 7, loss = 0.5640516802668571\n",
      "Epoch: 8, loss = 0.5195331387221813\n",
      "Epoch: 9, loss = 0.47104930505156517\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.729969933629036\n",
      "Epoch: 1, loss = 0.6834419742226601\n",
      "Epoch: 2, loss = 0.6410914584994316\n",
      "Epoch: 3, loss = 0.5994448438286781\n",
      "Epoch: 4, loss = 0.5568822696805\n",
      "Epoch: 5, loss = 0.5131351090967655\n",
      "Epoch: 6, loss = 0.4676937386393547\n",
      "Epoch: 7, loss = 0.41985850036144257\n",
      "Epoch: 8, loss = 0.3689381368458271\n",
      "Epoch: 9, loss = 0.31456316262483597\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.7333459183573723\n",
      "Epoch: 1, loss = 0.6986559182405472\n",
      "Epoch: 2, loss = 0.6641641482710838\n",
      "Epoch: 3, loss = 0.6290376335382462\n",
      "Epoch: 4, loss = 0.5928840339183807\n",
      "Epoch: 5, loss = 0.5553633347153664\n",
      "Epoch: 6, loss = 0.5161524079740047\n",
      "Epoch: 7, loss = 0.4749310128390789\n",
      "Epoch: 8, loss = 0.4314114935696125\n",
      "Epoch: 9, loss = 0.3852224051952362\n",
      "\n",
      "T = 0\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.49052197734514874\n",
      "Epoch: 1, loss = 0.4318644925951957\n",
      "Epoch: 2, loss = 0.3695255480706692\n",
      "Epoch: 3, loss = 0.30135844709972537\n",
      "Epoch: 4, loss = 0.2265056795440614\n",
      "Epoch: 5, loss = 0.14422643836587667\n",
      "Epoch: 6, loss = 0.05389721877872944\n",
      "Epoch: 7, loss = -0.04463427327573301\n",
      "Epoch: 8, loss = -0.14971077411125103\n",
      "Epoch: 9, loss = -0.2543807631979386\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.2631751957039038\n",
      "Epoch: 1, loss = 0.18240171608825523\n",
      "Epoch: 2, loss = 0.09535575533906619\n",
      "Epoch: 3, loss = 0.0033563991698125992\n",
      "Epoch: 4, loss = -0.09790703499068816\n",
      "Epoch: 5, loss = -0.20670769829303026\n",
      "Epoch: 6, loss = -0.32401963385442895\n",
      "Epoch: 7, loss = -0.4497891987363497\n",
      "Epoch: 8, loss = -0.5830031087001164\n",
      "Epoch: 9, loss = -0.7209861526886621\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.464611458281676\n",
      "Epoch: 1, loss = 0.4052155166864395\n",
      "Epoch: 2, loss = 0.34261406833926833\n",
      "Epoch: 3, loss = 0.27485574347277486\n",
      "Epoch: 4, loss = 0.20105273781033856\n",
      "Epoch: 5, loss = 0.12061007243270676\n",
      "Epoch: 6, loss = 0.0328211912419647\n",
      "Epoch: 7, loss = -0.06269016582518816\n",
      "Epoch: 8, loss = -0.1661301615725582\n",
      "Epoch: 9, loss = -0.2770354443540176\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.2982101154824098\n",
      "Epoch: 1, loss = 0.22603395332892734\n",
      "Epoch: 2, loss = 0.15027243985484043\n",
      "Epoch: 3, loss = 0.0681581183647116\n",
      "Epoch: 4, loss = -0.020785264670848843\n",
      "Epoch: 5, loss = -0.11702777848889431\n",
      "Epoch: 6, loss = -0.2203371993576487\n",
      "Epoch: 7, loss = -0.3299012252440055\n",
      "Epoch: 8, loss = -0.4439754711153607\n",
      "Epoch: 9, loss = -0.5577019328872361\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.37430058668057126\n",
      "Epoch: 1, loss = 0.316589326908191\n",
      "Epoch: 2, loss = 0.2563565423091253\n",
      "Epoch: 3, loss = 0.19175931873420873\n",
      "Epoch: 4, loss = 0.12152861951229472\n",
      "Epoch: 5, loss = 0.04478990713444849\n",
      "Epoch: 6, loss = -0.03922530775889754\n",
      "Epoch: 7, loss = -0.1312125859161218\n",
      "Epoch: 8, loss = -0.2316443636858215\n",
      "Epoch: 9, loss = -0.340454447393616\n",
      "\n",
      "T = 1\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = -0.3015694839414209\n",
      "Epoch: 1, loss = -0.4109247839078307\n",
      "Epoch: 2, loss = -0.5416420996189117\n",
      "Epoch: 3, loss = -0.6515255440026522\n",
      "Epoch: 4, loss = -0.7478500828146935\n",
      "Epoch: 5, loss = -0.6995507590472698\n",
      "Epoch: 6, loss = -0.7724818550050259\n",
      "Epoch: 7, loss = -0.8787692934274673\n",
      "Epoch: 8, loss = -0.8670857883989811\n",
      "Epoch: 9, loss = -0.9058144353330135\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = -0.7832292336970568\n",
      "Epoch: 1, loss = -0.9181753695011139\n",
      "Epoch: 2, loss = -1.055822428315878\n",
      "Epoch: 3, loss = -1.1822617538273335\n",
      "Epoch: 4, loss = -1.1578134633600712\n",
      "Epoch: 5, loss = -0.6961387991905212\n",
      "Epoch: 6, loss = -0.5566718587651849\n",
      "Epoch: 7, loss = 0.23126564174890518\n",
      "Epoch: 8, loss = -0.04199415957555175\n",
      "Epoch: 9, loss = -0.8866596296429634\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = -0.3164527161279693\n",
      "Epoch: 1, loss = -0.4369336711242795\n",
      "Epoch: 2, loss = -0.5666090743616223\n",
      "Epoch: 3, loss = -0.693859163671732\n",
      "Epoch: 4, loss = -0.8052951246500015\n",
      "Epoch: 5, loss = -0.8743255566805601\n",
      "Epoch: 6, loss = -1.0516092330217361\n",
      "Epoch: 7, loss = -1.1644237376749516\n",
      "Epoch: 8, loss = -1.2691564373672009\n",
      "Epoch: 9, loss = -1.2215527221560478\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = -0.5996139729395509\n",
      "Epoch: 1, loss = -0.7347374130040407\n",
      "Epoch: 2, loss = -0.8645048197358847\n",
      "Epoch: 3, loss = -0.8625331725925207\n",
      "Epoch: 4, loss = -0.9292230922728777\n",
      "Epoch: 5, loss = -0.636432446539402\n",
      "Epoch: 6, loss = -0.959953922778368\n",
      "Epoch: 7, loss = -1.0658697038888931\n",
      "Epoch: 8, loss = -1.124317780137062\n",
      "Epoch: 9, loss = -1.1852212846279144\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = -0.4068382512778044\n",
      "Epoch: 1, loss = -0.5092020854353905\n",
      "Epoch: 2, loss = -0.647159531712532\n",
      "Epoch: 3, loss = -0.774896290153265\n",
      "Epoch: 4, loss = -0.8760585952550173\n",
      "Epoch: 5, loss = -1.0137523375451565\n",
      "Epoch: 6, loss = -1.1334944851696491\n",
      "Epoch: 7, loss = -1.1856476739048958\n",
      "Epoch: 8, loss = -1.1399310231208801\n",
      "Epoch: 9, loss = -1.3300422467291355\n",
      "\n",
      "T = 2\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = -0.8947036772966385\n",
      "Epoch: 1, loss = -0.9701023519039152\n",
      "Epoch: 2, loss = -0.8677435137331485\n",
      "Epoch: 3, loss = -0.8380152061581612\n",
      "Epoch: 4, loss = -0.9222400948405264\n",
      "Epoch: 5, loss = -1.1385740369558333\n",
      "Epoch: 6, loss = -1.3057905882596967\n",
      "Epoch: 7, loss = -1.3906421780586242\n",
      "Epoch: 8, loss = -1.4887386977672576\n",
      "Epoch: 9, loss = -1.54427342414856\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = -1.1319556057453155\n",
      "Epoch: 1, loss = -1.2984931737184522\n",
      "Epoch: 2, loss = -1.4177573919296265\n",
      "Epoch: 3, loss = -1.3887398272752762\n",
      "Epoch: 4, loss = -1.3003158420324326\n",
      "Epoch: 5, loss = -1.566631007194519\n",
      "Epoch: 6, loss = -1.4570242732763292\n",
      "Epoch: 7, loss = -1.554442945122719\n",
      "Epoch: 8, loss = -1.7640731930732727\n",
      "Epoch: 9, loss = -1.8560036063194276\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = -1.0089821740984917\n",
      "Epoch: 1, loss = -1.1911167204380033\n",
      "Epoch: 2, loss = -1.1706212669610976\n",
      "Epoch: 3, loss = -1.0791080258786676\n",
      "Epoch: 4, loss = -1.0446329742670057\n",
      "Epoch: 5, loss = -1.0799082919955254\n",
      "Epoch: 6, loss = -1.181170970201492\n",
      "Epoch: 7, loss = -1.3616190612316132\n",
      "Epoch: 8, loss = -1.4575925916433334\n",
      "Epoch: 9, loss = -1.5125661849975587\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = -1.2539011359214784\n",
      "Epoch: 1, loss = -1.2384200423955916\n",
      "Epoch: 2, loss = -0.9294734969735148\n",
      "Epoch: 3, loss = -0.3919524744153024\n",
      "Epoch: 4, loss = -0.32169256284832964\n",
      "Epoch: 5, loss = -0.6319074213504792\n",
      "Epoch: 6, loss = -1.0165605962276458\n",
      "Epoch: 7, loss = -1.1859741538763044\n",
      "Epoch: 8, loss = -1.2723996311426164\n",
      "Epoch: 9, loss = -1.3397449582815169\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = -1.2661731332540513\n",
      "Epoch: 1, loss = -1.3611046969890594\n",
      "Epoch: 2, loss = -1.3007366269826888\n",
      "Epoch: 3, loss = -1.267546832561493\n",
      "Epoch: 4, loss = -1.1781971514225005\n",
      "Epoch: 5, loss = -1.026197624206543\n",
      "Epoch: 6, loss = -0.835305482149124\n",
      "Epoch: 7, loss = -0.95540639385581\n",
      "Epoch: 8, loss = -1.238219851255417\n",
      "Epoch: 9, loss = -1.429458558559418\n",
      "\n",
      "T = 3\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = -1.4586858078837395\n",
      "Epoch: 1, loss = -1.6175666352113085\n",
      "Epoch: 2, loss = -1.7071516116460166\n",
      "Epoch: 3, loss = -1.6429346352815624\n",
      "Epoch: 4, loss = -1.6708871175845466\n",
      "Epoch: 5, loss = -1.5866872866948447\n",
      "Epoch: 6, loss = -1.3293528718252976\n",
      "Epoch: 7, loss = -1.5036555950840316\n",
      "Epoch: 8, loss = -1.6222149481376014\n",
      "Epoch: 9, loss = -1.2034122732778392\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = -1.8603877673546474\n",
      "Epoch: 1, loss = -1.8035142521063485\n",
      "Epoch: 2, loss = -1.8080167497197788\n",
      "Epoch: 3, loss = -1.4881812793513138\n",
      "Epoch: 4, loss = -1.442714663843314\n",
      "Epoch: 5, loss = -1.7446925987799962\n",
      "Epoch: 6, loss = -1.5774043872952463\n",
      "Epoch: 7, loss = -1.6301038789873323\n",
      "Epoch: 8, loss = -1.577620000268022\n",
      "Epoch: 9, loss = -1.4491188774506252\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = -1.4772399465243025\n",
      "Epoch: 1, loss = -1.5605784108241396\n",
      "Epoch: 2, loss = -1.4640062004327776\n",
      "Epoch: 3, loss = -1.1133396830409763\n",
      "Epoch: 4, loss = -1.0776957534253595\n",
      "Epoch: 5, loss = -0.8135051081577936\n",
      "Epoch: 6, loss = -0.7593284292767444\n",
      "Epoch: 7, loss = -1.000102149322629\n",
      "Epoch: 8, loss = -1.0922205249468488\n",
      "Epoch: 9, loss = -0.9317017408708731\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = -1.3698904688159623\n",
      "Epoch: 1, loss = -1.3538550958037376\n",
      "Epoch: 2, loss = -1.3710571279128394\n",
      "Epoch: 3, loss = -1.2345646396279335\n",
      "Epoch: 4, loss = -1.133737172931433\n",
      "Epoch: 5, loss = -0.8626176963249843\n",
      "Epoch: 6, loss = -0.7944986447691919\n",
      "Epoch: 7, loss = -1.031943435470263\n",
      "Epoch: 8, loss = -1.3286588316162429\n",
      "Epoch: 9, loss = -1.470139463742574\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = -1.4850573490063348\n",
      "Epoch: 1, loss = -1.5768759151299792\n",
      "Epoch: 2, loss = -1.4759738941987357\n",
      "Epoch: 3, loss = -1.4850377353529134\n",
      "Epoch: 4, loss = -1.6853395452102027\n",
      "Epoch: 5, loss = -1.802965670824051\n",
      "Epoch: 6, loss = -1.8529805789391196\n",
      "Epoch: 7, loss = -1.761351391673088\n",
      "Epoch: 8, loss = -1.834503591060638\n",
      "Epoch: 9, loss = -1.4666143183906872\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.8343117833137512\n",
      "Epoch: 1, loss = 0.7972749918699265\n",
      "Epoch: 2, loss = 0.7639063000679016\n",
      "Epoch: 3, loss = 0.7307401970028877\n",
      "Epoch: 4, loss = 0.6967381536960602\n",
      "Epoch: 5, loss = 0.6615797951817513\n",
      "Epoch: 6, loss = 0.624359242618084\n",
      "Epoch: 7, loss = 0.5844887346029282\n",
      "Epoch: 8, loss = 0.5414410643279552\n",
      "Epoch: 9, loss = 0.49476340040564537\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.7739237174391747\n",
      "Epoch: 1, loss = 0.7152590304613113\n",
      "Epoch: 2, loss = 0.6652219742536545\n",
      "Epoch: 3, loss = 0.6177457123994827\n",
      "Epoch: 4, loss = 0.5690100565552711\n",
      "Epoch: 5, loss = 0.5187578201293945\n",
      "Epoch: 6, loss = 0.4668896980583668\n",
      "Epoch: 7, loss = 0.4124934710562229\n",
      "Epoch: 8, loss = 0.3551187813282013\n",
      "Epoch: 9, loss = 0.29415640234947205\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.8077187612652779\n",
      "Epoch: 1, loss = 0.776576891541481\n",
      "Epoch: 2, loss = 0.7452234998345375\n",
      "Epoch: 3, loss = 0.7128882631659508\n",
      "Epoch: 4, loss = 0.6790819689631462\n",
      "Epoch: 5, loss = 0.6433211416006088\n",
      "Epoch: 6, loss = 0.6051415279507637\n",
      "Epoch: 7, loss = 0.5640516802668571\n",
      "Epoch: 8, loss = 0.5195331387221813\n",
      "Epoch: 9, loss = 0.47104930505156517\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.729969933629036\n",
      "Epoch: 1, loss = 0.6834419742226601\n",
      "Epoch: 2, loss = 0.6410914584994316\n",
      "Epoch: 3, loss = 0.5994448438286781\n",
      "Epoch: 4, loss = 0.5568822696805\n",
      "Epoch: 5, loss = 0.5131351090967655\n",
      "Epoch: 6, loss = 0.4676937386393547\n",
      "Epoch: 7, loss = 0.41985850036144257\n",
      "Epoch: 8, loss = 0.3689381368458271\n",
      "Epoch: 9, loss = 0.31456316262483597\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.7333459183573723\n",
      "Epoch: 1, loss = 0.6986559182405472\n",
      "Epoch: 2, loss = 0.6641641482710838\n",
      "Epoch: 3, loss = 0.6290376335382462\n",
      "Epoch: 4, loss = 0.5928840339183807\n",
      "Epoch: 5, loss = 0.5553633347153664\n",
      "Epoch: 6, loss = 0.5161524079740047\n",
      "Epoch: 7, loss = 0.4749310128390789\n",
      "Epoch: 8, loss = 0.4314114935696125\n",
      "Epoch: 9, loss = 0.3852224051952362\n",
      "\n",
      "T = 0\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.5103982351720333\n",
      "Epoch: 1, loss = 0.4371834546327591\n",
      "Epoch: 2, loss = 0.3561863945797086\n",
      "Epoch: 3, loss = 0.2643822970567271\n",
      "Epoch: 4, loss = 0.15947459475137293\n",
      "Epoch: 5, loss = 0.03972358023747802\n",
      "Epoch: 6, loss = -0.09577037114650011\n",
      "Epoch: 7, loss = -0.24726921296678483\n",
      "Epoch: 8, loss = -0.40885693626478314\n",
      "Epoch: 9, loss = -0.5803261650726199\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.2719305790960789\n",
      "Epoch: 1, loss = 0.16860951436683536\n",
      "Epoch: 2, loss = 0.05195153335807845\n",
      "Epoch: 3, loss = -0.07552617706824094\n",
      "Epoch: 4, loss = -0.21767667611129582\n",
      "Epoch: 5, loss = -0.3738510673865676\n",
      "Epoch: 6, loss = -0.5404384918510914\n",
      "Epoch: 7, loss = -0.7122709024697542\n",
      "Epoch: 8, loss = -0.8830852098762989\n",
      "Epoch: 9, loss = -1.0395896099507809\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.48299642838537693\n",
      "Epoch: 1, loss = 0.4107547476887703\n",
      "Epoch: 2, loss = 0.3313036635518074\n",
      "Epoch: 3, loss = 0.24324627686291933\n",
      "Epoch: 4, loss = 0.14311043923953548\n",
      "Epoch: 5, loss = 0.030795700382441282\n",
      "Epoch: 6, loss = -0.09542169084306806\n",
      "Epoch: 7, loss = -0.23506023501977324\n",
      "Epoch: 8, loss = -0.38341486162971705\n",
      "Epoch: 9, loss = -0.5280982339754701\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.3102379599586129\n",
      "Epoch: 1, loss = 0.21766242268495262\n",
      "Epoch: 2, loss = 0.11478925606934354\n",
      "Epoch: 3, loss = 0.001520508958492428\n",
      "Epoch: 4, loss = -0.12511244614142925\n",
      "Epoch: 5, loss = -0.2645944969262928\n",
      "Epoch: 6, loss = -0.4179787961766124\n",
      "Epoch: 7, loss = -0.5638474281877279\n",
      "Epoch: 8, loss = -0.6472138483077288\n",
      "Epoch: 9, loss = -0.7116694264113903\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.38867295160889626\n",
      "Epoch: 1, loss = 0.31613722536712885\n",
      "Epoch: 2, loss = 0.23797424603253603\n",
      "Epoch: 3, loss = 0.15058740973472595\n",
      "Epoch: 4, loss = 0.05181764782173559\n",
      "Epoch: 5, loss = -0.05966903327498585\n",
      "Epoch: 6, loss = -0.18492198025342077\n",
      "Epoch: 7, loss = -0.3238509907387197\n",
      "Epoch: 8, loss = -0.4722947273403406\n",
      "Epoch: 9, loss = -0.6211070865392685\n",
      "\n",
      "T = 1\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = -0.6625782810151578\n",
      "Epoch: 1, loss = -0.853493706633647\n",
      "Epoch: 2, loss = -0.9411105314890545\n",
      "Epoch: 3, loss = -0.8436247011025748\n",
      "Epoch: 4, loss = -0.6689824064572653\n",
      "Epoch: 5, loss = -0.6518923516074817\n",
      "Epoch: 6, loss = -0.8715614099055529\n",
      "Epoch: 7, loss = -0.7568993537376325\n",
      "Epoch: 8, loss = -0.9291432338456314\n",
      "Epoch: 9, loss = -1.0850482856233914\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = -1.093463438252608\n",
      "Epoch: 1, loss = -1.195120304822922\n",
      "Epoch: 2, loss = -1.1995279838641484\n",
      "Epoch: 3, loss = -1.2559182867407797\n",
      "Epoch: 4, loss = -1.2178509682416916\n",
      "Epoch: 5, loss = -1.4215642288327217\n",
      "Epoch: 6, loss = -1.4785919984181723\n",
      "Epoch: 7, loss = -1.44726311104993\n",
      "Epoch: 8, loss = -1.5406005109349885\n",
      "Epoch: 9, loss = -1.6816727568705878\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = -0.6208221515019734\n",
      "Epoch: 1, loss = -0.7942324094474316\n",
      "Epoch: 2, loss = -0.9880636818706989\n",
      "Epoch: 3, loss = -1.0831663658221562\n",
      "Epoch: 4, loss = -1.129499112566312\n",
      "Epoch: 5, loss = -1.1879441017905872\n",
      "Epoch: 6, loss = -1.3429997141162555\n",
      "Epoch: 7, loss = -1.2264292525748413\n",
      "Epoch: 8, loss = -1.0064681048194568\n",
      "Epoch: 9, loss = -0.4566996482511362\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = -0.5778606701642276\n",
      "Epoch: 1, loss = -0.7706807379921277\n",
      "Epoch: 2, loss = -0.8371757181982199\n",
      "Epoch: 3, loss = -0.9508186976114908\n",
      "Epoch: 4, loss = -0.9949091523885725\n",
      "Epoch: 5, loss = -1.0754260395963988\n",
      "Epoch: 6, loss = -1.2423589949806533\n",
      "Epoch: 7, loss = -1.3751146718859673\n",
      "Epoch: 8, loss = -1.1763719307879605\n",
      "Epoch: 9, loss = -1.2537110770742097\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = -0.6896711836258571\n",
      "Epoch: 1, loss = -0.8397456556558609\n",
      "Epoch: 2, loss = -1.03771044810613\n",
      "Epoch: 3, loss = -1.1851635326941807\n",
      "Epoch: 4, loss = -1.165605867902438\n",
      "Epoch: 5, loss = -0.847467176616192\n",
      "Epoch: 6, loss = -1.1934831328690052\n",
      "Epoch: 7, loss = -1.1979416621228063\n",
      "Epoch: 8, loss = -1.046677942077319\n",
      "Epoch: 9, loss = -0.8056521024554968\n",
      "\n",
      "T = 2\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = -1.2824915293604136\n",
      "Epoch: 1, loss = -1.0711653772741556\n",
      "Epoch: 2, loss = -1.2696629986166954\n",
      "Epoch: 3, loss = -1.4234974011778831\n",
      "Epoch: 4, loss = -1.507982900366187\n",
      "Epoch: 5, loss = -1.5165954669937491\n",
      "Epoch: 6, loss = -1.3270450760610402\n",
      "Epoch: 7, loss = -0.8429493056610227\n",
      "Epoch: 8, loss = -1.1224902290850878\n",
      "Epoch: 9, loss = -1.492294479161501\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = -1.7358202524483204\n",
      "Epoch: 1, loss = -1.6980976816266775\n",
      "Epoch: 2, loss = -1.9208093285560608\n",
      "Epoch: 3, loss = -1.7445876160636544\n",
      "Epoch: 4, loss = -1.8581522889435291\n",
      "Epoch: 5, loss = -1.9042783975601196\n",
      "Epoch: 6, loss = -1.988382987678051\n",
      "Epoch: 7, loss = -1.7068343171849847\n",
      "Epoch: 8, loss = -1.7535985745489597\n",
      "Epoch: 9, loss = -1.3862313516438007\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = -0.9380277735181153\n",
      "Epoch: 1, loss = -1.2942250669002533\n",
      "Epoch: 2, loss = -1.4145498592406511\n",
      "Epoch: 3, loss = -1.4544500410556793\n",
      "Epoch: 4, loss = -1.3587515037506819\n",
      "Epoch: 5, loss = -1.456503914669156\n",
      "Epoch: 6, loss = -1.3022088035941124\n",
      "Epoch: 7, loss = -1.5688589736819267\n",
      "Epoch: 8, loss = -1.5917969718575478\n",
      "Epoch: 9, loss = -1.5301959654316306\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = -0.9747812757268548\n",
      "Epoch: 1, loss = -1.1023355973884463\n",
      "Epoch: 2, loss = -1.3704723324626684\n",
      "Epoch: 3, loss = -1.5051357373595238\n",
      "Epoch: 4, loss = -1.467863729223609\n",
      "Epoch: 5, loss = -1.3928588693961501\n",
      "Epoch: 6, loss = -1.2075863347854465\n",
      "Epoch: 7, loss = -1.2653342233970761\n",
      "Epoch: 8, loss = -1.5405373349785805\n",
      "Epoch: 9, loss = -1.6881551966071129\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = -0.6773256864398718\n",
      "Epoch: 1, loss = -1.1557443030178547\n",
      "Epoch: 2, loss = -1.3355997446924448\n",
      "Epoch: 3, loss = -1.4149604048579931\n",
      "Epoch: 4, loss = -1.423865806311369\n",
      "Epoch: 5, loss = -1.224172015907243\n",
      "Epoch: 6, loss = -1.0473026409745216\n",
      "Epoch: 7, loss = -1.4730064235627651\n",
      "Epoch: 8, loss = -1.6193842515349388\n",
      "Epoch: 9, loss = -1.6553384028375149\n",
      "\n",
      "T = 3\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = -1.6189096808433534\n",
      "Epoch: 1, loss = -1.6952235609292983\n",
      "Epoch: 2, loss = -1.2606319636106487\n",
      "Epoch: 3, loss = -1.7574899613857267\n",
      "Epoch: 4, loss = -1.7781296193599707\n",
      "Epoch: 5, loss = -1.8371263235807422\n",
      "Epoch: 6, loss = -1.7997911214828488\n",
      "Epoch: 7, loss = -1.8302061527967455\n",
      "Epoch: 8, loss = -1.7671683609485624\n",
      "Epoch: 9, loss = -1.6561074085533627\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = -1.5959916338324551\n",
      "Epoch: 1, loss = -1.25927169919014\n",
      "Epoch: 2, loss = -1.3569888323545458\n",
      "Epoch: 3, loss = -1.7376549094915392\n",
      "Epoch: 4, loss = -1.6585029020905493\n",
      "Epoch: 5, loss = -1.7385561227798463\n",
      "Epoch: 6, loss = -1.5830079138278959\n",
      "Epoch: 7, loss = -1.1413952454924585\n",
      "Epoch: 8, loss = -1.573371335864067\n",
      "Epoch: 9, loss = -1.832483121752739\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = -1.5986767232418064\n",
      "Epoch: 1, loss = -1.22812769934535\n",
      "Epoch: 2, loss = -1.5968100458383565\n",
      "Epoch: 3, loss = -1.7126111537218092\n",
      "Epoch: 4, loss = -1.6206437699496745\n",
      "Epoch: 5, loss = -1.4053651265799998\n",
      "Epoch: 6, loss = -1.3818332381546494\n",
      "Epoch: 7, loss = -1.7286971837282183\n",
      "Epoch: 8, loss = -1.684753686189651\n",
      "Epoch: 9, loss = -1.6725794743746518\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = -1.6711672246456144\n",
      "Epoch: 1, loss = -1.5182662077248097\n",
      "Epoch: 2, loss = -1.6246369630098336\n",
      "Epoch: 3, loss = -1.6497657239437105\n",
      "Epoch: 4, loss = -1.8114194929599763\n",
      "Epoch: 5, loss = -1.8569679856300356\n",
      "Epoch: 6, loss = -1.7840144485235216\n",
      "Epoch: 7, loss = -1.6564755037426953\n",
      "Epoch: 8, loss = -1.5099329743534324\n",
      "Epoch: 9, loss = -1.7741732001304624\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = -1.7440596520900726\n",
      "Epoch: 1, loss = -1.5813066482543947\n",
      "Epoch: 2, loss = -1.8969188332557676\n",
      "Epoch: 3, loss = -1.7473113119602202\n",
      "Epoch: 4, loss = -1.727500342577696\n",
      "Epoch: 5, loss = -1.003859531879425\n",
      "Epoch: 6, loss = -1.2405986135825513\n",
      "Epoch: 7, loss = -1.720466876029968\n",
      "Epoch: 8, loss = -1.8362380683422084\n",
      "Epoch: 9, loss = -1.9097061365842818\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.8343117833137512\n",
      "Epoch: 1, loss = 0.7972749918699265\n",
      "Epoch: 2, loss = 0.7639063000679016\n",
      "Epoch: 3, loss = 0.7307401970028877\n",
      "Epoch: 4, loss = 0.6967381536960602\n",
      "Epoch: 5, loss = 0.6615797951817513\n",
      "Epoch: 6, loss = 0.624359242618084\n",
      "Epoch: 7, loss = 0.5844887346029282\n",
      "Epoch: 8, loss = 0.5414410643279552\n",
      "Epoch: 9, loss = 0.49476340040564537\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.7739237174391747\n",
      "Epoch: 1, loss = 0.7152590304613113\n",
      "Epoch: 2, loss = 0.6652219742536545\n",
      "Epoch: 3, loss = 0.6177457123994827\n",
      "Epoch: 4, loss = 0.5690100565552711\n",
      "Epoch: 5, loss = 0.5187578201293945\n",
      "Epoch: 6, loss = 0.4668896980583668\n",
      "Epoch: 7, loss = 0.4124934710562229\n",
      "Epoch: 8, loss = 0.3551187813282013\n",
      "Epoch: 9, loss = 0.29415640234947205\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.8077187612652779\n",
      "Epoch: 1, loss = 0.776576891541481\n",
      "Epoch: 2, loss = 0.7452234998345375\n",
      "Epoch: 3, loss = 0.7128882631659508\n",
      "Epoch: 4, loss = 0.6790819689631462\n",
      "Epoch: 5, loss = 0.6433211416006088\n",
      "Epoch: 6, loss = 0.6051415279507637\n",
      "Epoch: 7, loss = 0.5640516802668571\n",
      "Epoch: 8, loss = 0.5195331387221813\n",
      "Epoch: 9, loss = 0.47104930505156517\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.729969933629036\n",
      "Epoch: 1, loss = 0.6834419742226601\n",
      "Epoch: 2, loss = 0.6410914584994316\n",
      "Epoch: 3, loss = 0.5994448438286781\n",
      "Epoch: 4, loss = 0.5568822696805\n",
      "Epoch: 5, loss = 0.5131351090967655\n",
      "Epoch: 6, loss = 0.4676937386393547\n",
      "Epoch: 7, loss = 0.41985850036144257\n",
      "Epoch: 8, loss = 0.3689381368458271\n",
      "Epoch: 9, loss = 0.31456316262483597\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.7333459183573723\n",
      "Epoch: 1, loss = 0.6986559182405472\n",
      "Epoch: 2, loss = 0.6641641482710838\n",
      "Epoch: 3, loss = 0.6290376335382462\n",
      "Epoch: 4, loss = 0.5928840339183807\n",
      "Epoch: 5, loss = 0.5553633347153664\n",
      "Epoch: 6, loss = 0.5161524079740047\n",
      "Epoch: 7, loss = 0.4749310128390789\n",
      "Epoch: 8, loss = 0.4314114935696125\n",
      "Epoch: 9, loss = 0.3852224051952362\n",
      "\n",
      "T = 0\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.5278336902459463\n",
      "Epoch: 1, loss = 0.4248689965655406\n",
      "Epoch: 2, loss = 0.3008134663105011\n",
      "Epoch: 3, loss = 0.14811509381979704\n",
      "Epoch: 4, loss = -0.03993417287711056\n",
      "Epoch: 5, loss = -0.2681874245560417\n",
      "Epoch: 6, loss = -0.5251190091172856\n",
      "Epoch: 7, loss = -0.7601768150925634\n",
      "Epoch: 8, loss = -0.8730339532097181\n",
      "Epoch: 9, loss = -1.2159728432695067\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.2700213299443325\n",
      "Epoch: 1, loss = 0.11442957484784226\n",
      "Epoch: 2, loss = -0.06900290601576367\n",
      "Epoch: 3, loss = -0.2854989959547917\n",
      "Epoch: 4, loss = -0.5339546091854571\n",
      "Epoch: 5, loss = -0.7835557957490284\n",
      "Epoch: 6, loss = -1.0215112020572028\n",
      "Epoch: 7, loss = -1.2145861834287646\n",
      "Epoch: 8, loss = -1.0608238553007443\n",
      "Epoch: 9, loss = -0.5626121088862419\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.4961108701924483\n",
      "Epoch: 1, loss = 0.3991619056711595\n",
      "Epoch: 2, loss = 0.2792746820487082\n",
      "Epoch: 3, loss = 0.1386553829846283\n",
      "Epoch: 4, loss = -0.0331463462401492\n",
      "Epoch: 5, loss = -0.23996523562042665\n",
      "Epoch: 6, loss = -0.4777988453085223\n",
      "Epoch: 7, loss = -0.7391507749756178\n",
      "Epoch: 8, loss = -0.9532238120834033\n",
      "Epoch: 9, loss = -1.060022821029027\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.32269122637808323\n",
      "Epoch: 1, loss = 0.19865700866406164\n",
      "Epoch: 2, loss = 0.035044522141106434\n",
      "Epoch: 3, loss = -0.14352840852613247\n",
      "Epoch: 4, loss = -0.33970120409503574\n",
      "Epoch: 5, loss = -0.5478320444623629\n",
      "Epoch: 6, loss = -0.7024617443482081\n",
      "Epoch: 7, loss = -0.8512587199608486\n",
      "Epoch: 8, loss = -0.9157418943941594\n",
      "Epoch: 9, loss = -1.0248202979564667\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.3921672378977139\n",
      "Epoch: 1, loss = 0.28806850872933865\n",
      "Epoch: 2, loss = 0.16660853444288176\n",
      "Epoch: 3, loss = 0.02035951804524909\n",
      "Epoch: 4, loss = -0.15451651454592746\n",
      "Epoch: 5, loss = -0.35428974839548266\n",
      "Epoch: 6, loss = -0.5589576512575148\n",
      "Epoch: 7, loss = -0.723328864822785\n",
      "Epoch: 8, loss = -0.8134300727397202\n",
      "Epoch: 9, loss = -0.9493827509383358\n",
      "\n",
      "T = 1\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = -1.2578194156289098\n",
      "Epoch: 1, loss = -1.2866894230246544\n",
      "Epoch: 2, loss = -0.8948308702558277\n",
      "Epoch: 3, loss = -0.616480367258191\n",
      "Epoch: 4, loss = -0.7206785656511783\n",
      "Epoch: 5, loss = -0.8779198125004768\n",
      "Epoch: 6, loss = -1.190531615912914\n",
      "Epoch: 7, loss = -1.3574900314211846\n",
      "Epoch: 8, loss = -1.4779432475566863\n",
      "Epoch: 9, loss = -1.560755839943886\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = -0.928852131962776\n",
      "Epoch: 1, loss = -1.1581525169312956\n",
      "Epoch: 2, loss = -1.3747264891862871\n",
      "Epoch: 3, loss = -1.5471180886030194\n",
      "Epoch: 4, loss = -1.6431944593787193\n",
      "Epoch: 5, loss = -1.6841125592589374\n",
      "Epoch: 6, loss = -1.4910545557737347\n",
      "Epoch: 7, loss = -1.5512143976986406\n",
      "Epoch: 8, loss = -1.6933341860771176\n",
      "Epoch: 9, loss = -1.946639978885651\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = -1.2222203537821767\n",
      "Epoch: 1, loss = -1.249770273268223\n",
      "Epoch: 2, loss = -1.3941122204065322\n",
      "Epoch: 3, loss = -1.3302684143185615\n",
      "Epoch: 4, loss = -1.3261810332536694\n",
      "Epoch: 5, loss = -1.155018094927072\n",
      "Epoch: 6, loss = -1.5180901199579242\n",
      "Epoch: 7, loss = -1.577180010080338\n",
      "Epoch: 8, loss = -1.7276700288057323\n",
      "Epoch: 9, loss = -1.7537872821092608\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = -1.04793194681406\n",
      "Epoch: 1, loss = -0.9119649983942509\n",
      "Epoch: 2, loss = -0.8073531083762646\n",
      "Epoch: 3, loss = -1.1534365594387055\n",
      "Epoch: 4, loss = -1.1027132309973238\n",
      "Epoch: 5, loss = -1.2198625177145004\n",
      "Epoch: 6, loss = -1.253552412986755\n",
      "Epoch: 7, loss = -1.3406634822487833\n",
      "Epoch: 8, loss = -1.4696131974458697\n",
      "Epoch: 9, loss = -1.601838561892509\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = -1.0891506373882291\n",
      "Epoch: 1, loss = -1.0289295256137847\n",
      "Epoch: 2, loss = -1.3374446466565135\n",
      "Epoch: 3, loss = -1.4690298706293106\n",
      "Epoch: 4, loss = -1.5017005860805512\n",
      "Epoch: 5, loss = -1.591861155629158\n",
      "Epoch: 6, loss = -1.64054939635098\n",
      "Epoch: 7, loss = -1.7323630928993226\n",
      "Epoch: 8, loss = -1.7479229032993315\n",
      "Epoch: 9, loss = -1.7560297727584837\n",
      "\n",
      "T = 2\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = -1.397690250538289\n",
      "Epoch: 1, loss = -1.16490658359336\n",
      "Epoch: 2, loss = -1.414802435785532\n",
      "Epoch: 3, loss = -1.5987113416194916\n",
      "Epoch: 4, loss = -1.5598159565457272\n",
      "Epoch: 5, loss = -1.6797365312065398\n",
      "Epoch: 6, loss = -1.4707006007166845\n",
      "Epoch: 7, loss = -1.6802612223795486\n",
      "Epoch: 8, loss = -1.693233069564615\n",
      "Epoch: 9, loss = -1.7933829277753832\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = -1.8800587153860502\n",
      "Epoch: 1, loss = -1.98400242626667\n",
      "Epoch: 2, loss = -2.061918137328966\n",
      "Epoch: 3, loss = -2.137916230729648\n",
      "Epoch: 4, loss = -2.0989718990666524\n",
      "Epoch: 5, loss = -2.184367362942015\n",
      "Epoch: 6, loss = -2.114446902913707\n",
      "Epoch: 7, loss = -2.1219898590019772\n",
      "Epoch: 8, loss = -2.258803080235209\n",
      "Epoch: 9, loss = -2.23388721048832\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = -1.72616156722818\n",
      "Epoch: 1, loss = -1.4537386942122659\n",
      "Epoch: 2, loss = -1.519718000798353\n",
      "Epoch: 3, loss = -1.3675577802849668\n",
      "Epoch: 4, loss = -1.062528877386025\n",
      "Epoch: 5, loss = -1.4813147122040389\n",
      "Epoch: 6, loss = -1.6844710218054908\n",
      "Epoch: 7, loss = -1.7789501334939681\n",
      "Epoch: 8, loss = -1.895424610802105\n",
      "Epoch: 9, loss = -1.9036855378321236\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = -1.5042061530319706\n",
      "Epoch: 1, loss = -1.3394362213356157\n",
      "Epoch: 2, loss = -1.379021227359772\n",
      "Epoch: 3, loss = -1.6760636568069462\n",
      "Epoch: 4, loss = -1.7439388377325875\n",
      "Epoch: 5, loss = -1.5651424851800708\n",
      "Epoch: 6, loss = -1.509462754907352\n",
      "Epoch: 7, loss = -1.646402289159596\n",
      "Epoch: 8, loss = -1.6064107601663893\n",
      "Epoch: 9, loss = -1.8072112745472362\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = -1.812898948788643\n",
      "Epoch: 1, loss = -1.6952430536704401\n",
      "Epoch: 2, loss = -1.3664538344102246\n",
      "Epoch: 3, loss = -1.7420900112816267\n",
      "Epoch: 4, loss = -1.831609246986253\n",
      "Epoch: 5, loss = -1.9358587690762112\n",
      "Epoch: 6, loss = -1.889313612665449\n",
      "Epoch: 7, loss = -1.592985530516931\n",
      "Epoch: 8, loss = -1.6456307814057383\n",
      "Epoch: 9, loss = -1.9443480627877372\n",
      "\n",
      "T = 3\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = -1.6224142971138156\n",
      "Epoch: 1, loss = -1.5909718275070188\n",
      "Epoch: 2, loss = -1.8062401149008007\n",
      "Epoch: 3, loss = -1.8634563850031944\n",
      "Epoch: 4, loss = -1.895983502268791\n",
      "Epoch: 5, loss = -1.9590866963068647\n",
      "Epoch: 6, loss = -2.010395447413127\n",
      "Epoch: 7, loss = -2.0665159324804936\n",
      "Epoch: 8, loss = -2.0896338936355376\n",
      "Epoch: 9, loss = -2.1079351123836303\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = -1.7642653588619501\n",
      "Epoch: 1, loss = -1.9326099993454087\n",
      "Epoch: 2, loss = -2.140793963438935\n",
      "Epoch: 3, loss = -2.12248821121951\n",
      "Epoch: 4, loss = -2.1977024045255447\n",
      "Epoch: 5, loss = -2.2266332457462945\n",
      "Epoch: 6, loss = -2.2837744818793406\n",
      "Epoch: 7, loss = -2.3096704350577464\n",
      "Epoch: 8, loss = -2.35213407377402\n",
      "Epoch: 9, loss = -2.354059022333887\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = -1.6619420581393776\n",
      "Epoch: 1, loss = -1.7024726890441442\n",
      "Epoch: 2, loss = -1.6700815649496181\n",
      "Epoch: 3, loss = -1.761875328504377\n",
      "Epoch: 4, loss = -1.8737433966663155\n",
      "Epoch: 5, loss = -1.9325899365875456\n",
      "Epoch: 6, loss = -2.006238136026594\n",
      "Epoch: 7, loss = -2.004607990384102\n",
      "Epoch: 8, loss = -1.955958651171791\n",
      "Epoch: 9, loss = -2.0463666419188185\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = -1.7546676480107837\n",
      "Epoch: 1, loss = -1.69918698279394\n",
      "Epoch: 2, loss = -1.7997876877586043\n",
      "Epoch: 3, loss = -1.848763348327743\n",
      "Epoch: 4, loss = -1.8369352701637485\n",
      "Epoch: 5, loss = -1.8439713269472122\n",
      "Epoch: 6, loss = -1.8900595787498689\n",
      "Epoch: 7, loss = -1.92679019106759\n",
      "Epoch: 8, loss = -1.9514353854788673\n",
      "Epoch: 9, loss = -1.9827563282516267\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = -2.0009900345984435\n",
      "Epoch: 1, loss = -1.90595104586747\n",
      "Epoch: 2, loss = -2.041257686085171\n",
      "Epoch: 3, loss = -2.135303157899115\n",
      "Epoch: 4, loss = -2.196529171533055\n",
      "Epoch: 5, loss = -2.2463868326610994\n",
      "Epoch: 6, loss = -2.2837305714686713\n",
      "Epoch: 7, loss = -2.308994965420829\n",
      "Epoch: 8, loss = -2.3496247579654055\n",
      "Epoch: 9, loss = -2.3208838933044\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.8343117833137512\n",
      "Epoch: 1, loss = 0.7972749918699265\n",
      "Epoch: 2, loss = 0.7639063000679016\n",
      "Epoch: 3, loss = 0.7307401970028877\n",
      "Epoch: 4, loss = 0.6967381536960602\n",
      "Epoch: 5, loss = 0.6615797951817513\n",
      "Epoch: 6, loss = 0.624359242618084\n",
      "Epoch: 7, loss = 0.5844887346029282\n",
      "Epoch: 8, loss = 0.5414410643279552\n",
      "Epoch: 9, loss = 0.49476340040564537\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.7739237174391747\n",
      "Epoch: 1, loss = 0.7152590304613113\n",
      "Epoch: 2, loss = 0.6652219742536545\n",
      "Epoch: 3, loss = 0.6177457123994827\n",
      "Epoch: 4, loss = 0.5690100565552711\n",
      "Epoch: 5, loss = 0.5187578201293945\n",
      "Epoch: 6, loss = 0.4668896980583668\n",
      "Epoch: 7, loss = 0.4124934710562229\n",
      "Epoch: 8, loss = 0.3551187813282013\n",
      "Epoch: 9, loss = 0.29415640234947205\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.8077187612652779\n",
      "Epoch: 1, loss = 0.776576891541481\n",
      "Epoch: 2, loss = 0.7452234998345375\n",
      "Epoch: 3, loss = 0.7128882631659508\n",
      "Epoch: 4, loss = 0.6790819689631462\n",
      "Epoch: 5, loss = 0.6433211416006088\n",
      "Epoch: 6, loss = 0.6051415279507637\n",
      "Epoch: 7, loss = 0.5640516802668571\n",
      "Epoch: 8, loss = 0.5195331387221813\n",
      "Epoch: 9, loss = 0.47104930505156517\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.729969933629036\n",
      "Epoch: 1, loss = 0.6834419742226601\n",
      "Epoch: 2, loss = 0.6410914584994316\n",
      "Epoch: 3, loss = 0.5994448438286781\n",
      "Epoch: 4, loss = 0.5568822696805\n",
      "Epoch: 5, loss = 0.5131351090967655\n",
      "Epoch: 6, loss = 0.4676937386393547\n",
      "Epoch: 7, loss = 0.41985850036144257\n",
      "Epoch: 8, loss = 0.3689381368458271\n",
      "Epoch: 9, loss = 0.31456316262483597\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.7333459183573723\n",
      "Epoch: 1, loss = 0.6986559182405472\n",
      "Epoch: 2, loss = 0.6641641482710838\n",
      "Epoch: 3, loss = 0.6290376335382462\n",
      "Epoch: 4, loss = 0.5928840339183807\n",
      "Epoch: 5, loss = 0.5553633347153664\n",
      "Epoch: 6, loss = 0.5161524079740047\n",
      "Epoch: 7, loss = 0.4749310128390789\n",
      "Epoch: 8, loss = 0.4314114935696125\n",
      "Epoch: 9, loss = 0.3852224051952362\n",
      "\n",
      "T = 0\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.5211547158658504\n",
      "Epoch: 1, loss = 0.33868848700076337\n",
      "Epoch: 2, loss = 0.07510341762099415\n",
      "Epoch: 3, loss = -0.3039833523333073\n",
      "Epoch: 4, loss = -0.7867119967937471\n",
      "Epoch: 5, loss = -1.2172438785433768\n",
      "Epoch: 6, loss = -1.3956934809684753\n",
      "Epoch: 7, loss = -1.5668059319257739\n",
      "Epoch: 8, loss = -1.1255787648260593\n",
      "Epoch: 9, loss = -0.78642489425838\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.23103481531143188\n",
      "Epoch: 1, loss = -0.05990755269303918\n",
      "Epoch: 2, loss = -0.4484477452933789\n",
      "Epoch: 3, loss = -0.8179497018456461\n",
      "Epoch: 4, loss = -0.445475029014051\n",
      "Epoch: 5, loss = -0.795846926048398\n",
      "Epoch: 6, loss = -1.0476002976298333\n",
      "Epoch: 7, loss = -1.171707487106323\n",
      "Epoch: 8, loss = -1.3891720220446582\n",
      "Epoch: 9, loss = -1.5120402187108992\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.4866897895932199\n",
      "Epoch: 1, loss = 0.3193076971918345\n",
      "Epoch: 2, loss = 0.0778032979927957\n",
      "Epoch: 3, loss = -0.2526807059533895\n",
      "Epoch: 4, loss = -0.6803371667861938\n",
      "Epoch: 5, loss = -1.1165497526526453\n",
      "Epoch: 6, loss = -1.3871997416019441\n",
      "Epoch: 7, loss = -1.2501182720065118\n",
      "Epoch: 8, loss = -1.2773470975458623\n",
      "Epoch: 9, loss = -1.5242655441164976\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.29329740256071096\n",
      "Epoch: 1, loss = 0.07696035045664758\n",
      "Epoch: 2, loss = -0.21220310712233187\n",
      "Epoch: 3, loss = -0.4860351376235486\n",
      "Epoch: 4, loss = -0.7555714614689351\n",
      "Epoch: 5, loss = -1.0177027985453604\n",
      "Epoch: 6, loss = -1.1940940052270888\n",
      "Epoch: 7, loss = -1.2624611034989357\n",
      "Epoch: 8, loss = -1.3633461728692058\n",
      "Epoch: 9, loss = -1.4357325948774813\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.3756956487894058\n",
      "Epoch: 1, loss = 0.1903068317100406\n",
      "Epoch: 2, loss = -0.06846506181173026\n",
      "Epoch: 3, loss = -0.4134983763098718\n",
      "Epoch: 4, loss = -0.7278574198484422\n",
      "Epoch: 5, loss = -1.0093542009592056\n",
      "Epoch: 6, loss = -0.9916895668953657\n",
      "Epoch: 7, loss = -1.2108211174607277\n",
      "Epoch: 8, loss = -1.2190386578440666\n",
      "Epoch: 9, loss = -1.1930826246738435\n",
      "\n",
      "T = 1\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = -1.0541160222556858\n",
      "Epoch: 1, loss = -1.501987955636448\n",
      "Epoch: 2, loss = -1.6010749803649054\n",
      "Epoch: 3, loss = -1.6747696747382481\n",
      "Epoch: 4, loss = -1.759754281904963\n",
      "Epoch: 5, loss = -1.785756886833244\n",
      "Epoch: 6, loss = -1.77321131941345\n",
      "Epoch: 7, loss = -1.8821265482240255\n",
      "Epoch: 8, loss = -1.888848641680347\n",
      "Epoch: 9, loss = -1.9025222924020553\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = -1.4126368282983703\n",
      "Epoch: 1, loss = -1.5782240902384124\n",
      "Epoch: 2, loss = -1.7783602285716262\n",
      "Epoch: 3, loss = -1.8593363993697698\n",
      "Epoch: 4, loss = -1.873851653602387\n",
      "Epoch: 5, loss = -2.0051017122136225\n",
      "Epoch: 6, loss = -1.8841614485200906\n",
      "Epoch: 7, loss = -2.04838788178232\n",
      "Epoch: 8, loss = -2.050700013836225\n",
      "Epoch: 9, loss = -1.8329307453499901\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = -1.3385753010710084\n",
      "Epoch: 1, loss = -1.5958464600973652\n",
      "Epoch: 2, loss = -1.6446987688541406\n",
      "Epoch: 3, loss = -1.7011042071713338\n",
      "Epoch: 4, loss = -1.7513076166311903\n",
      "Epoch: 5, loss = -1.7961000386211614\n",
      "Epoch: 6, loss = -1.8581064889828365\n",
      "Epoch: 7, loss = -1.9099358303679355\n",
      "Epoch: 8, loss = -1.9322111432751021\n",
      "Epoch: 9, loss = -1.969909394780795\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = -0.9437898105631272\n",
      "Epoch: 1, loss = -1.2369398023519247\n",
      "Epoch: 2, loss = -1.450197383140524\n",
      "Epoch: 3, loss = -1.4968198405371773\n",
      "Epoch: 4, loss = -1.562170891712109\n",
      "Epoch: 5, loss = -1.626922438955969\n",
      "Epoch: 6, loss = -1.6921905146704779\n",
      "Epoch: 7, loss = -1.759482777780957\n",
      "Epoch: 8, loss = -1.7780541492005186\n",
      "Epoch: 9, loss = -1.7681550300783582\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = -1.0715784520531693\n",
      "Epoch: 1, loss = -1.4726088320215547\n",
      "Epoch: 2, loss = -1.5801380127668383\n",
      "Epoch: 3, loss = -1.628461562510994\n",
      "Epoch: 4, loss = -1.6994584600130722\n",
      "Epoch: 5, loss = -1.7899740644627151\n",
      "Epoch: 6, loss = -1.8621358391311433\n",
      "Epoch: 7, loss = -1.9004167707429986\n",
      "Epoch: 8, loss = -1.963414553966787\n",
      "Epoch: 9, loss = -2.019799611634679\n",
      "\n",
      "T = 2\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = -1.836367554962636\n",
      "Epoch: 1, loss = -1.9065450882682435\n",
      "Epoch: 2, loss = -1.9687552245763629\n",
      "Epoch: 3, loss = -2.0108696841276603\n",
      "Epoch: 4, loss = -2.0491886729231243\n",
      "Epoch: 5, loss = -2.0819817947653614\n",
      "Epoch: 6, loss = -2.11983223259449\n",
      "Epoch: 7, loss = -2.154236368261851\n",
      "Epoch: 8, loss = -2.1892744623697724\n",
      "Epoch: 9, loss = -2.223437626774494\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = -2.0466585096258383\n",
      "Epoch: 1, loss = -1.9683833784208846\n",
      "Epoch: 2, loss = -2.1839538190800405\n",
      "Epoch: 3, loss = -2.206801450166564\n",
      "Epoch: 4, loss = -2.2746910206400432\n",
      "Epoch: 5, loss = -2.2394943724458027\n",
      "Epoch: 6, loss = -2.3157054564127555\n",
      "Epoch: 7, loss = -2.3634186942990003\n",
      "Epoch: 8, loss = -2.4081888233239837\n",
      "Epoch: 9, loss = -2.453969231018654\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = -1.9937181031474704\n",
      "Epoch: 1, loss = -2.098518175574449\n",
      "Epoch: 2, loss = -2.0594325231818043\n",
      "Epoch: 3, loss = -2.162077565605825\n",
      "Epoch: 4, loss = -2.1438741387369546\n",
      "Epoch: 5, loss = -2.2661076807058773\n",
      "Epoch: 6, loss = -2.2113055724364057\n",
      "Epoch: 7, loss = -2.3172566581230893\n",
      "Epoch: 8, loss = -2.324898998897809\n",
      "Epoch: 9, loss = -2.36127642599436\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = -1.8043835592957644\n",
      "Epoch: 1, loss = -1.8182200735005047\n",
      "Epoch: 2, loss = -1.8780408286704464\n",
      "Epoch: 3, loss = -1.9577966521565744\n",
      "Epoch: 4, loss = -1.9786180709130488\n",
      "Epoch: 5, loss = -2.069613848741239\n",
      "Epoch: 6, loss = -2.0863907658136807\n",
      "Epoch: 7, loss = -2.1804488805624156\n",
      "Epoch: 8, loss = -2.294290474974192\n",
      "Epoch: 9, loss = -2.2814714822631608\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = -1.9946313691683688\n",
      "Epoch: 1, loss = -2.0724220665601583\n",
      "Epoch: 2, loss = -2.0649921596050267\n",
      "Epoch: 3, loss = -2.1310217546728927\n",
      "Epoch: 4, loss = -2.1528102509104277\n",
      "Epoch: 5, loss = -2.2026561624728718\n",
      "Epoch: 6, loss = -2.2248595288166633\n",
      "Epoch: 7, loss = -2.2731968703178262\n",
      "Epoch: 8, loss = -2.3056969413390522\n",
      "Epoch: 9, loss = -2.346416906668589\n",
      "\n",
      "T = 3\n",
      "\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = -2.230249234858681\n",
      "Epoch: 1, loss = -2.177024927647676\n",
      "Epoch: 2, loss = -2.290365737150697\n",
      "Epoch: 3, loss = -2.289828257744803\n",
      "Epoch: 4, loss = -2.3565931477967443\n",
      "Epoch: 5, loss = -2.3819237666971547\n",
      "Epoch: 6, loss = -2.423620034666622\n",
      "Epoch: 7, loss = -2.4552259401363505\n",
      "Epoch: 8, loss = -2.4896828795180608\n",
      "Epoch: 9, loss = -2.5258179263157015\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = -2.4644666506963606\n",
      "Epoch: 1, loss = -2.452946678680532\n",
      "Epoch: 2, loss = -2.589593254468022\n",
      "Epoch: 3, loss = -2.583742381895289\n",
      "Epoch: 4, loss = -2.6808444454389435\n",
      "Epoch: 5, loss = -2.6914939836544143\n",
      "Epoch: 6, loss = -2.744312523042454\n",
      "Epoch: 7, loss = -2.7622125955188968\n",
      "Epoch: 8, loss = -2.8058008691843823\n",
      "Epoch: 9, loss = -2.837016047800289\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = -2.3747792901361695\n",
      "Epoch: 1, loss = -2.45233255712425\n",
      "Epoch: 2, loss = -2.4981022862827067\n",
      "Epoch: 3, loss = -2.5491337276556902\n",
      "Epoch: 4, loss = -2.596217123024603\n",
      "Epoch: 5, loss = -2.637749163543476\n",
      "Epoch: 6, loss = -2.6773836200728147\n",
      "Epoch: 7, loss = -2.7352674182723553\n",
      "Epoch: 8, loss = -2.8116225372342507\n",
      "Epoch: 9, loss = -2.8515267985708586\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = -2.2758310296956235\n",
      "Epoch: 1, loss = -2.352652769426213\n",
      "Epoch: 2, loss = -2.4250875382738966\n",
      "Epoch: 3, loss = -2.484924563590218\n",
      "Epoch: 4, loss = -2.521758495008245\n",
      "Epoch: 5, loss = -2.574986408738529\n",
      "Epoch: 6, loss = -2.6029978885370144\n",
      "Epoch: 7, loss = -2.656284745125209\n",
      "Epoch: 8, loss = -2.689598459531279\n",
      "Epoch: 9, loss = -2.737371798823861\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = -2.3285737826543693\n",
      "Epoch: 1, loss = -2.401308785466588\n",
      "Epoch: 2, loss = -2.488856947597334\n",
      "Epoch: 3, loss = -2.5373587582041233\n",
      "Epoch: 4, loss = -2.5999438771430197\n",
      "Epoch: 5, loss = -2.648151658913669\n",
      "Epoch: 6, loss = -2.690130170653848\n",
      "Epoch: 7, loss = -2.7340395941453814\n",
      "Epoch: 8, loss = -2.7768651054185964\n",
      "Epoch: 9, loss = -2.8282812079962576\n",
      "xx is :[0.2135408889655744, 0.2115829726863397, 0.38616938161093484, 0.20778644909258082, 0.15886543600633674, 0.11947580383557051]\n"
     ]
    }
   ],
   "source": [
    "re_al = RegressionExperiment(regressor=alregegre, x_data=x_data, y_data=y_labels, config_al=config_al)\n",
    "xx = re_al.run_exp()\n",
    "print(\"xx is :{}\".format(xx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.8183656136194866\n",
      "Epoch: 1, loss = 0.7646834949652355\n",
      "Epoch: 2, loss = 0.7138570000727972\n",
      "Epoch: 3, loss = 0.6610143532355627\n",
      "Epoch: 4, loss = 0.6039798756440481\n",
      "Epoch: 5, loss = 0.5403289571404457\n",
      "Epoch: 6, loss = 0.46841879189014446\n",
      "Epoch: 7, loss = 0.38682224974036217\n",
      "Epoch: 8, loss = 0.29388865083456045\n",
      "Epoch: 9, loss = 0.1884552662571271\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.7641843557357788\n",
      "Epoch: 1, loss = 0.6817546784877777\n",
      "Epoch: 2, loss = 0.6102657268444698\n",
      "Epoch: 3, loss = 0.536622221271197\n",
      "Epoch: 4, loss = 0.4592393065492313\n",
      "Epoch: 5, loss = 0.37618664403756463\n",
      "Epoch: 6, loss = 0.2855764453609785\n",
      "Epoch: 7, loss = 0.18582022252182168\n",
      "Epoch: 8, loss = 0.07562972807014981\n",
      "Epoch: 9, loss = -0.04634533915668726\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.7983540048201879\n",
      "Epoch: 1, loss = 0.750200092792511\n",
      "Epoch: 2, loss = 0.7003947496414186\n",
      "Epoch: 3, loss = 0.6467303683360417\n",
      "Epoch: 4, loss = 0.5875769307216008\n",
      "Epoch: 5, loss = 0.5210975830753645\n",
      "Epoch: 6, loss = 0.445516973733902\n",
      "Epoch: 7, loss = 0.3592721497019132\n",
      "Epoch: 8, loss = 0.2610904599229495\n",
      "Epoch: 9, loss = 0.14995725297679505\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.7215756128231684\n",
      "Epoch: 1, loss = 0.6563625236352284\n",
      "Epoch: 2, loss = 0.5942991524934769\n",
      "Epoch: 3, loss = 0.5303062871098518\n",
      "Epoch: 4, loss = 0.4626760929822922\n",
      "Epoch: 5, loss = 0.3896717304984728\n",
      "Epoch: 6, loss = 0.30958184227347374\n",
      "Epoch: 7, loss = 0.2210137490183115\n",
      "Epoch: 8, loss = 0.12266869284212589\n",
      "Epoch: 9, loss = 0.014010728569701317\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.7153281519810358\n",
      "Epoch: 1, loss = 0.6639233380556107\n",
      "Epoch: 2, loss = 0.6116606841484705\n",
      "Epoch: 3, loss = 0.5568309600154558\n",
      "Epoch: 4, loss = 0.49825109789768857\n",
      "Epoch: 5, loss = 0.43473916252454126\n",
      "Epoch: 6, loss = 0.3652870108683904\n",
      "Epoch: 7, loss = 0.28883397082487744\n",
      "Epoch: 8, loss = 0.20436628287037212\n",
      "Epoch: 9, loss = 0.11088666381935278\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.8143937662243843\n",
      "Epoch: 1, loss = 0.7457415908575058\n",
      "Epoch: 2, loss = 0.6775983907282352\n",
      "Epoch: 3, loss = 0.6030754968523979\n",
      "Epoch: 4, loss = 0.516944756731391\n",
      "Epoch: 5, loss = 0.4156727008521557\n",
      "Epoch: 6, loss = 0.2959118876606226\n",
      "Epoch: 7, loss = 0.15458587452303618\n",
      "Epoch: 8, loss = -0.009671504143625498\n",
      "Epoch: 9, loss = -0.1919259624555707\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.7468650192022324\n",
      "Epoch: 1, loss = 0.6433260217308998\n",
      "Epoch: 2, loss = 0.5464299414306879\n",
      "Epoch: 3, loss = 0.44270262867212296\n",
      "Epoch: 4, loss = 0.3286035805940628\n",
      "Epoch: 5, loss = 0.20008519478142262\n",
      "Epoch: 6, loss = 0.053986210259608924\n",
      "Epoch: 7, loss = -0.1121634051669389\n",
      "Epoch: 8, loss = -0.30011238949373364\n",
      "Epoch: 9, loss = -0.5092223584651947\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.7934218198060989\n",
      "Epoch: 1, loss = 0.7290878966450691\n",
      "Epoch: 2, loss = 0.6602148003876209\n",
      "Epoch: 3, loss = 0.5824650824069977\n",
      "Epoch: 4, loss = 0.491922527551651\n",
      "Epoch: 5, loss = 0.3846374563872814\n",
      "Epoch: 6, loss = 0.25735072139650583\n",
      "Epoch: 7, loss = 0.10779609228484333\n",
      "Epoch: 8, loss = -0.06503426021663472\n",
      "Epoch: 9, loss = -0.2600989523343742\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.7104667760431767\n",
      "Epoch: 1, loss = 0.624744102358818\n",
      "Epoch: 2, loss = 0.5395172648131847\n",
      "Epoch: 3, loss = 0.44790269806981087\n",
      "Epoch: 4, loss = 0.34550124779343605\n",
      "Epoch: 5, loss = 0.22853634785860777\n",
      "Epoch: 6, loss = 0.0944197810604237\n",
      "Epoch: 7, loss = -0.05729791504563764\n",
      "Epoch: 8, loss = -0.2237531233113259\n",
      "Epoch: 9, loss = -0.39754688274115324\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.7110259532928467\n",
      "Epoch: 1, loss = 0.641809094697237\n",
      "Epoch: 2, loss = 0.5696201361715794\n",
      "Epoch: 3, loss = 0.4913914203643799\n",
      "Epoch: 4, loss = 0.40438782796263695\n",
      "Epoch: 5, loss = 0.30621495097875595\n",
      "Epoch: 6, loss = 0.19451787183061242\n",
      "Epoch: 7, loss = 0.06693696300499141\n",
      "Epoch: 8, loss = -0.07837454089894891\n",
      "Epoch: 9, loss = -0.24257118627429008\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.8008486454685528\n",
      "Epoch: 1, loss = 0.7021746610601742\n",
      "Epoch: 2, loss = 0.5941924328605334\n",
      "Epoch: 3, loss = 0.4604257916410764\n",
      "Epoch: 4, loss = 0.2887496091425419\n",
      "Epoch: 5, loss = 0.06942133434737723\n",
      "Epoch: 6, loss = -0.1986658121459186\n",
      "Epoch: 7, loss = -0.49376814067363745\n",
      "Epoch: 8, loss = -0.7624886110424997\n",
      "Epoch: 9, loss = -0.9531706546743711\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.7199024309714636\n",
      "Epoch: 1, loss = 0.5735201388597488\n",
      "Epoch: 2, loss = 0.4208752202490965\n",
      "Epoch: 3, loss = 0.24307090571771064\n",
      "Epoch: 4, loss = 0.028739494853653035\n",
      "Epoch: 5, loss = -0.2306271415824691\n",
      "Epoch: 6, loss = -0.5375185621281465\n",
      "Epoch: 7, loss = -0.8699043815334637\n",
      "Epoch: 8, loss = -1.1282388269901278\n",
      "Epoch: 9, loss = -1.3020466913779576\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.7783618172009783\n",
      "Epoch: 1, loss = 0.6822906136512757\n",
      "Epoch: 2, loss = 0.5703615620732306\n",
      "Epoch: 3, loss = 0.4298551095028718\n",
      "Epoch: 4, loss = 0.24936274811625486\n",
      "Epoch: 5, loss = 0.021071722886214644\n",
      "Epoch: 6, loss = -0.2567423307336867\n",
      "Epoch: 7, loss = -0.5694839047888914\n",
      "Epoch: 8, loss = -0.8436180017888547\n",
      "Epoch: 9, loss = -0.9665770853559175\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.690897099673748\n",
      "Epoch: 1, loss = 0.56570053845644\n",
      "Epoch: 2, loss = 0.4314889001349608\n",
      "Epoch: 3, loss = 0.2728322700907786\n",
      "Epoch: 4, loss = 0.07830369868315758\n",
      "Epoch: 5, loss = -0.1532541101332754\n",
      "Epoch: 6, loss = -0.3962128038207689\n",
      "Epoch: 7, loss = -0.6252407083908716\n",
      "Epoch: 8, loss = -0.8355577600498996\n",
      "Epoch: 9, loss = -0.8983322456479071\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.6961687207221984\n",
      "Epoch: 1, loss = 0.5936207721630732\n",
      "Epoch: 2, loss = 0.4801578819751739\n",
      "Epoch: 3, loss = 0.3469625016053518\n",
      "Epoch: 4, loss = 0.186076195910573\n",
      "Epoch: 5, loss = -0.009692642682542402\n",
      "Epoch: 6, loss = -0.24542530439794064\n",
      "Epoch: 7, loss = -0.5194339863955975\n",
      "Epoch: 8, loss = -0.817069875697295\n",
      "Epoch: 9, loss = -1.1071435088912644\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.7590695977211\n",
      "Epoch: 1, loss = 0.577646916359663\n",
      "Epoch: 2, loss = 0.32399913016706705\n",
      "Epoch: 3, loss = -0.033748793718405064\n",
      "Epoch: 4, loss = -0.3686451710760595\n",
      "Epoch: 5, loss = -0.6078841848298907\n",
      "Epoch: 6, loss = -0.8891694072633983\n",
      "Epoch: 7, loss = -1.135650360584259\n",
      "Epoch: 8, loss = -1.3491423189640044\n",
      "Epoch: 9, loss = -1.5965743005275725\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.6751525491476059\n",
      "Epoch: 1, loss = 0.42815046235918996\n",
      "Epoch: 2, loss = 0.12007029810920362\n",
      "Epoch: 3, loss = -0.29665543618611995\n",
      "Epoch: 4, loss = -0.8061484307050706\n",
      "Epoch: 5, loss = -1.2740332484245298\n",
      "Epoch: 6, loss = -1.5540366560220717\n",
      "Epoch: 7, loss = -1.691649064421654\n",
      "Epoch: 8, loss = -1.6466173157095911\n",
      "Epoch: 9, loss = -1.7746934831142427\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.7498362526297568\n",
      "Epoch: 1, loss = 0.5674924537539483\n",
      "Epoch: 2, loss = 0.3054682184010744\n",
      "Epoch: 3, loss = -0.07495567784644665\n",
      "Epoch: 4, loss = -0.5250161703675985\n",
      "Epoch: 5, loss = -0.90672892332077\n",
      "Epoch: 6, loss = -1.1702401429414744\n",
      "Epoch: 7, loss = -1.3856206417083743\n",
      "Epoch: 8, loss = -1.5155355989933017\n",
      "Epoch: 9, loss = -1.6520486563444134\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.6519603326916695\n",
      "Epoch: 1, loss = 0.43201748430728915\n",
      "Epoch: 2, loss = 0.149325885437429\n",
      "Epoch: 3, loss = -0.21855947780422866\n",
      "Epoch: 4, loss = -0.5930975332856179\n",
      "Epoch: 5, loss = -0.9236192777752875\n",
      "Epoch: 6, loss = -1.145404622144997\n",
      "Epoch: 7, loss = -1.3933377742767334\n",
      "Epoch: 8, loss = -1.4523135080933574\n",
      "Epoch: 9, loss = -1.6723965466022495\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.6635843873023987\n",
      "Epoch: 1, loss = 0.483189795166254\n",
      "Epoch: 2, loss = 0.2505636237561702\n",
      "Epoch: 3, loss = -0.06808300164993852\n",
      "Epoch: 4, loss = -0.4915359638631345\n",
      "Epoch: 5, loss = -0.9735125944018365\n",
      "Epoch: 6, loss = -1.3152398943901065\n",
      "Epoch: 7, loss = -1.5608886748552326\n",
      "Epoch: 8, loss = -1.700271201133728\n",
      "Epoch: 9, loss = -1.7721696615219114\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.6844429994622868\n",
      "Epoch: 1, loss = 0.24569939806436494\n",
      "Epoch: 2, loss = -0.3297215614002197\n",
      "Epoch: 3, loss = -0.6650604126561022\n",
      "Epoch: 4, loss = -1.096072706911299\n",
      "Epoch: 5, loss = -1.4066926642424522\n",
      "Epoch: 6, loss = -1.6882597315642562\n",
      "Epoch: 7, loss = -1.7959023656116593\n",
      "Epoch: 8, loss = -1.886801199780571\n",
      "Epoch: 9, loss = -1.9419230769077933\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.5815719999372962\n",
      "Epoch: 1, loss = 0.04876079453000179\n",
      "Epoch: 2, loss = -0.7841748251683185\n",
      "Epoch: 3, loss = -1.3669896870851517\n",
      "Epoch: 4, loss = -1.589398004942471\n",
      "Epoch: 5, loss = -1.7097006489833195\n",
      "Epoch: 6, loss = -1.8377373384104838\n",
      "Epoch: 7, loss = -1.868712196747462\n",
      "Epoch: 8, loss = -1.9592508706781602\n",
      "Epoch: 9, loss = -1.9930766042735844\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.6772785439259478\n",
      "Epoch: 1, loss = 0.2230586311003815\n",
      "Epoch: 2, loss = -0.5181641405862236\n",
      "Epoch: 3, loss = -1.0138409435749056\n",
      "Epoch: 4, loss = -1.409897049268087\n",
      "Epoch: 5, loss = -1.5279261594017346\n",
      "Epoch: 6, loss = -1.7107144378953505\n",
      "Epoch: 7, loss = -1.7110348037547538\n",
      "Epoch: 8, loss = -1.8581967237922883\n",
      "Epoch: 9, loss = -1.9273653974135716\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.5687730602092214\n",
      "Epoch: 1, loss = 0.08176980880347802\n",
      "Epoch: 2, loss = -0.4691851449509462\n",
      "Epoch: 3, loss = -1.0108777301179037\n",
      "Epoch: 4, loss = -1.369976685278946\n",
      "Epoch: 5, loss = -1.6273830516470806\n",
      "Epoch: 6, loss = -1.6953304269247587\n",
      "Epoch: 7, loss = -1.8430942131413364\n",
      "Epoch: 8, loss = -1.8409222670727305\n",
      "Epoch: 9, loss = -1.9351463110910525\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.5899321076770623\n",
      "Epoch: 1, loss = 0.1880397022857021\n",
      "Epoch: 2, loss = -0.48490214885936855\n",
      "Epoch: 3, loss = -1.2065636689464247\n",
      "Epoch: 4, loss = -1.4816691002084152\n",
      "Epoch: 5, loss = -1.7761817226807273\n",
      "Epoch: 6, loss = -1.7186362122495973\n",
      "Epoch: 7, loss = -1.9542837159501174\n",
      "Epoch: 8, loss = -2.022448809610473\n",
      "Epoch: 9, loss = -2.0885825024710774\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 0.4934278160799294\n",
      "Epoch: 1, loss = -0.454342067351236\n",
      "Epoch: 2, loss = -1.086572420071153\n",
      "Epoch: 3, loss = -1.6329899235245058\n",
      "Epoch: 4, loss = -1.8785511640941397\n",
      "Epoch: 5, loss = -1.9640896267750685\n",
      "Epoch: 6, loss = -2.038028025451829\n",
      "Epoch: 7, loss = -2.099374018171255\n",
      "Epoch: 8, loss = -2.153067950816716\n",
      "Epoch: 9, loss = -2.2008245771422095\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 0.34655951571938426\n",
      "Epoch: 1, loss = -1.0254157314843997\n",
      "Epoch: 2, loss = -1.6632780399830902\n",
      "Epoch: 3, loss = -1.8501932108226955\n",
      "Epoch: 4, loss = -1.9890391572433361\n",
      "Epoch: 5, loss = -2.066482248113436\n",
      "Epoch: 6, loss = -2.0684404478353606\n",
      "Epoch: 7, loss = -2.1857132249895264\n",
      "Epoch: 8, loss = -2.2400170476997596\n",
      "Epoch: 9, loss = -2.2682498561985356\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 0.48237058088895607\n",
      "Epoch: 1, loss = -0.6290413777426102\n",
      "Epoch: 2, loss = -1.3761351796634047\n",
      "Epoch: 3, loss = -1.6742018732954473\n",
      "Epoch: 4, loss = -1.7776053317767728\n",
      "Epoch: 5, loss = -1.903333342031521\n",
      "Epoch: 6, loss = -1.9865611208712357\n",
      "Epoch: 7, loss = -2.052783249932176\n",
      "Epoch: 8, loss = -2.1203470552230588\n",
      "Epoch: 9, loss = -2.1736099115189376\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 0.3473233593052584\n",
      "Epoch: 1, loss = -0.7633470885784308\n",
      "Epoch: 2, loss = -1.1464032198795502\n",
      "Epoch: 3, loss = -1.7377688420169495\n",
      "Epoch: 4, loss = -1.8572820543366324\n",
      "Epoch: 5, loss = -1.9442022350781114\n",
      "Epoch: 6, loss = -2.024208375636269\n",
      "Epoch: 7, loss = -2.0947522516636288\n",
      "Epoch: 8, loss = -2.158316169591512\n",
      "Epoch: 9, loss = -2.2148750564631285\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 0.42028493575258735\n",
      "Epoch: 1, loss = -0.7435316729140193\n",
      "Epoch: 2, loss = -1.6005328205578457\n",
      "Epoch: 3, loss = -1.8236200397505482\n",
      "Epoch: 4, loss = -1.9724196581279534\n",
      "Epoch: 5, loss = -2.0747359333669433\n",
      "Epoch: 6, loss = -2.154435500502587\n",
      "Epoch: 7, loss = -2.221202258678044\n",
      "Epoch: 8, loss = -2.2787789132665184\n",
      "Epoch: 9, loss = -2.3320095258600584\n"
     ]
    }
   ],
   "source": [
    "blregegre = BaselineRegressor()\n",
    "re_bl = RegressionExperiment(regressor=blregegre, x_data=x_data, y_data=y_labels, config_al=config_al)\n",
    "yy = re_bl.run_exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "alr.compare_err_plot(frac_err_baseline=yy, frac_err_al_ens=xx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
