{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3f552d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uq360 version 0.2 needs to be installed\n",
    "#!pip install uq360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6743d0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the libraries are found\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from uq360.metrics import picp, mpiw, compute_regression_metrics\n",
    "    from uq360.metrics import UncertaintyCharacteristicsCurve as ucc\n",
    "\n",
    "    from uq360.algorithms import * \n",
    "    from uq360.algorithms.actively_learned_model import ActivelyLearnedModel\n",
    "    from uq360.algorithms.ensemble_heteroscedastic_regression import EnsembleHeteroscedasticRegression\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import torch\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "    \n",
    "    print('All the libraries are found')\n",
    "    \n",
    "except:\n",
    "    print(\"One or more libraries need to be installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d413a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '/data/MGP/TestPointsN2_CH4_H2O_000.xlsx'\n",
    "df=pd.read_excel(file_name,header=1).dropna(how='all', axis=1)\n",
    "df.drop('#',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed02bd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    \n",
    "    #xls_new = pd.ExcelFile(file_name)\n",
    "    df=pd.read_excel(file_name,header=1).dropna(how='all', axis=1)\n",
    "    df.drop('#',axis=1,inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "data = read_data(r'/data/MGP/TestPointsN2_CH4_H2O_000.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bac7b6e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63116, 36)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82295a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_col(df,col1,col2):\n",
    "    phi_surge = 0.076\n",
    "    df[col2] = 100*(df[col1]-phi_surge)/phi_surge\n",
    "    \n",
    "    return df\n",
    "\n",
    "col1 = 'phi'\n",
    "col2 = 'surge_distance_from_eq'\n",
    "data = create_new_col(data,col1,col2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3caadd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_columns(df,columns,input_columns,output_columns):\n",
    "    \n",
    "    df = df[columns]\n",
    "    \n",
    "    df = pd.concat([df[input_columns],df[output_columns]],axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9c73908",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['Pin [kPa]','Tin [K]','N [rpm]','Differential Pressure [kPa]','Total Consumed power','phi',\n",
    "         'Surge Distance','surge_distance_from_eq','GVFin','Qin [m3/s]','GVFout','Qv_out [m3/s]']\n",
    "INPUT_C = ['Pin [kPa]','Tin [K]','N [rpm]','Differential Pressure [kPa]','Total Consumed power']\n",
    "OUTPUT_C = ['surge_distance_from_eq']\n",
    "data_1 = select_columns(data,columns,INPUT_C,OUTPUT_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1886282a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63116, 6)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "81c915fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data and select the number of samples to be considered\n",
    "def scale_data(df,samples):\n",
    "    df_1 = df[0:samples]\n",
    "    df_x = df_1.iloc[:, :-1].values\n",
    "    y_labels = np.squeeze(df_1.iloc[:, -1:].values, axis=1)\n",
    "    y_labels = y_labels.reshape((-1,1))\n",
    "\n",
    "    # scale the values\n",
    "    scaler = StandardScaler()\n",
    "    scaling = scaler.fit(df_x)\n",
    "    x_data = scaling.transform(df_x)\n",
    "    \n",
    "    \n",
    "    return df, y_labels, x_data\n",
    "\n",
    "data,y_labels,x_data = scale_data(data_1,20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6e41d825",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    Offline sample and query, two mandatory arguments (and the data):\n",
    "    - Position where to start sampling\n",
    "    - Number of points to sample\n",
    "'''\n",
    "def sample_(start_index, n_points, X_data=x_data):\n",
    "    return x_data[start_index:start_index+n_points,:]\n",
    "\n",
    "def querry_(start_index, n_points, y_labels=y_labels):\n",
    "    return y_labels[start_index:start_index+n_points]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9ed8f844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define configuration for both models, regression baseline and regression with Active Learning\n",
    "def config_():\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    # define config for Heteroscedastic regression\n",
    "    config_HR = {\"num_features\": 5, \"num_hidden\": 32, \"num_outputs\": 1, \"batch_size\": 16, \"num_epochs\": 10,\n",
    "                      \"lr\": 0.001}\n",
    "    HR_kwargs = {\"model_type\":'mlp',\n",
    "                   \"config\": config_HR,\n",
    "                   \"device\": device}\n",
    "    # define config for ensemble\n",
    "    config_ensemble = {\"num_models\": 1, \n",
    "              \"batch_size\": 16,\n",
    "              \"model_kwargs\":HR_kwargs, }\n",
    "\n",
    "    ninit = 128 \n",
    "    T = 2 #4 # do not change this,\n",
    "    # define config for active learning object\n",
    "    # T = # no of iterations\n",
    "    # K = # no of uncertain points\n",
    "    #K=64\n",
    "    config_AL = {\"num_init\": 512 , \n",
    "     \"T\": 2, \n",
    "     \"K\": 16, \n",
    "     \"M\": 4, \n",
    "     \"sampling_function\": sample_, \n",
    "     \"querry_function\" : querry_,\n",
    "     \"model_function\": EnsembleHeteroscedasticRegression,\n",
    "     \"model_kwargs\": {\"model_type\":'ensembleheteroscedasticregression', \n",
    "                                                 \"config\":config_ensemble, \n",
    "                                                 \"device\":device}, }\n",
    "    \n",
    "    return config_HR,HR_kwargs,config_AL\n",
    "config_HR,HR_kwargs,config_AL = config_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7e82f363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that the data set has the good dimension\n",
    "def verify_dimension(data_x,config_AL,config_HR):\n",
    "    \n",
    "    assert(data_x.shape[0] >= config_AL[\"num_init\"] + config_AL[\"T\"]*config_AL[\"M\"]*config_AL[\"K\"])\n",
    "    assert(data_x.shape[1] == config_HR[\"num_features\"])\n",
    "    \n",
    "    return True\n",
    "\n",
    "verify_dimension(x_data,config_AL,config_HR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b7f16289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import al_model\n",
    "from al_model import ActivelyLearnedModel\n",
    "\n",
    "def baseline(config_AL):\n",
    "    # Baseline without AL\n",
    "    \n",
    "    K_train_list = [8,16, 32, 64, 128, 256,512,1024]  # T=2 better graph\n",
    "    #K_train_list = [10,20,40,80,160,320,640,1280]   # T=2 better graph\n",
    "    \n",
    "    frac_err_baseline = []\n",
    "    ninit=128\n",
    "    N_test = 512\n",
    "    device = torch.device(\"cpu\")\n",
    "    T=2\n",
    "    for i in range(len(K_train_list)):\n",
    "\n",
    "        # Update dictiorary to have no active learning and the correct amount of points\n",
    "        config_AL[\"model_kwargs\"][\"config\"][\"num_models\"] = 5\n",
    "        config_AL[\"num_init\"] = ninit + K_train_list[i] * T\n",
    "        print(config_AL[\"num_init\"])\n",
    "        config_AL[\"T\"] = 0  # no AL here\n",
    "\n",
    "        # Instantiate the class object and train the model\n",
    "        uq_model = ActivelyLearnedModel(config=config_AL, device=device, online=False)\n",
    "        uq_model = uq_model.fit() \n",
    "\n",
    "        # Create a test dataset\n",
    "        X_test = sample_(int(config_AL[\"num_init\"] + config_AL[\"T\"]*config_AL[\"M\"]*config_AL[\"K\"]), int(N_test))\n",
    "        y_test = querry_(int(config_AL[\"num_init\"] + config_AL[\"T\"]*config_AL[\"M\"]*config_AL[\"K\"]), int(N_test))\n",
    "        y_test = np.reshape(y_test, (-1,))\n",
    "        print(X_test.shape,y_test.shape)\n",
    "\n",
    "        res = uq_model.predict(X_test) \n",
    "        \n",
    "        y_test_pred = np.squeeze(res.y_mean, axis=1)\n",
    "\n",
    "        frac_err_baseline.append(np.sqrt(np.sum(np.square(y_test - y_test_pred)))/np.sqrt(np.sum(np.square(y_test))))\n",
    "        print('iteration---------',i)\n",
    "        \n",
    "    return  frac_err_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8623052b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n",
      "(144, 5) (144, 1)\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 898.4305962456598\n",
      "Epoch: 1, loss = 841.9984130859375\n",
      "Epoch: 2, loss = 791.683580186632\n",
      "Epoch: 3, loss = 746.269066704644\n",
      "Epoch: 4, loss = 704.8650546603733\n",
      "Epoch: 5, loss = 666.8555365668403\n",
      "Epoch: 6, loss = 631.6302931043837\n",
      "Epoch: 7, loss = 598.7425062391493\n",
      "Epoch: 8, loss = 567.9075080023872\n",
      "Epoch: 9, loss = 538.954857720269\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 965.1288452148438\n",
      "Epoch: 1, loss = 884.461934407552\n",
      "Epoch: 2, loss = 814.487528483073\n",
      "Epoch: 3, loss = 753.5339423285591\n",
      "Epoch: 4, loss = 700.1666734483506\n",
      "Epoch: 5, loss = 653.0705057779948\n",
      "Epoch: 6, loss = 611.1160888671875\n",
      "Epoch: 7, loss = 573.4895528157551\n",
      "Epoch: 8, loss = 539.5186021592882\n",
      "Epoch: 9, loss = 508.7084757486979\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 831.8299323187933\n",
      "Epoch: 1, loss = 782.1129998101128\n",
      "Epoch: 2, loss = 736.9740024142795\n",
      "Epoch: 3, loss = 695.5342746310763\n",
      "Epoch: 4, loss = 657.2066650390625\n",
      "Epoch: 5, loss = 621.733147515191\n",
      "Epoch: 6, loss = 588.7441914876301\n",
      "Epoch: 7, loss = 557.9911295572916\n",
      "Epoch: 8, loss = 529.1675109863281\n",
      "Epoch: 9, loss = 502.08947753906256\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1027.9260660807292\n",
      "Epoch: 1, loss = 950.8528645833334\n",
      "Epoch: 2, loss = 883.5544433593748\n",
      "Epoch: 3, loss = 824.4656304253472\n",
      "Epoch: 4, loss = 772.4295145670573\n",
      "Epoch: 5, loss = 726.2377014160156\n",
      "Epoch: 6, loss = 684.7676391601562\n",
      "Epoch: 7, loss = 647.2556762695312\n",
      "Epoch: 8, loss = 613.1069776746962\n",
      "Epoch: 9, loss = 581.7256368001302\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1037.6180962456597\n",
      "Epoch: 1, loss = 968.0659518771702\n",
      "Epoch: 2, loss = 905.4275750054253\n",
      "Epoch: 3, loss = 848.4538065592449\n",
      "Epoch: 4, loss = 796.6734619140625\n",
      "Epoch: 5, loss = 749.5461866590713\n",
      "Epoch: 6, loss = 706.4554070366754\n",
      "Epoch: 7, loss = 666.8699442545573\n",
      "Epoch: 8, loss = 630.3010796440972\n",
      "Epoch: 9, loss = 596.5063137478298\n",
      "(512, 5) (512,)\n",
      "(512, 5)\n",
      "iteration--------- 0\n",
      "160\n",
      "(160, 5) (160, 1)\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 882.3797851562499\n",
      "Epoch: 1, loss = 821.3006927490235\n",
      "Epoch: 2, loss = 767.480093383789\n",
      "Epoch: 3, loss = 719.3537475585937\n",
      "Epoch: 4, loss = 675.8295745849609\n",
      "Epoch: 5, loss = 636.0492126464844\n",
      "Epoch: 6, loss = 599.2829406738281\n",
      "Epoch: 7, loss = 565.1214477539063\n",
      "Epoch: 8, loss = 533.2882507324221\n",
      "Epoch: 9, loss = 503.5216278076172\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 951.9503479003906\n",
      "Epoch: 1, loss = 864.11103515625\n",
      "Epoch: 2, loss = 789.2086700439454\n",
      "Epoch: 3, loss = 724.9125640869141\n",
      "Epoch: 4, loss = 669.260287475586\n",
      "Epoch: 5, loss = 620.6255645751953\n",
      "Epoch: 6, loss = 577.6608245849609\n",
      "Epoch: 7, loss = 539.4036102294922\n",
      "Epoch: 8, loss = 505.15346374511716\n",
      "Epoch: 9, loss = 474.3328338623047\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 819.7843597412109\n",
      "Epoch: 1, loss = 765.702749633789\n",
      "Epoch: 2, loss = 717.1279327392579\n",
      "Epoch: 3, loss = 672.8321136474609\n",
      "Epoch: 4, loss = 632.1458892822266\n",
      "Epoch: 5, loss = 594.6893829345703\n",
      "Epoch: 6, loss = 560.0461700439454\n",
      "Epoch: 7, loss = 527.9127624511718\n",
      "Epoch: 8, loss = 497.981314086914\n",
      "Epoch: 9, loss = 470.0781463623047\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1008.7026672363281\n",
      "Epoch: 1, loss = 925.0114318847654\n",
      "Epoch: 2, loss = 853.1808563232423\n",
      "Epoch: 3, loss = 791.1416168212891\n",
      "Epoch: 4, loss = 737.2164794921877\n",
      "Epoch: 5, loss = 689.7799774169921\n",
      "Epoch: 6, loss = 647.5352020263672\n",
      "Epoch: 7, loss = 609.5377899169922\n",
      "Epoch: 8, loss = 575.0799682617188\n",
      "Epoch: 9, loss = 543.6161682128907\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1008.5006713867188\n",
      "Epoch: 1, loss = 934.7726989746094\n",
      "Epoch: 2, loss = 869.243618774414\n",
      "Epoch: 3, loss = 810.1571166992187\n",
      "Epoch: 4, loss = 756.9264739990235\n",
      "Epoch: 5, loss = 708.7861480712891\n",
      "Epoch: 6, loss = 664.9957580566406\n",
      "Epoch: 7, loss = 624.9114013671874\n",
      "Epoch: 8, loss = 588.1326202392578\n",
      "Epoch: 9, loss = 554.3052612304687\n",
      "(512, 5) (512,)\n",
      "(512, 5)\n",
      "iteration--------- 1\n",
      "192\n",
      "(192, 5) (192, 1)\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 830.2141418457031\n",
      "Epoch: 1, loss = 763.0455958048503\n",
      "Epoch: 2, loss = 705.3176651000977\n",
      "Epoch: 3, loss = 654.3970464070637\n",
      "Epoch: 4, loss = 608.7954864501953\n",
      "Epoch: 5, loss = 567.3345209757487\n",
      "Epoch: 6, loss = 529.3690083821614\n",
      "Epoch: 7, loss = 494.48532613118493\n",
      "Epoch: 8, loss = 462.3887736002604\n",
      "Epoch: 9, loss = 432.8206787109375\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 888.0977884928386\n",
      "Epoch: 1, loss = 793.4888356526692\n",
      "Epoch: 2, loss = 715.2874908447266\n",
      "Epoch: 3, loss = 649.6230748494465\n",
      "Epoch: 4, loss = 593.6974232991537\n",
      "Epoch: 5, loss = 545.4306996663412\n",
      "Epoch: 6, loss = 503.31858571370446\n",
      "Epoch: 7, loss = 466.2904713948568\n",
      "Epoch: 8, loss = 433.5046501159668\n",
      "Epoch: 9, loss = 404.3385187784831\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 764.0686645507812\n",
      "Epoch: 1, loss = 704.9092025756836\n",
      "Epoch: 2, loss = 652.8854115804036\n",
      "Epoch: 3, loss = 606.0911661783854\n",
      "Epoch: 4, loss = 563.6972325642904\n",
      "Epoch: 5, loss = 525.0839996337891\n",
      "Epoch: 6, loss = 489.7881164550781\n",
      "Epoch: 7, loss = 457.35627746582037\n",
      "Epoch: 8, loss = 427.50668970743817\n",
      "Epoch: 9, loss = 400.02251815795904\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 943.3957163492837\n",
      "Epoch: 1, loss = 852.5446955362956\n",
      "Epoch: 2, loss = 776.9278717041014\n",
      "Epoch: 3, loss = 713.0874760945638\n",
      "Epoch: 4, loss = 658.4955749511719\n",
      "Epoch: 5, loss = 610.991953531901\n",
      "Epoch: 6, loss = 569.1363830566405\n",
      "Epoch: 7, loss = 531.7985331217449\n",
      "Epoch: 8, loss = 498.1610488891602\n",
      "Epoch: 9, loss = 467.6603088378906\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 941.9976552327474\n",
      "Epoch: 1, loss = 862.2002970377605\n",
      "Epoch: 2, loss = 792.6022822062175\n",
      "Epoch: 3, loss = 730.8373667399089\n",
      "Epoch: 4, loss = 675.9477971394857\n",
      "Epoch: 5, loss = 626.8768920898439\n",
      "Epoch: 6, loss = 582.6304804484049\n",
      "Epoch: 7, loss = 542.6461461385092\n",
      "Epoch: 8, loss = 506.32565943400067\n",
      "Epoch: 9, loss = 473.28429667154944\n",
      "(512, 5) (512,)\n",
      "(512, 5)\n",
      "iteration--------- 2\n",
      "256\n",
      "(256, 5) (256, 1)\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 830.0010871887207\n",
      "Epoch: 1, loss = 742.6080837249756\n",
      "Epoch: 2, loss = 670.3105239868164\n",
      "Epoch: 3, loss = 608.4228286743164\n",
      "Epoch: 4, loss = 554.1210632324219\n",
      "Epoch: 5, loss = 505.8100757598877\n",
      "Epoch: 6, loss = 462.48266983032227\n",
      "Epoch: 7, loss = 423.71551513671875\n",
      "Epoch: 8, loss = 388.99205017089844\n",
      "Epoch: 9, loss = 357.94425678253174\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 878.6649780273438\n",
      "Epoch: 1, loss = 760.4824676513672\n",
      "Epoch: 2, loss = 667.2558975219727\n",
      "Epoch: 3, loss = 592.183162689209\n",
      "Epoch: 4, loss = 530.2799186706543\n",
      "Epoch: 5, loss = 478.3896999359131\n",
      "Epoch: 6, loss = 434.3243169784546\n",
      "Epoch: 7, loss = 396.5128402709961\n",
      "Epoch: 8, loss = 363.7860517501831\n",
      "Epoch: 9, loss = 335.34144592285156\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 761.5409183502197\n",
      "Epoch: 1, loss = 684.7040328979492\n",
      "Epoch: 2, loss = 619.0579605102539\n",
      "Epoch: 3, loss = 561.8750305175781\n",
      "Epoch: 4, loss = 511.62607765197754\n",
      "Epoch: 5, loss = 467.0589952468872\n",
      "Epoch: 6, loss = 427.3281707763672\n",
      "Epoch: 7, loss = 391.76616287231445\n",
      "Epoch: 8, loss = 360.0419912338257\n",
      "Epoch: 9, loss = 331.7398633956909\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 928.7416572570801\n",
      "Epoch: 1, loss = 815.7029991149902\n",
      "Epoch: 2, loss = 726.0212001800537\n",
      "Epoch: 3, loss = 653.3865165710449\n",
      "Epoch: 4, loss = 592.9739589691162\n",
      "Epoch: 5, loss = 541.5016117095947\n",
      "Epoch: 6, loss = 496.9061584472656\n",
      "Epoch: 7, loss = 457.73504161834717\n",
      "Epoch: 8, loss = 422.93244457244873\n",
      "Epoch: 9, loss = 391.8124055862427\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 942.1095848083496\n",
      "Epoch: 1, loss = 841.0305004119873\n",
      "Epoch: 2, loss = 755.1963348388672\n",
      "Epoch: 3, loss = 681.4940338134766\n",
      "Epoch: 4, loss = 617.6361770629883\n",
      "Epoch: 5, loss = 561.8316955566406\n",
      "Epoch: 6, loss = 512.6840400695801\n",
      "Epoch: 7, loss = 469.2267541885376\n",
      "Epoch: 8, loss = 430.89912128448486\n",
      "Epoch: 9, loss = 397.0838899612427\n",
      "(512, 5) (512,)\n",
      "(512, 5)\n",
      "iteration--------- 3\n",
      "384\n",
      "(384, 5) (384, 1)\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 814.3714866638184\n",
      "Epoch: 1, loss = 696.9803377787272\n",
      "Epoch: 2, loss = 604.7906583150228\n",
      "Epoch: 3, loss = 528.0444742838542\n",
      "Epoch: 4, loss = 462.7792396545411\n",
      "Epoch: 5, loss = 407.149689992269\n",
      "Epoch: 6, loss = 359.92482185363764\n",
      "Epoch: 7, loss = 319.8429253896078\n",
      "Epoch: 8, loss = 285.96314811706543\n",
      "Epoch: 9, loss = 257.3206208546957\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 866.9333826700847\n",
      "Epoch: 1, loss = 705.0775388081868\n",
      "Epoch: 2, loss = 589.980052947998\n",
      "Epoch: 3, loss = 503.5634282430012\n",
      "Epoch: 4, loss = 436.13356335957843\n",
      "Epoch: 5, loss = 382.30482673645025\n",
      "Epoch: 6, loss = 338.62949371337896\n",
      "Epoch: 7, loss = 302.8314628601074\n",
      "Epoch: 8, loss = 273.12732760111487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, loss = 248.1901798248291\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 759.4494895935056\n",
      "Epoch: 1, loss = 651.6540489196776\n",
      "Epoch: 2, loss = 564.7783508300781\n",
      "Epoch: 3, loss = 492.6186580657959\n",
      "Epoch: 4, loss = 431.8582611083984\n",
      "Epoch: 5, loss = 380.4859402974447\n",
      "Epoch: 6, loss = 337.16309611002606\n",
      "Epoch: 7, loss = 300.68866539001465\n",
      "Epoch: 8, loss = 269.88222694396967\n",
      "Epoch: 9, loss = 243.85606034596762\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 924.5444018046061\n",
      "Epoch: 1, loss = 769.0888074239095\n",
      "Epoch: 2, loss = 656.282283782959\n",
      "Epoch: 3, loss = 569.7670822143555\n",
      "Epoch: 4, loss = 500.2990659077962\n",
      "Epoch: 5, loss = 442.71903610229487\n",
      "Epoch: 6, loss = 394.13672383626306\n",
      "Epoch: 7, loss = 352.8475462595622\n",
      "Epoch: 8, loss = 317.7144050598144\n",
      "Epoch: 9, loss = 287.74806086222327\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 938.7463455200194\n",
      "Epoch: 1, loss = 795.9606463114419\n",
      "Epoch: 2, loss = 683.6434847513837\n",
      "Epoch: 3, loss = 592.9776382446289\n",
      "Epoch: 4, loss = 518.4671815236409\n",
      "Epoch: 5, loss = 456.6268196105957\n",
      "Epoch: 6, loss = 405.01229031880695\n",
      "Epoch: 7, loss = 361.7225964864095\n",
      "Epoch: 8, loss = 325.2109076182047\n",
      "Epoch: 9, loss = 294.30814297993976\n",
      "(512, 5) (512,)\n",
      "(512, 5)\n",
      "iteration--------- 4\n",
      "640\n",
      "(640, 5) (640, 1)\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 791.6090385437011\n",
      "Epoch: 1, loss = 611.5893508911134\n",
      "Epoch: 2, loss = 485.23695907592776\n",
      "Epoch: 3, loss = 390.98752250671384\n",
      "Epoch: 4, loss = 320.4809684753419\n",
      "Epoch: 5, loss = 267.30515518188486\n",
      "Epoch: 6, loss = 226.83657588958738\n",
      "Epoch: 7, loss = 195.53457355499265\n",
      "Epoch: 8, loss = 170.9392971038818\n",
      "Epoch: 9, loss = 151.1987102508545\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 820.0463447570801\n",
      "Epoch: 1, loss = 597.0212654113769\n",
      "Epoch: 2, loss = 461.0647373199463\n",
      "Epoch: 3, loss = 369.757117843628\n",
      "Epoch: 4, loss = 305.3272075653077\n",
      "Epoch: 5, loss = 258.24796104431147\n",
      "Epoch: 6, loss = 222.68440151214608\n",
      "Epoch: 7, loss = 194.99581241607666\n",
      "Epoch: 8, loss = 172.89254913330075\n",
      "Epoch: 9, loss = 154.86904582977292\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 724.3997039794922\n",
      "Epoch: 1, loss = 563.1204177856447\n",
      "Epoch: 2, loss = 448.51187553405754\n",
      "Epoch: 3, loss = 363.71170120239265\n",
      "Epoch: 4, loss = 300.21882095336923\n",
      "Epoch: 5, loss = 252.35262527465812\n",
      "Epoch: 6, loss = 215.728258895874\n",
      "Epoch: 7, loss = 187.2374567031861\n",
      "Epoch: 8, loss = 164.6124866485596\n",
      "Epoch: 9, loss = 146.30895004272463\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 883.4382621765138\n",
      "Epoch: 1, loss = 661.9505447387695\n",
      "Epoch: 2, loss = 525.2523948669434\n",
      "Epoch: 3, loss = 429.0421085357666\n",
      "Epoch: 4, loss = 356.6852142333985\n",
      "Epoch: 5, loss = 300.9961208343506\n",
      "Epoch: 6, loss = 257.81184844970704\n",
      "Epoch: 7, loss = 223.97218208312995\n",
      "Epoch: 8, loss = 196.94518070220946\n",
      "Epoch: 9, loss = 175.00325775146496\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 904.6578155517578\n",
      "Epoch: 1, loss = 697.4481658935547\n",
      "Epoch: 2, loss = 552.6929893493652\n",
      "Epoch: 3, loss = 447.2699340820314\n",
      "Epoch: 4, loss = 368.8313926696777\n",
      "Epoch: 5, loss = 309.88014602661127\n",
      "Epoch: 6, loss = 264.9230876922607\n",
      "Epoch: 7, loss = 229.98515415191653\n",
      "Epoch: 8, loss = 202.29141139984128\n",
      "Epoch: 9, loss = 179.92970275878906\n",
      "(512, 5) (512,)\n",
      "(512, 5)\n",
      "iteration--------- 5\n",
      "1152\n",
      "(1152, 5) (1152, 1)\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 714.3719656202526\n",
      "Epoch: 1, loss = 466.40995640224867\n",
      "Epoch: 2, loss = 321.48490672641344\n",
      "Epoch: 3, loss = 234.11640241410996\n",
      "Epoch: 4, loss = 179.78624852498382\n",
      "Epoch: 5, loss = 143.90060096316864\n",
      "Epoch: 6, loss = 118.75793663660687\n",
      "Epoch: 7, loss = 100.18058586120607\n",
      "Epoch: 8, loss = 85.91735871632889\n",
      "Epoch: 9, loss = 74.6203418837653\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 726.2334632873536\n",
      "Epoch: 1, loss = 443.2858303917779\n",
      "Epoch: 2, loss = 306.67064497205934\n",
      "Epoch: 3, loss = 229.49844763014045\n",
      "Epoch: 4, loss = 181.22024695078522\n",
      "Epoch: 5, loss = 148.40546396043567\n",
      "Epoch: 6, loss = 124.64674811893038\n",
      "Epoch: 7, loss = 106.60541688071359\n",
      "Epoch: 8, loss = 92.43193440967136\n",
      "Epoch: 9, loss = 81.01917129092745\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 658.2050730387369\n",
      "Epoch: 1, loss = 432.95592837863506\n",
      "Epoch: 2, loss = 302.3183858659532\n",
      "Epoch: 3, loss = 223.410387357076\n",
      "Epoch: 4, loss = 173.64641496870263\n",
      "Epoch: 5, loss = 140.31650363074408\n",
      "Epoch: 6, loss = 116.56492349836564\n",
      "Epoch: 7, loss = 98.80142095353872\n",
      "Epoch: 8, loss = 85.04599830839368\n",
      "Epoch: 9, loss = 74.10173691643608\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 785.376831478543\n",
      "Epoch: 1, loss = 503.39186583624945\n",
      "Epoch: 2, loss = 356.15059132046184\n",
      "Epoch: 3, loss = 265.17378955417206\n",
      "Epoch: 4, loss = 206.75044536590585\n",
      "Epoch: 5, loss = 167.230518023173\n",
      "Epoch: 6, loss = 139.0841568840875\n",
      "Epoch: 7, loss = 118.1184337933858\n",
      "Epoch: 8, loss = 101.9007369147407\n",
      "Epoch: 9, loss = 89.00455930497914\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 811.2038091023765\n",
      "Epoch: 1, loss = 527.7591843075221\n",
      "Epoch: 2, loss = 367.5358698103164\n",
      "Epoch: 3, loss = 271.71899414062494\n",
      "Epoch: 4, loss = 211.38875908321813\n",
      "Epoch: 5, loss = 170.9649148517185\n",
      "Epoch: 6, loss = 142.30714718500775\n",
      "Epoch: 7, loss = 120.99927526050146\n",
      "Epoch: 8, loss = 104.5452591578166\n",
      "Epoch: 9, loss = 91.48137235641478\n",
      "(512, 5) (512,)\n",
      "(512, 5)\n",
      "iteration--------- 6\n",
      "2176\n",
      "(2176, 5) (2176, 1)\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 602.6712411992689\n",
      "Epoch: 1, loss = 292.48874894310444\n",
      "Epoch: 2, loss = 172.25754176869108\n",
      "Epoch: 3, loss = 116.96004856334012\n",
      "Epoch: 4, loss = 86.00293204363652\n",
      "Epoch: 5, loss = 66.3137024009929\n",
      "Epoch: 6, loss = 52.8139505526599\n",
      "Epoch: 7, loss = 43.14551068754757\n",
      "Epoch: 8, loss = 36.045953399994794\n",
      "Epoch: 9, loss = 30.74814620438745\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 596.5654421413647\n",
      "Epoch: 1, loss = 281.1441314921661\n",
      "Epoch: 2, loss = 173.8652252870447\n",
      "Epoch: 3, loss = 122.41802066915182\n",
      "Epoch: 4, loss = 92.14931516086358\n",
      "Epoch: 5, loss = 72.18568225467904\n",
      "Epoch: 6, loss = 58.13629834792192\n",
      "Epoch: 7, loss = 47.86076970661389\n",
      "Epoch: 8, loss = 40.17983282313627\n",
      "Epoch: 9, loss = 34.365427508073715\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 555.5941415674546\n",
      "Epoch: 1, loss = 275.98758865805223\n",
      "Epoch: 2, loss = 166.08216106190395\n",
      "Epoch: 3, loss = 114.28926507164448\n",
      "Epoch: 4, loss = 84.70131360783299\n",
      "Epoch: 5, loss = 65.64137830453761\n",
      "Epoch: 6, loss = 52.46810554055604\n",
      "Epoch: 7, loss = 42.98568178625667\n",
      "Epoch: 8, loss = 35.99575861762552\n",
      "Epoch: 9, loss = 30.777855234987587\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 657.1485280429621\n",
      "Epoch: 1, loss = 326.5974994547224\n",
      "Epoch: 2, loss = 197.277276319616\n",
      "Epoch: 3, loss = 135.6378624579486\n",
      "Epoch: 4, loss = 100.77938323862412\n",
      "Epoch: 5, loss = 78.49128784852866\n",
      "Epoch: 6, loss = 63.066672423306656\n",
      "Epoch: 7, loss = 51.857846358243144\n",
      "Epoch: 8, loss = 43.471807003021226\n",
      "Epoch: 9, loss = 37.07759889434363\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 686.8451929653395\n",
      "Epoch: 1, loss = 336.0442002801336\n",
      "Epoch: 2, loss = 202.23399094974297\n",
      "Epoch: 3, loss = 139.6685304641724\n",
      "Epoch: 4, loss = 104.27304511911728\n",
      "Epoch: 5, loss = 81.56132430188796\n",
      "Epoch: 6, loss = 65.82289958000187\n",
      "Epoch: 7, loss = 54.37683193823868\n",
      "Epoch: 8, loss = 45.78044532327089\n",
      "Epoch: 9, loss = 39.195557369905345\n",
      "(512, 5) (512,)\n",
      "(512, 5)\n",
      "iteration--------- 7\n"
     ]
    }
   ],
   "source": [
    "errors_baseline=baseline(config_AL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9e3d9025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9832069153690315,\n",
       " 0.981279078934709,\n",
       " 0.9772570872177739,\n",
       " 0.968153441778212,\n",
       " 0.947666175885187,\n",
       " 0.9032063961193391,\n",
       " 0.8050311975395468,\n",
       " 0.6547413422749819]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "70b36fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import al_model\n",
    "from al_model import ActivelyLearnedModel\n",
    "\n",
    "\n",
    "def with_al(config_AL):\n",
    "    # AL, ensemble of 5 NNs\n",
    "    frac_err_AL_ens = []\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "    #checkpoint=keras.callbacks.ModelCheckpoint(\"mcp_AL.h5\", save_best_only=True)\n",
    "    K_train_list = [8,16, 32, 64, 128, 256,512,1024] # make it as a global variable\n",
    "    #K_train_list = [10,20,40,80,160,320,640,1280]\n",
    "    N_test = 512\n",
    "    ninit=128\n",
    "    T=2\n",
    "    for i in range(len(K_train_list)):\n",
    "\n",
    "        # Update dictiorary for the correct amount of points\n",
    "        config_AL[\"model_kwargs\"][\"config\"][\"num_models\"] = 5\n",
    "        config_AL[\"num_init\"] = ninit\n",
    "        config_AL[\"K\"] = K_train_list[i]\n",
    "        config_AL[\"M\"] = 4\n",
    "        config_AL[\"T\"] = T\n",
    "\n",
    "        # Instantiate the class object and train the model\n",
    "        uq_model = ActivelyLearnedModel(config=config_AL, device=device, online=False)\n",
    "        uq_model = uq_model.fit()\n",
    "\n",
    "        # Create a test dataset\n",
    "        X_test = sample_(int(config_AL[\"num_init\"] + config_AL[\"T\"]*config_AL[\"M\"]*config_AL[\"K\"]), int(N_test))\n",
    "        y_test = querry_(int(config_AL[\"num_init\"] + config_AL[\"T\"]*config_AL[\"M\"]*config_AL[\"K\"]), int(N_test))\n",
    "        y_test = np.reshape(y_test, (-1,))\n",
    "        \n",
    "        print('test set size is', X_test.shape)\n",
    "\n",
    "        res = uq_model.predict(X_test)\n",
    "        y_test_pred = np.squeeze(res.y_mean, axis=1)\n",
    "\n",
    "        frac_err_AL_ens.append(np.sqrt(np.sum(np.square(y_test - y_test_pred)))/np.sqrt(np.sum(np.square(y_test))))\n",
    "        \n",
    "    return frac_err_AL_ens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e285fff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 5) (128, 1)\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 1191.2982559204102\n",
      "Epoch: 1, loss = 1112.3001022338867\n",
      "Epoch: 2, loss = 1042.4789962768555\n",
      "Epoch: 3, loss = 979.9024047851562\n",
      "Epoch: 4, loss = 923.5063323974609\n",
      "Epoch: 5, loss = 872.1197929382324\n",
      "Epoch: 6, loss = 824.9293441772461\n",
      "Epoch: 7, loss = 781.4889907836914\n",
      "Epoch: 8, loss = 741.2865028381348\n",
      "Epoch: 9, loss = 703.913215637207\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 1466.8523406982422\n",
      "Epoch: 1, loss = 1361.7539520263672\n",
      "Epoch: 2, loss = 1271.8415603637695\n",
      "Epoch: 3, loss = 1194.2659530639648\n",
      "Epoch: 4, loss = 1126.8563232421875\n",
      "Epoch: 5, loss = 1067.5989875793457\n",
      "Epoch: 6, loss = 1015.0295562744141\n",
      "Epoch: 7, loss = 967.7179412841797\n",
      "Epoch: 8, loss = 924.6259880065918\n",
      "Epoch: 9, loss = 885.1318016052246\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 1071.7694931030273\n",
      "Epoch: 1, loss = 1010.6565399169922\n",
      "Epoch: 2, loss = 955.9369773864746\n",
      "Epoch: 3, loss = 906.2810859680176\n",
      "Epoch: 4, loss = 860.9972839355469\n",
      "Epoch: 5, loss = 819.4949264526367\n",
      "Epoch: 6, loss = 781.1721649169922\n",
      "Epoch: 7, loss = 745.5577507019043\n",
      "Epoch: 8, loss = 712.1323394775391\n",
      "Epoch: 9, loss = 680.5844497680664\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1293.5209579467773\n",
      "Epoch: 1, loss = 1194.2896728515625\n",
      "Epoch: 2, loss = 1108.2813034057617\n",
      "Epoch: 3, loss = 1032.7312469482422\n",
      "Epoch: 4, loss = 966.1247100830078\n",
      "Epoch: 5, loss = 907.0381317138672\n",
      "Epoch: 6, loss = 854.1164855957031\n",
      "Epoch: 7, loss = 806.3752632141113\n",
      "Epoch: 8, loss = 762.9778175354004\n",
      "Epoch: 9, loss = 723.2085380554199\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 898.7104911804199\n",
      "Epoch: 1, loss = 840.0486068725586\n",
      "Epoch: 2, loss = 786.9890327453613\n",
      "Epoch: 3, loss = 738.5519905090332\n",
      "Epoch: 4, loss = 694.3204956054688\n",
      "Epoch: 5, loss = 653.8603439331055\n",
      "Epoch: 6, loss = 616.7093925476074\n",
      "Epoch: 7, loss = 582.5687713623047\n",
      "Epoch: 8, loss = 551.1674556732178\n",
      "Epoch: 9, loss = 522.2213726043701\n",
      "(8, 5) (8, 1)\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 642.2740207248264\n",
      "Epoch: 1, loss = 597.7061598036025\n",
      "Epoch: 2, loss = 558.1615736219618\n",
      "Epoch: 3, loss = 522.6170383029514\n",
      "Epoch: 4, loss = 490.5236070421007\n",
      "Epoch: 5, loss = 461.4054802788629\n",
      "Epoch: 6, loss = 434.8407779269748\n",
      "Epoch: 7, loss = 410.6040496826172\n",
      "Epoch: 8, loss = 388.40574306911896\n",
      "Epoch: 9, loss = 368.0266859266493\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 826.9938422309028\n",
      "Epoch: 1, loss = 776.7212388780382\n",
      "Epoch: 2, loss = 732.0675387912327\n",
      "Epoch: 3, loss = 691.8889702690972\n",
      "Epoch: 4, loss = 655.45607164171\n",
      "Epoch: 5, loss = 622.0247667100695\n",
      "Epoch: 6, loss = 590.9833374023438\n",
      "Epoch: 7, loss = 562.0288865831163\n",
      "Epoch: 8, loss = 534.8567945692274\n",
      "Epoch: 9, loss = 509.297600640191\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 636.9654914008246\n",
      "Epoch: 1, loss = 599.3393385145399\n",
      "Epoch: 2, loss = 565.1544901529946\n",
      "Epoch: 3, loss = 533.6031290690104\n",
      "Epoch: 4, loss = 504.3934699164497\n",
      "Epoch: 5, loss = 477.32486300998255\n",
      "Epoch: 6, loss = 452.1637912326389\n",
      "Epoch: 7, loss = 428.7252214219835\n",
      "Epoch: 8, loss = 406.89204576280383\n",
      "Epoch: 9, loss = 386.47224595811633\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 667.5636054144966\n",
      "Epoch: 1, loss = 617.8902486165364\n",
      "Epoch: 2, loss = 574.1631605360244\n",
      "Epoch: 3, loss = 535.240966796875\n",
      "Epoch: 4, loss = 500.4444376627604\n",
      "Epoch: 5, loss = 469.2060004340278\n",
      "Epoch: 6, loss = 441.04094780815973\n",
      "Epoch: 7, loss = 415.56001451280383\n",
      "Epoch: 8, loss = 392.40450880262586\n",
      "Epoch: 9, loss = 371.3107689751519\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 475.81673007541235\n",
      "Epoch: 1, loss = 442.5271911621094\n",
      "Epoch: 2, loss = 412.84927537706164\n",
      "Epoch: 3, loss = 386.2031487358941\n",
      "Epoch: 4, loss = 362.27966986762146\n",
      "Epoch: 5, loss = 340.750729031033\n",
      "Epoch: 6, loss = 321.3326026068794\n",
      "Epoch: 7, loss = 303.75431654188367\n",
      "Epoch: 8, loss = 287.7774980333116\n",
      "Epoch: 9, loss = 273.1958685980903\n",
      "(136, 5) (136, 1)\n",
      "(8, 5) (8, 1)\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 350.71158515082465\n",
      "Epoch: 1, loss = 327.73841857910156\n",
      "Epoch: 2, loss = 306.9007347954644\n",
      "Epoch: 3, loss = 287.86421034071185\n",
      "Epoch: 4, loss = 270.5433349609375\n",
      "Epoch: 5, loss = 254.79326375325522\n",
      "Epoch: 6, loss = 240.49075995551212\n",
      "Epoch: 7, loss = 227.4739702012804\n",
      "Epoch: 8, loss = 215.59721883138022\n",
      "Epoch: 9, loss = 204.75418090820312\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 483.00109354654944\n",
      "Epoch: 1, loss = 454.8005710177951\n",
      "Epoch: 2, loss = 428.65979003906244\n",
      "Epoch: 3, loss = 404.3040296766493\n",
      "Epoch: 4, loss = 381.6342976888021\n",
      "Epoch: 5, loss = 360.5395626491971\n",
      "Epoch: 6, loss = 340.9604288736979\n",
      "Epoch: 7, loss = 322.80834452311194\n",
      "Epoch: 8, loss = 305.9490102132161\n",
      "Epoch: 9, loss = 290.29179212782117\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 365.3606465657552\n",
      "Epoch: 1, loss = 343.3991360134549\n",
      "Epoch: 2, loss = 323.2327457004123\n",
      "Epoch: 3, loss = 304.6714697943794\n",
      "Epoch: 4, loss = 287.6309339735243\n",
      "Epoch: 5, loss = 272.0244598388672\n",
      "Epoch: 6, loss = 257.7069668240017\n",
      "Epoch: 7, loss = 244.56833394368485\n",
      "Epoch: 8, loss = 232.4979248046875\n",
      "Epoch: 9, loss = 221.37962341308594\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 351.0206536187066\n",
      "Epoch: 1, loss = 327.18616061740454\n",
      "Epoch: 2, loss = 305.8584289550781\n",
      "Epoch: 3, loss = 286.7089301215278\n",
      "Epoch: 4, loss = 269.5156758626302\n",
      "Epoch: 5, loss = 254.02209472656253\n",
      "Epoch: 6, loss = 240.04332139756943\n",
      "Epoch: 7, loss = 227.37479485405817\n",
      "Epoch: 8, loss = 215.82971360948352\n",
      "Epoch: 9, loss = 205.27071295844183\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 257.72434319390186\n",
      "Epoch: 1, loss = 241.24337005615232\n",
      "Epoch: 2, loss = 226.41048685709637\n",
      "Epoch: 3, loss = 213.00531090630426\n",
      "Epoch: 4, loss = 200.90858289930551\n",
      "Epoch: 5, loss = 189.9815936618381\n",
      "Epoch: 6, loss = 180.07950592041013\n",
      "Epoch: 7, loss = 171.0820041232639\n",
      "Epoch: 8, loss = 162.88170454237195\n",
      "Epoch: 9, loss = 155.38186306423614\n",
      "(144, 5) (144, 1)\n",
      "test set size is (512, 5)\n",
      "(128, 5) (128, 1)\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 897.685188293457\n",
      "Epoch: 1, loss = 848.5712051391602\n",
      "Epoch: 2, loss = 804.253589630127\n",
      "Epoch: 3, loss = 763.6846466064453\n",
      "Epoch: 4, loss = 726.2823677062988\n",
      "Epoch: 5, loss = 691.5584869384766\n",
      "Epoch: 6, loss = 659.1344413757324\n",
      "Epoch: 7, loss = 628.6419715881348\n",
      "Epoch: 8, loss = 599.8716239929199\n",
      "Epoch: 9, loss = 572.6521377563477\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 972.3930740356445\n",
      "Epoch: 1, loss = 899.7449493408203\n",
      "Epoch: 2, loss = 835.8917846679688\n",
      "Epoch: 3, loss = 779.40576171875\n",
      "Epoch: 4, loss = 729.3008918762207\n",
      "Epoch: 5, loss = 684.5720558166504\n",
      "Epoch: 6, loss = 644.3593559265137\n",
      "Epoch: 7, loss = 607.9808044433594\n",
      "Epoch: 8, loss = 574.887020111084\n",
      "Epoch: 9, loss = 544.6257514953613\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 837.1575355529785\n",
      "Epoch: 1, loss = 793.044319152832\n",
      "Epoch: 2, loss = 752.7186813354492\n",
      "Epoch: 3, loss = 715.3726119995117\n",
      "Epoch: 4, loss = 680.4784660339355\n",
      "Epoch: 5, loss = 647.8640251159668\n",
      "Epoch: 6, loss = 617.2914085388184\n",
      "Epoch: 7, loss = 588.5320930480957\n",
      "Epoch: 8, loss = 561.4288597106934\n",
      "Epoch: 9, loss = 535.7296600341797\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1028.651138305664\n",
      "Epoch: 1, loss = 960.0750274658203\n",
      "Epoch: 2, loss = 899.5022125244141\n",
      "Epoch: 3, loss = 845.526985168457\n",
      "Epoch: 4, loss = 797.3591690063477\n",
      "Epoch: 5, loss = 754.1536026000977\n",
      "Epoch: 6, loss = 715.0536499023438\n",
      "Epoch: 7, loss = 679.363208770752\n",
      "Epoch: 8, loss = 646.6425476074219\n",
      "Epoch: 9, loss = 616.4611358642578\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1040.047607421875\n",
      "Epoch: 1, loss = 977.7626686096191\n",
      "Epoch: 2, loss = 921.2749824523926\n",
      "Epoch: 3, loss = 869.441047668457\n",
      "Epoch: 4, loss = 821.8450469970703\n",
      "Epoch: 5, loss = 778.1071014404297\n",
      "Epoch: 6, loss = 737.8054580688477\n",
      "Epoch: 7, loss = 700.5327453613281\n",
      "Epoch: 8, loss = 665.9112129211426\n",
      "Epoch: 9, loss = 633.6882514953613\n",
      "(16, 5) (16, 1)\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 546.778086344401\n",
      "Epoch: 1, loss = 512.9224751790365\n",
      "Epoch: 2, loss = 482.071287367079\n",
      "Epoch: 3, loss = 453.79540167914496\n",
      "Epoch: 4, loss = 427.850850423177\n",
      "Epoch: 5, loss = 404.0159878200955\n",
      "Epoch: 6, loss = 382.0772501627604\n",
      "Epoch: 7, loss = 361.8799760606554\n",
      "Epoch: 8, loss = 343.25164455837677\n",
      "Epoch: 9, loss = 326.06746588812933\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 509.95542229546436\n",
      "Epoch: 1, loss = 472.0452609592014\n",
      "Epoch: 2, loss = 438.3389010959201\n",
      "Epoch: 3, loss = 408.27873908148865\n",
      "Epoch: 4, loss = 381.50012715657556\n",
      "Epoch: 5, loss = 357.5828586154514\n",
      "Epoch: 6, loss = 336.1678398980034\n",
      "Epoch: 7, loss = 316.90846252441406\n",
      "Epoch: 8, loss = 299.5128004286025\n",
      "Epoch: 9, loss = 283.74316067165796\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 502.2678900824653\n",
      "Epoch: 1, loss = 471.882820977105\n",
      "Epoch: 2, loss = 443.8213331434462\n",
      "Epoch: 3, loss = 417.7460496690539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, loss = 393.5498962402344\n",
      "Epoch: 5, loss = 371.15004984537757\n",
      "Epoch: 6, loss = 350.43325127495655\n",
      "Epoch: 7, loss = 331.28615993923614\n",
      "Epoch: 8, loss = 313.61203850640186\n",
      "Epoch: 9, loss = 297.29043070475257\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 574.3634304470487\n",
      "Epoch: 1, loss = 536.790276421441\n",
      "Epoch: 2, loss = 503.0679083930121\n",
      "Epoch: 3, loss = 472.5042673746745\n",
      "Epoch: 4, loss = 444.76968044704864\n",
      "Epoch: 5, loss = 419.5035756429037\n",
      "Epoch: 6, loss = 396.4086473253038\n",
      "Epoch: 7, loss = 375.25295003255206\n",
      "Epoch: 8, loss = 355.8215637207031\n",
      "Epoch: 9, loss = 337.9070705837674\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 590.381096733941\n",
      "Epoch: 1, loss = 552.4956207275391\n",
      "Epoch: 2, loss = 518.2287919786241\n",
      "Epoch: 3, loss = 486.99664984809027\n",
      "Epoch: 4, loss = 458.55838012695307\n",
      "Epoch: 5, loss = 432.6700337727865\n",
      "Epoch: 6, loss = 409.028074476454\n",
      "Epoch: 7, loss = 387.3625962999132\n",
      "Epoch: 8, loss = 367.4210425482855\n",
      "Epoch: 9, loss = 348.98651292588977\n",
      "(144, 5) (144, 1)\n",
      "(16, 5) (16, 1)\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 298.9377014160156\n",
      "Epoch: 1, loss = 278.84298248291014\n",
      "Epoch: 2, loss = 260.68737945556643\n",
      "Epoch: 3, loss = 244.26404724121093\n",
      "Epoch: 4, loss = 229.4433151245117\n",
      "Epoch: 5, loss = 216.05224304199217\n",
      "Epoch: 6, loss = 203.95037231445315\n",
      "Epoch: 7, loss = 192.96564178466795\n",
      "Epoch: 8, loss = 182.9579246520996\n",
      "Epoch: 9, loss = 173.81512908935548\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 259.09236907958984\n",
      "Epoch: 1, loss = 240.1509246826172\n",
      "Epoch: 2, loss = 223.37478103637693\n",
      "Epoch: 3, loss = 208.4898376464844\n",
      "Epoch: 4, loss = 195.29696044921874\n",
      "Epoch: 5, loss = 183.57200012207028\n",
      "Epoch: 6, loss = 173.09460525512694\n",
      "Epoch: 7, loss = 163.6881576538086\n",
      "Epoch: 8, loss = 155.19478149414064\n",
      "Epoch: 9, loss = 147.4829299926758\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 269.1731002807618\n",
      "Epoch: 1, loss = 250.74129409790038\n",
      "Epoch: 2, loss = 234.20644378662112\n",
      "Epoch: 3, loss = 219.3115982055664\n",
      "Epoch: 4, loss = 205.93755416870118\n",
      "Epoch: 5, loss = 193.89976425170894\n",
      "Epoch: 6, loss = 183.067219543457\n",
      "Epoch: 7, loss = 173.30059661865235\n",
      "Epoch: 8, loss = 164.44557647705076\n",
      "Epoch: 9, loss = 156.38230667114257\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 308.3480911254882\n",
      "Epoch: 1, loss = 287.5042297363281\n",
      "Epoch: 2, loss = 268.8398620605468\n",
      "Epoch: 3, loss = 251.99540405273436\n",
      "Epoch: 4, loss = 236.76549377441407\n",
      "Epoch: 5, loss = 223.00583343505855\n",
      "Epoch: 6, loss = 210.54937820434571\n",
      "Epoch: 7, loss = 199.21724700927737\n",
      "Epoch: 8, loss = 188.86420974731448\n",
      "Epoch: 9, loss = 179.37871704101565\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 322.7483612060547\n",
      "Epoch: 1, loss = 300.6747268676757\n",
      "Epoch: 2, loss = 280.6879722595215\n",
      "Epoch: 3, loss = 262.5682678222656\n",
      "Epoch: 4, loss = 246.1977767944336\n",
      "Epoch: 5, loss = 231.405020904541\n",
      "Epoch: 6, loss = 218.02385482788085\n",
      "Epoch: 7, loss = 205.87349319458005\n",
      "Epoch: 8, loss = 194.8056541442871\n",
      "Epoch: 9, loss = 184.69021911621095\n",
      "(160, 5) (160, 1)\n",
      "test set size is (512, 5)\n",
      "(128, 5) (128, 1)\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 897.685188293457\n",
      "Epoch: 1, loss = 848.5712051391602\n",
      "Epoch: 2, loss = 804.253589630127\n",
      "Epoch: 3, loss = 763.6846466064453\n",
      "Epoch: 4, loss = 726.2823677062988\n",
      "Epoch: 5, loss = 691.5584869384766\n",
      "Epoch: 6, loss = 659.1344413757324\n",
      "Epoch: 7, loss = 628.6419715881348\n",
      "Epoch: 8, loss = 599.8716239929199\n",
      "Epoch: 9, loss = 572.6521377563477\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 972.3930740356445\n",
      "Epoch: 1, loss = 899.7449493408203\n",
      "Epoch: 2, loss = 835.8917846679688\n",
      "Epoch: 3, loss = 779.40576171875\n",
      "Epoch: 4, loss = 729.3008918762207\n",
      "Epoch: 5, loss = 684.5720558166504\n",
      "Epoch: 6, loss = 644.3593559265137\n",
      "Epoch: 7, loss = 607.9808044433594\n",
      "Epoch: 8, loss = 574.887020111084\n",
      "Epoch: 9, loss = 544.6257514953613\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 837.1575355529785\n",
      "Epoch: 1, loss = 793.044319152832\n",
      "Epoch: 2, loss = 752.7186813354492\n",
      "Epoch: 3, loss = 715.3726119995117\n",
      "Epoch: 4, loss = 680.4784660339355\n",
      "Epoch: 5, loss = 647.8640251159668\n",
      "Epoch: 6, loss = 617.2914085388184\n",
      "Epoch: 7, loss = 588.5320930480957\n",
      "Epoch: 8, loss = 561.4288597106934\n",
      "Epoch: 9, loss = 535.7296600341797\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1028.651138305664\n",
      "Epoch: 1, loss = 960.0750274658203\n",
      "Epoch: 2, loss = 899.5022125244141\n",
      "Epoch: 3, loss = 845.526985168457\n",
      "Epoch: 4, loss = 797.3591690063477\n",
      "Epoch: 5, loss = 754.1536026000977\n",
      "Epoch: 6, loss = 715.0536499023438\n",
      "Epoch: 7, loss = 679.363208770752\n",
      "Epoch: 8, loss = 646.6425476074219\n",
      "Epoch: 9, loss = 616.4611358642578\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1040.047607421875\n",
      "Epoch: 1, loss = 977.7626686096191\n",
      "Epoch: 2, loss = 921.2749824523926\n",
      "Epoch: 3, loss = 869.441047668457\n",
      "Epoch: 4, loss = 821.8450469970703\n",
      "Epoch: 5, loss = 778.1071014404297\n",
      "Epoch: 6, loss = 737.8054580688477\n",
      "Epoch: 7, loss = 700.5327453613281\n",
      "Epoch: 8, loss = 665.9112129211426\n",
      "Epoch: 9, loss = 633.6882514953613\n",
      "(32, 5) (32, 1)\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 520.0680297851563\n",
      "Epoch: 1, loss = 484.5566497802734\n",
      "Epoch: 2, loss = 452.75812988281245\n",
      "Epoch: 3, loss = 423.94871215820314\n",
      "Epoch: 4, loss = 397.7447143554687\n",
      "Epoch: 5, loss = 373.8668899536133\n",
      "Epoch: 6, loss = 352.0857055664062\n",
      "Epoch: 7, loss = 332.18672943115234\n",
      "Epoch: 8, loss = 313.9619430541992\n",
      "Epoch: 9, loss = 297.2106674194336\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 483.0708053588868\n",
      "Epoch: 1, loss = 443.887451171875\n",
      "Epoch: 2, loss = 409.6558776855469\n",
      "Epoch: 3, loss = 379.51336975097655\n",
      "Epoch: 4, loss = 352.93102111816415\n",
      "Epoch: 5, loss = 329.4156692504883\n",
      "Epoch: 6, loss = 308.52370758056645\n",
      "Epoch: 7, loss = 289.84962005615233\n",
      "Epoch: 8, loss = 273.09471588134767\n",
      "Epoch: 9, loss = 257.98626708984375\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 470.07803039550777\n",
      "Epoch: 1, loss = 438.64626922607425\n",
      "Epoch: 2, loss = 410.1086059570312\n",
      "Epoch: 3, loss = 383.9490692138672\n",
      "Epoch: 4, loss = 359.95918731689454\n",
      "Epoch: 5, loss = 337.9562088012695\n",
      "Epoch: 6, loss = 317.78610992431635\n",
      "Epoch: 7, loss = 299.3179275512695\n",
      "Epoch: 8, loss = 282.4038970947266\n",
      "Epoch: 9, loss = 266.88336181640625\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 547.3628845214844\n",
      "Epoch: 1, loss = 508.23102722167977\n",
      "Epoch: 2, loss = 473.6742248535156\n",
      "Epoch: 3, loss = 442.6464721679687\n",
      "Epoch: 4, loss = 414.70873718261726\n",
      "Epoch: 5, loss = 389.4087982177735\n",
      "Epoch: 6, loss = 366.44801788330074\n",
      "Epoch: 7, loss = 345.50502929687497\n",
      "Epoch: 8, loss = 326.35782012939455\n",
      "Epoch: 9, loss = 308.817953491211\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 564.4319244384765\n",
      "Epoch: 1, loss = 525.0384399414063\n",
      "Epoch: 2, loss = 489.9600646972657\n",
      "Epoch: 3, loss = 458.26536254882814\n",
      "Epoch: 4, loss = 429.60711975097655\n",
      "Epoch: 5, loss = 403.67802734375005\n",
      "Epoch: 6, loss = 380.09545288085934\n",
      "Epoch: 7, loss = 358.5696228027344\n",
      "Epoch: 8, loss = 338.8550537109375\n",
      "Epoch: 9, loss = 320.74046783447267\n",
      "(160, 5) (160, 1)\n",
      "(32, 5) (32, 1)\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 285.89461135864263\n",
      "Epoch: 1, loss = 263.2916539510091\n",
      "Epoch: 2, loss = 243.2954756418864\n",
      "Epoch: 3, loss = 225.5959097544352\n",
      "Epoch: 4, loss = 209.93498293558758\n",
      "Epoch: 5, loss = 196.04364140828451\n",
      "Epoch: 6, loss = 183.66510454813638\n",
      "Epoch: 7, loss = 172.57591629028323\n",
      "Epoch: 8, loss = 162.59749348958331\n",
      "Epoch: 9, loss = 153.58477274576822\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 244.89122517903644\n",
      "Epoch: 1, loss = 223.90895525614422\n",
      "Epoch: 2, loss = 205.78371810913086\n",
      "Epoch: 3, loss = 190.09918085734049\n",
      "Epoch: 4, loss = 176.4950116475423\n",
      "Epoch: 5, loss = 164.61273829142252\n",
      "Epoch: 6, loss = 154.14583905537924\n",
      "Epoch: 7, loss = 144.85251744588217\n",
      "Epoch: 8, loss = 136.53582382202148\n",
      "Epoch: 9, loss = 129.04795392354328\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 256.2425727844238\n",
      "Epoch: 1, loss = 235.48211669921875\n",
      "Epoch: 2, loss = 217.25862503051755\n",
      "Epoch: 3, loss = 201.25100708007812\n",
      "Epoch: 4, loss = 187.2232500712077\n",
      "Epoch: 5, loss = 174.85265096028647\n",
      "Epoch: 6, loss = 163.88300387064615\n",
      "Epoch: 7, loss = 154.10037485758463\n",
      "Epoch: 8, loss = 145.3355566660563\n",
      "Epoch: 9, loss = 137.43961652119958\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 297.64301554361975\n",
      "Epoch: 1, loss = 273.3541615804036\n",
      "Epoch: 2, loss = 252.114138285319\n",
      "Epoch: 3, loss = 233.40517679850262\n",
      "Epoch: 4, loss = 216.91134961446124\n",
      "Epoch: 5, loss = 202.34232966105142\n",
      "Epoch: 6, loss = 189.36559232076013\n",
      "Epoch: 7, loss = 177.73398017883298\n",
      "Epoch: 8, loss = 167.25813420613608\n",
      "Epoch: 9, loss = 157.7732098897298\n",
      "\n",
      "Training model 4\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss = 297.65268325805664\n",
      "Epoch: 1, loss = 273.71052296956384\n",
      "Epoch: 2, loss = 252.51088841756183\n",
      "Epoch: 3, loss = 233.72858937581378\n",
      "Epoch: 4, loss = 217.10978635152176\n",
      "Epoch: 5, loss = 202.34440549214685\n",
      "Epoch: 6, loss = 189.16579818725583\n",
      "Epoch: 7, loss = 177.36699930826822\n",
      "Epoch: 8, loss = 166.7682202657064\n",
      "Epoch: 9, loss = 157.20245297749835\n",
      "(192, 5) (192, 1)\n",
      "test set size is (512, 5)\n",
      "(128, 5) (128, 1)\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 897.685188293457\n",
      "Epoch: 1, loss = 848.5712051391602\n",
      "Epoch: 2, loss = 804.253589630127\n",
      "Epoch: 3, loss = 763.6846466064453\n",
      "Epoch: 4, loss = 726.2823677062988\n",
      "Epoch: 5, loss = 691.5584869384766\n",
      "Epoch: 6, loss = 659.1344413757324\n",
      "Epoch: 7, loss = 628.6419715881348\n",
      "Epoch: 8, loss = 599.8716239929199\n",
      "Epoch: 9, loss = 572.6521377563477\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 972.3930740356445\n",
      "Epoch: 1, loss = 899.7449493408203\n",
      "Epoch: 2, loss = 835.8917846679688\n",
      "Epoch: 3, loss = 779.40576171875\n",
      "Epoch: 4, loss = 729.3008918762207\n",
      "Epoch: 5, loss = 684.5720558166504\n",
      "Epoch: 6, loss = 644.3593559265137\n",
      "Epoch: 7, loss = 607.9808044433594\n",
      "Epoch: 8, loss = 574.887020111084\n",
      "Epoch: 9, loss = 544.6257514953613\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 837.1575355529785\n",
      "Epoch: 1, loss = 793.044319152832\n",
      "Epoch: 2, loss = 752.7186813354492\n",
      "Epoch: 3, loss = 715.3726119995117\n",
      "Epoch: 4, loss = 680.4784660339355\n",
      "Epoch: 5, loss = 647.8640251159668\n",
      "Epoch: 6, loss = 617.2914085388184\n",
      "Epoch: 7, loss = 588.5320930480957\n",
      "Epoch: 8, loss = 561.4288597106934\n",
      "Epoch: 9, loss = 535.7296600341797\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1028.651138305664\n",
      "Epoch: 1, loss = 960.0750274658203\n",
      "Epoch: 2, loss = 899.5022125244141\n",
      "Epoch: 3, loss = 845.526985168457\n",
      "Epoch: 4, loss = 797.3591690063477\n",
      "Epoch: 5, loss = 754.1536026000977\n",
      "Epoch: 6, loss = 715.0536499023438\n",
      "Epoch: 7, loss = 679.363208770752\n",
      "Epoch: 8, loss = 646.6425476074219\n",
      "Epoch: 9, loss = 616.4611358642578\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1040.047607421875\n",
      "Epoch: 1, loss = 977.7626686096191\n",
      "Epoch: 2, loss = 921.2749824523926\n",
      "Epoch: 3, loss = 869.441047668457\n",
      "Epoch: 4, loss = 821.8450469970703\n",
      "Epoch: 5, loss = 778.1071014404297\n",
      "Epoch: 6, loss = 737.8054580688477\n",
      "Epoch: 7, loss = 700.5327453613281\n",
      "Epoch: 8, loss = 665.9112129211426\n",
      "Epoch: 9, loss = 633.6882514953613\n",
      "(64, 5) (64, 1)\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 523.5955073038738\n",
      "Epoch: 1, loss = 480.69245402018225\n",
      "Epoch: 2, loss = 442.8410491943359\n",
      "Epoch: 3, loss = 409.2242012023925\n",
      "Epoch: 4, loss = 379.2486050923666\n",
      "Epoch: 5, loss = 352.40780385335285\n",
      "Epoch: 6, loss = 328.36473210652673\n",
      "Epoch: 7, loss = 306.76864751180017\n",
      "Epoch: 8, loss = 287.31628672281903\n",
      "Epoch: 9, loss = 269.7465578715006\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 485.9772300720215\n",
      "Epoch: 1, loss = 438.18054453531903\n",
      "Epoch: 2, loss = 397.57596842447913\n",
      "Epoch: 3, loss = 362.9320335388184\n",
      "Epoch: 4, loss = 333.2591934204102\n",
      "Epoch: 5, loss = 307.65283330281574\n",
      "Epoch: 6, loss = 285.36166508992517\n",
      "Epoch: 7, loss = 265.8267936706543\n",
      "Epoch: 8, loss = 248.5817387898763\n",
      "Epoch: 9, loss = 233.25648816426596\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 477.90952301025385\n",
      "Epoch: 1, loss = 439.0384470621745\n",
      "Epoch: 2, loss = 404.39591217041016\n",
      "Epoch: 3, loss = 373.3180885314941\n",
      "Epoch: 4, loss = 345.4011942545573\n",
      "Epoch: 5, loss = 320.3374735514323\n",
      "Epoch: 6, loss = 297.83138656616217\n",
      "Epoch: 7, loss = 277.62195332845056\n",
      "Epoch: 8, loss = 259.42591349283856\n",
      "Epoch: 9, loss = 243.03483835856122\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 563.9875818888346\n",
      "Epoch: 1, loss = 514.6039123535156\n",
      "Epoch: 2, loss = 471.70386123657227\n",
      "Epoch: 3, loss = 434.08876037597656\n",
      "Epoch: 4, loss = 400.9496854146321\n",
      "Epoch: 5, loss = 371.6072413126628\n",
      "Epoch: 6, loss = 345.4208539326986\n",
      "Epoch: 7, loss = 322.0389480590821\n",
      "Epoch: 8, loss = 301.06909942626953\n",
      "Epoch: 9, loss = 282.2090034484863\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 557.1763814290365\n",
      "Epoch: 1, loss = 509.32733154296875\n",
      "Epoch: 2, loss = 467.69415918986\n",
      "Epoch: 3, loss = 431.1736119588216\n",
      "Epoch: 4, loss = 399.04816945393884\n",
      "Epoch: 5, loss = 370.64644368489587\n",
      "Epoch: 6, loss = 345.3590494791667\n",
      "Epoch: 7, loss = 322.6989046732585\n",
      "Epoch: 8, loss = 302.2714017232259\n",
      "Epoch: 9, loss = 283.8009211222331\n",
      "(192, 5) (192, 1)\n",
      "(64, 5) (64, 1)\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 244.26621866226196\n",
      "Epoch: 1, loss = 218.6976900100708\n",
      "Epoch: 2, loss = 197.25114488601685\n",
      "Epoch: 3, loss = 179.2173056602478\n",
      "Epoch: 4, loss = 163.9395260810852\n",
      "Epoch: 5, loss = 150.82847023010254\n",
      "Epoch: 6, loss = 139.4505729675293\n",
      "Epoch: 7, loss = 129.49822807312012\n",
      "Epoch: 8, loss = 120.71916770935059\n",
      "Epoch: 9, loss = 112.92130613327026\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 211.00022649765015\n",
      "Epoch: 1, loss = 187.92739582061768\n",
      "Epoch: 2, loss = 168.88592386245728\n",
      "Epoch: 3, loss = 153.0886631011963\n",
      "Epoch: 4, loss = 139.8309531211853\n",
      "Epoch: 5, loss = 128.54554295539856\n",
      "Epoch: 6, loss = 118.80653762817383\n",
      "Epoch: 7, loss = 110.30114960670471\n",
      "Epoch: 8, loss = 102.79249858856201\n",
      "Epoch: 9, loss = 96.10646057128906\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 215.27723217010498\n",
      "Epoch: 1, loss = 192.82425165176392\n",
      "Epoch: 2, loss = 174.03863334655762\n",
      "Epoch: 3, loss = 158.22536611557007\n",
      "Epoch: 4, loss = 144.81443071365356\n",
      "Epoch: 5, loss = 133.34079933166504\n",
      "Epoch: 6, loss = 123.42920017242432\n",
      "Epoch: 7, loss = 114.7638795375824\n",
      "Epoch: 8, loss = 107.11408734321594\n",
      "Epoch: 9, loss = 100.31917190551758\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 252.02731847763062\n",
      "Epoch: 1, loss = 225.28254747390747\n",
      "Epoch: 2, loss = 202.94077444076538\n",
      "Epoch: 3, loss = 184.1803331375122\n",
      "Epoch: 4, loss = 168.29282093048096\n",
      "Epoch: 5, loss = 154.65589570999146\n",
      "Epoch: 6, loss = 142.80084133148193\n",
      "Epoch: 7, loss = 132.38362979888916\n",
      "Epoch: 8, loss = 123.12759566307068\n",
      "Epoch: 9, loss = 114.8317244052887\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 255.1965627670288\n",
      "Epoch: 1, loss = 228.53708219528198\n",
      "Epoch: 2, loss = 206.0348653793335\n",
      "Epoch: 3, loss = 186.9341435432434\n",
      "Epoch: 4, loss = 170.63056898117065\n",
      "Epoch: 5, loss = 156.60732126235962\n",
      "Epoch: 6, loss = 144.44031238555908\n",
      "Epoch: 7, loss = 133.82263112068176\n",
      "Epoch: 8, loss = 124.4929826259613\n",
      "Epoch: 9, loss = 116.22691535949707\n",
      "(256, 5) (256, 1)\n",
      "test set size is (512, 5)\n",
      "(128, 5) (128, 1)\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 897.685188293457\n",
      "Epoch: 1, loss = 848.5712051391602\n",
      "Epoch: 2, loss = 804.253589630127\n",
      "Epoch: 3, loss = 763.6846466064453\n",
      "Epoch: 4, loss = 726.2823677062988\n",
      "Epoch: 5, loss = 691.5584869384766\n",
      "Epoch: 6, loss = 659.1344413757324\n",
      "Epoch: 7, loss = 628.6419715881348\n",
      "Epoch: 8, loss = 599.8716239929199\n",
      "Epoch: 9, loss = 572.6521377563477\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 972.3930740356445\n",
      "Epoch: 1, loss = 899.7449493408203\n",
      "Epoch: 2, loss = 835.8917846679688\n",
      "Epoch: 3, loss = 779.40576171875\n",
      "Epoch: 4, loss = 729.3008918762207\n",
      "Epoch: 5, loss = 684.5720558166504\n",
      "Epoch: 6, loss = 644.3593559265137\n",
      "Epoch: 7, loss = 607.9808044433594\n",
      "Epoch: 8, loss = 574.887020111084\n",
      "Epoch: 9, loss = 544.6257514953613\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 837.1575355529785\n",
      "Epoch: 1, loss = 793.044319152832\n",
      "Epoch: 2, loss = 752.7186813354492\n",
      "Epoch: 3, loss = 715.3726119995117\n",
      "Epoch: 4, loss = 680.4784660339355\n",
      "Epoch: 5, loss = 647.8640251159668\n",
      "Epoch: 6, loss = 617.2914085388184\n",
      "Epoch: 7, loss = 588.5320930480957\n",
      "Epoch: 8, loss = 561.4288597106934\n",
      "Epoch: 9, loss = 535.7296600341797\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1028.651138305664\n",
      "Epoch: 1, loss = 960.0750274658203\n",
      "Epoch: 2, loss = 899.5022125244141\n",
      "Epoch: 3, loss = 845.526985168457\n",
      "Epoch: 4, loss = 797.3591690063477\n",
      "Epoch: 5, loss = 754.1536026000977\n",
      "Epoch: 6, loss = 715.0536499023438\n",
      "Epoch: 7, loss = 679.363208770752\n",
      "Epoch: 8, loss = 646.6425476074219\n",
      "Epoch: 9, loss = 616.4611358642578\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1040.047607421875\n",
      "Epoch: 1, loss = 977.7626686096191\n",
      "Epoch: 2, loss = 921.2749824523926\n",
      "Epoch: 3, loss = 869.441047668457\n",
      "Epoch: 4, loss = 821.8450469970703\n",
      "Epoch: 5, loss = 778.1071014404297\n",
      "Epoch: 6, loss = 737.8054580688477\n",
      "Epoch: 7, loss = 700.5327453613281\n",
      "Epoch: 8, loss = 665.9112129211426\n",
      "Epoch: 9, loss = 633.6882514953613\n",
      "(128, 5) (128, 1)\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 527.6100368499756\n",
      "Epoch: 1, loss = 467.5196952819824\n",
      "Epoch: 2, loss = 417.80881118774414\n",
      "Epoch: 3, loss = 376.05681800842285\n",
      "Epoch: 4, loss = 340.61046028137207\n",
      "Epoch: 5, loss = 310.23933506011963\n",
      "Epoch: 6, loss = 284.0387716293335\n",
      "Epoch: 7, loss = 261.25058937072754\n",
      "Epoch: 8, loss = 241.34078311920166\n",
      "Epoch: 9, loss = 223.84137773513794\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 481.68645572662354\n",
      "Epoch: 1, loss = 419.1155843734741\n",
      "Epoch: 2, loss = 369.3452425003052\n",
      "Epoch: 3, loss = 329.24754524230957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, loss = 296.4165287017822\n",
      "Epoch: 5, loss = 269.1150665283203\n",
      "Epoch: 6, loss = 246.07673358917236\n",
      "Epoch: 7, loss = 226.38351154327393\n",
      "Epoch: 8, loss = 209.35045289993286\n",
      "Epoch: 9, loss = 194.47290229797363\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 464.06555557250977\n",
      "Epoch: 1, loss = 412.9252452850342\n",
      "Epoch: 2, loss = 369.8240079879761\n",
      "Epoch: 3, loss = 333.07129096984863\n",
      "Epoch: 4, loss = 301.5694808959961\n",
      "Epoch: 5, loss = 274.5171060562134\n",
      "Epoch: 6, loss = 251.13413763046265\n",
      "Epoch: 7, loss = 230.80370235443115\n",
      "Epoch: 8, loss = 213.07958602905273\n",
      "Epoch: 9, loss = 197.57088708877563\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 554.3867225646973\n",
      "Epoch: 1, loss = 488.3404417037964\n",
      "Epoch: 2, loss = 434.6854934692383\n",
      "Epoch: 3, loss = 390.2869920730591\n",
      "Epoch: 4, loss = 352.95217990875244\n",
      "Epoch: 5, loss = 321.1543245315552\n",
      "Epoch: 6, loss = 293.84046459198\n",
      "Epoch: 7, loss = 270.1591634750366\n",
      "Epoch: 8, loss = 249.50632047653198\n",
      "Epoch: 9, loss = 231.3562126159668\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 550.0757007598877\n",
      "Epoch: 1, loss = 488.2419424057007\n",
      "Epoch: 2, loss = 437.02897357940674\n",
      "Epoch: 3, loss = 393.92726612091064\n",
      "Epoch: 4, loss = 357.2895908355713\n",
      "Epoch: 5, loss = 325.75895977020264\n",
      "Epoch: 6, loss = 298.43240547180176\n",
      "Epoch: 7, loss = 274.58562660217285\n",
      "Epoch: 8, loss = 253.66504955291748\n",
      "Epoch: 9, loss = 235.1835412979126\n",
      "(256, 5) (256, 1)\n",
      "(128, 5) (128, 1)\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 203.39754581451416\n",
      "Epoch: 1, loss = 172.6722536087036\n",
      "Epoch: 2, loss = 149.35915279388425\n",
      "Epoch: 3, loss = 131.22304153442383\n",
      "Epoch: 4, loss = 116.68456141153972\n",
      "Epoch: 5, loss = 104.72945229212444\n",
      "Epoch: 6, loss = 94.72712182998656\n",
      "Epoch: 7, loss = 86.22283983230591\n",
      "Epoch: 8, loss = 78.89821434020999\n",
      "Epoch: 9, loss = 72.521475315094\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 175.45042737325028\n",
      "Epoch: 1, loss = 148.35059388478598\n",
      "Epoch: 2, loss = 127.92641766866049\n",
      "Epoch: 3, loss = 112.1413884162903\n",
      "Epoch: 4, loss = 99.55929597218834\n",
      "Epoch: 5, loss = 89.25341478983562\n",
      "Epoch: 6, loss = 80.6508331298828\n",
      "Epoch: 7, loss = 73.35700543721515\n",
      "Epoch: 8, loss = 67.09528573354086\n",
      "Epoch: 9, loss = 61.66512854894004\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 176.0611648559571\n",
      "Epoch: 1, loss = 149.90239238739017\n",
      "Epoch: 2, loss = 129.86327314376834\n",
      "Epoch: 3, loss = 114.18793137868242\n",
      "Epoch: 4, loss = 101.57222127914426\n",
      "Epoch: 5, loss = 91.20462385813394\n",
      "Epoch: 6, loss = 82.52742385864258\n",
      "Epoch: 7, loss = 75.16530275344849\n",
      "Epoch: 8, loss = 68.8404342333476\n",
      "Epoch: 9, loss = 63.35899424552918\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 205.21775182088217\n",
      "Epoch: 1, loss = 174.44924640655518\n",
      "Epoch: 2, loss = 150.95791562398273\n",
      "Epoch: 3, loss = 132.50831572214761\n",
      "Epoch: 4, loss = 117.58894395828248\n",
      "Epoch: 5, loss = 105.23242807388303\n",
      "Epoch: 6, loss = 94.80763196945192\n",
      "Epoch: 7, loss = 85.90216890970866\n",
      "Epoch: 8, loss = 78.20822890599568\n",
      "Epoch: 9, loss = 71.5197105407715\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 208.77262465159097\n",
      "Epoch: 1, loss = 177.74681250254312\n",
      "Epoch: 2, loss = 153.75848515828451\n",
      "Epoch: 3, loss = 134.9188954035441\n",
      "Epoch: 4, loss = 119.77812480926512\n",
      "Epoch: 5, loss = 107.34068902333577\n",
      "Epoch: 6, loss = 96.92970657348631\n",
      "Epoch: 7, loss = 88.09380880991618\n",
      "Epoch: 8, loss = 80.50417327880858\n",
      "Epoch: 9, loss = 73.90864117940268\n",
      "(384, 5) (384, 1)\n",
      "test set size is (512, 5)\n",
      "(128, 5) (128, 1)\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 897.685188293457\n",
      "Epoch: 1, loss = 848.5712051391602\n",
      "Epoch: 2, loss = 804.253589630127\n",
      "Epoch: 3, loss = 763.6846466064453\n",
      "Epoch: 4, loss = 726.2823677062988\n",
      "Epoch: 5, loss = 691.5584869384766\n",
      "Epoch: 6, loss = 659.1344413757324\n",
      "Epoch: 7, loss = 628.6419715881348\n",
      "Epoch: 8, loss = 599.8716239929199\n",
      "Epoch: 9, loss = 572.6521377563477\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 972.3930740356445\n",
      "Epoch: 1, loss = 899.7449493408203\n",
      "Epoch: 2, loss = 835.8917846679688\n",
      "Epoch: 3, loss = 779.40576171875\n",
      "Epoch: 4, loss = 729.3008918762207\n",
      "Epoch: 5, loss = 684.5720558166504\n",
      "Epoch: 6, loss = 644.3593559265137\n",
      "Epoch: 7, loss = 607.9808044433594\n",
      "Epoch: 8, loss = 574.887020111084\n",
      "Epoch: 9, loss = 544.6257514953613\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 837.1575355529785\n",
      "Epoch: 1, loss = 793.044319152832\n",
      "Epoch: 2, loss = 752.7186813354492\n",
      "Epoch: 3, loss = 715.3726119995117\n",
      "Epoch: 4, loss = 680.4784660339355\n",
      "Epoch: 5, loss = 647.8640251159668\n",
      "Epoch: 6, loss = 617.2914085388184\n",
      "Epoch: 7, loss = 588.5320930480957\n",
      "Epoch: 8, loss = 561.4288597106934\n",
      "Epoch: 9, loss = 535.7296600341797\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1028.651138305664\n",
      "Epoch: 1, loss = 960.0750274658203\n",
      "Epoch: 2, loss = 899.5022125244141\n",
      "Epoch: 3, loss = 845.526985168457\n",
      "Epoch: 4, loss = 797.3591690063477\n",
      "Epoch: 5, loss = 754.1536026000977\n",
      "Epoch: 6, loss = 715.0536499023438\n",
      "Epoch: 7, loss = 679.363208770752\n",
      "Epoch: 8, loss = 646.6425476074219\n",
      "Epoch: 9, loss = 616.4611358642578\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1040.047607421875\n",
      "Epoch: 1, loss = 977.7626686096191\n",
      "Epoch: 2, loss = 921.2749824523926\n",
      "Epoch: 3, loss = 869.441047668457\n",
      "Epoch: 4, loss = 821.8450469970703\n",
      "Epoch: 5, loss = 778.1071014404297\n",
      "Epoch: 6, loss = 737.8054580688477\n",
      "Epoch: 7, loss = 700.5327453613281\n",
      "Epoch: 8, loss = 665.9112129211426\n",
      "Epoch: 9, loss = 633.6882514953613\n",
      "(256, 5) (256, 1)\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 512.8505643208821\n",
      "Epoch: 1, loss = 426.0912488301595\n",
      "Epoch: 2, loss = 361.51868247985846\n",
      "Epoch: 3, loss = 311.5653991699219\n",
      "Epoch: 4, loss = 272.04778003692627\n",
      "Epoch: 5, loss = 240.19474569956466\n",
      "Epoch: 6, loss = 214.15790303548178\n",
      "Epoch: 7, loss = 192.58097203572592\n",
      "Epoch: 8, loss = 174.43259048461917\n",
      "Epoch: 9, loss = 158.95014635721842\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 461.33934720357263\n",
      "Epoch: 1, loss = 374.77192687988287\n",
      "Epoch: 2, loss = 313.81932131449383\n",
      "Epoch: 3, loss = 268.9714282353719\n",
      "Epoch: 4, loss = 234.66930166880292\n",
      "Epoch: 5, loss = 207.58993434906006\n",
      "Epoch: 6, loss = 185.65771516164142\n",
      "Epoch: 7, loss = 167.5359932581584\n",
      "Epoch: 8, loss = 152.29498402277628\n",
      "Epoch: 9, loss = 139.31883478164673\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 442.44400342305494\n",
      "Epoch: 1, loss = 369.7680219014486\n",
      "Epoch: 2, loss = 314.0341962178548\n",
      "Epoch: 3, loss = 270.45876693725586\n",
      "Epoch: 4, loss = 235.92248090108234\n",
      "Epoch: 5, loss = 208.19739977518717\n",
      "Epoch: 6, loss = 185.60433101654053\n",
      "Epoch: 7, loss = 166.90249061584475\n",
      "Epoch: 8, loss = 151.23345232009888\n",
      "Epoch: 9, loss = 137.93411540985107\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 525.0097700754801\n",
      "Epoch: 1, loss = 433.2583872477215\n",
      "Epoch: 2, loss = 366.7611115773519\n",
      "Epoch: 3, loss = 316.0928723017375\n",
      "Epoch: 4, loss = 276.16207345326734\n",
      "Epoch: 5, loss = 243.95579274495444\n",
      "Epoch: 6, loss = 217.5714902877808\n",
      "Epoch: 7, loss = 195.69987074534095\n",
      "Epoch: 8, loss = 177.32506624857587\n",
      "Epoch: 9, loss = 161.7178217569987\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 516.8451016743977\n",
      "Epoch: 1, loss = 433.75699933369947\n",
      "Epoch: 2, loss = 370.45542589823395\n",
      "Epoch: 3, loss = 320.60334110260015\n",
      "Epoch: 4, loss = 280.60599136352545\n",
      "Epoch: 5, loss = 248.01581732432044\n",
      "Epoch: 6, loss = 221.13267612457273\n",
      "Epoch: 7, loss = 198.6974232991537\n",
      "Epoch: 8, loss = 179.78577740987143\n",
      "Epoch: 9, loss = 163.69116703669232\n",
      "(384, 5) (384, 1)\n",
      "(256, 5) (256, 1)\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 133.38937692642213\n",
      "Epoch: 1, loss = 104.40820846557617\n",
      "Epoch: 2, loss = 85.17615346908568\n",
      "Epoch: 3, loss = 71.40293374061585\n",
      "Epoch: 4, loss = 61.00279235839842\n",
      "Epoch: 5, loss = 52.89501399993897\n",
      "Epoch: 6, loss = 46.41064271926881\n",
      "Epoch: 7, loss = 41.125206518173215\n",
      "Epoch: 8, loss = 36.759512662887566\n",
      "Epoch: 9, loss = 33.119035100936884\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 117.05248661041261\n",
      "Epoch: 1, loss = 91.30940427780153\n",
      "Epoch: 2, loss = 74.25015926361084\n",
      "Epoch: 3, loss = 62.09138898849487\n",
      "Epoch: 4, loss = 52.97184987068179\n",
      "Epoch: 5, loss = 45.88890948295594\n",
      "Epoch: 6, loss = 40.24987583160401\n",
      "Epoch: 7, loss = 35.68548166751862\n",
      "Epoch: 8, loss = 31.940364456176756\n",
      "Epoch: 9, loss = 28.835967016220096\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 113.9841932296753\n",
      "Epoch: 1, loss = 89.91102647781375\n",
      "Epoch: 2, loss = 73.57342228889463\n",
      "Epoch: 3, loss = 61.7851845741272\n",
      "Epoch: 4, loss = 52.8627586364746\n",
      "Epoch: 5, loss = 45.833678674697886\n",
      "Epoch: 6, loss = 40.155418181419385\n",
      "Epoch: 7, loss = 35.523033452034\n",
      "Epoch: 8, loss = 31.72478060722351\n",
      "Epoch: 9, loss = 28.59433360099792\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 134.03362770080565\n",
      "Epoch: 1, loss = 105.29952001571657\n",
      "Epoch: 2, loss = 85.80814666748047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, loss = 71.71411910057068\n",
      "Epoch: 4, loss = 61.02555522918701\n",
      "Epoch: 5, loss = 52.665421628952025\n",
      "Epoch: 6, loss = 45.987871742248544\n",
      "Epoch: 7, loss = 40.5767789363861\n",
      "Epoch: 8, loss = 36.1354585170746\n",
      "Epoch: 9, loss = 32.456752920150755\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 137.55263023376466\n",
      "Epoch: 1, loss = 108.00955133438111\n",
      "Epoch: 2, loss = 88.1063849925995\n",
      "Epoch: 3, loss = 73.80295867919922\n",
      "Epoch: 4, loss = 63.0000300884247\n",
      "Epoch: 5, loss = 54.55920925140382\n",
      "Epoch: 6, loss = 47.799367094039916\n",
      "Epoch: 7, loss = 42.282599902153024\n",
      "Epoch: 8, loss = 37.73187654018402\n",
      "Epoch: 9, loss = 33.94451801776886\n",
      "(640, 5) (640, 1)\n",
      "test set size is (512, 5)\n",
      "(128, 5) (128, 1)\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 897.685188293457\n",
      "Epoch: 1, loss = 848.5712051391602\n",
      "Epoch: 2, loss = 804.253589630127\n",
      "Epoch: 3, loss = 763.6846466064453\n",
      "Epoch: 4, loss = 726.2823677062988\n",
      "Epoch: 5, loss = 691.5584869384766\n",
      "Epoch: 6, loss = 659.1344413757324\n",
      "Epoch: 7, loss = 628.6419715881348\n",
      "Epoch: 8, loss = 599.8716239929199\n",
      "Epoch: 9, loss = 572.6521377563477\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 972.3930740356445\n",
      "Epoch: 1, loss = 899.7449493408203\n",
      "Epoch: 2, loss = 835.8917846679688\n",
      "Epoch: 3, loss = 779.40576171875\n",
      "Epoch: 4, loss = 729.3008918762207\n",
      "Epoch: 5, loss = 684.5720558166504\n",
      "Epoch: 6, loss = 644.3593559265137\n",
      "Epoch: 7, loss = 607.9808044433594\n",
      "Epoch: 8, loss = 574.887020111084\n",
      "Epoch: 9, loss = 544.6257514953613\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 837.1575355529785\n",
      "Epoch: 1, loss = 793.044319152832\n",
      "Epoch: 2, loss = 752.7186813354492\n",
      "Epoch: 3, loss = 715.3726119995117\n",
      "Epoch: 4, loss = 680.4784660339355\n",
      "Epoch: 5, loss = 647.8640251159668\n",
      "Epoch: 6, loss = 617.2914085388184\n",
      "Epoch: 7, loss = 588.5320930480957\n",
      "Epoch: 8, loss = 561.4288597106934\n",
      "Epoch: 9, loss = 535.7296600341797\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1028.651138305664\n",
      "Epoch: 1, loss = 960.0750274658203\n",
      "Epoch: 2, loss = 899.5022125244141\n",
      "Epoch: 3, loss = 845.526985168457\n",
      "Epoch: 4, loss = 797.3591690063477\n",
      "Epoch: 5, loss = 754.1536026000977\n",
      "Epoch: 6, loss = 715.0536499023438\n",
      "Epoch: 7, loss = 679.363208770752\n",
      "Epoch: 8, loss = 646.6425476074219\n",
      "Epoch: 9, loss = 616.4611358642578\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1040.047607421875\n",
      "Epoch: 1, loss = 977.7626686096191\n",
      "Epoch: 2, loss = 921.2749824523926\n",
      "Epoch: 3, loss = 869.441047668457\n",
      "Epoch: 4, loss = 821.8450469970703\n",
      "Epoch: 5, loss = 778.1071014404297\n",
      "Epoch: 6, loss = 737.8054580688477\n",
      "Epoch: 7, loss = 700.5327453613281\n",
      "Epoch: 8, loss = 665.9112129211426\n",
      "Epoch: 9, loss = 633.6882514953613\n",
      "(512, 5) (512, 1)\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 452.4648475646974\n",
      "Epoch: 1, loss = 335.6403858184813\n",
      "Epoch: 2, loss = 262.7965778350831\n",
      "Epoch: 3, loss = 213.124405670166\n",
      "Epoch: 4, loss = 177.45926456451414\n",
      "Epoch: 5, loss = 150.7548957824707\n",
      "Epoch: 6, loss = 129.966442489624\n",
      "Epoch: 7, loss = 113.38329486846922\n",
      "Epoch: 8, loss = 99.87506313323976\n",
      "Epoch: 9, loss = 88.7118567466736\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 404.05283966064457\n",
      "Epoch: 1, loss = 292.78876457214346\n",
      "Epoch: 2, loss = 227.85996665954593\n",
      "Epoch: 3, loss = 185.21245746612553\n",
      "Epoch: 4, loss = 154.98756256103516\n",
      "Epoch: 5, loss = 132.4428325653076\n",
      "Epoch: 6, loss = 114.96575107574465\n",
      "Epoch: 7, loss = 101.00226831436154\n",
      "Epoch: 8, loss = 89.58693304061887\n",
      "Epoch: 9, loss = 80.09468784332275\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 383.63695259094237\n",
      "Epoch: 1, loss = 287.7136737823487\n",
      "Epoch: 2, loss = 225.54536094665522\n",
      "Epoch: 3, loss = 182.85748023986818\n",
      "Epoch: 4, loss = 152.27891273498543\n",
      "Epoch: 5, loss = 129.5482858657837\n",
      "Epoch: 6, loss = 112.06305170059203\n",
      "Epoch: 7, loss = 98.25261650085451\n",
      "Epoch: 8, loss = 87.07008666992188\n",
      "Epoch: 9, loss = 77.83012256622315\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 458.69879150390625\n",
      "Epoch: 1, loss = 338.7482982635498\n",
      "Epoch: 2, loss = 265.5253311157226\n",
      "Epoch: 3, loss = 215.6202362060547\n",
      "Epoch: 4, loss = 179.58754024505612\n",
      "Epoch: 5, loss = 152.58568134307865\n",
      "Epoch: 6, loss = 131.66377143859862\n",
      "Epoch: 7, loss = 114.90097293853759\n",
      "Epoch: 8, loss = 101.07746496200562\n",
      "Epoch: 9, loss = 89.44076566696165\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 454.3174507141114\n",
      "Epoch: 1, loss = 343.3571929931642\n",
      "Epoch: 2, loss = 270.4439037322998\n",
      "Epoch: 3, loss = 219.5957120895386\n",
      "Epoch: 4, loss = 182.66391162872318\n",
      "Epoch: 5, loss = 154.8941051483154\n",
      "Epoch: 6, loss = 133.45195140838624\n",
      "Epoch: 7, loss = 116.48480749130249\n",
      "Epoch: 8, loss = 102.7406057357788\n",
      "Epoch: 9, loss = 91.34515724182131\n",
      "(640, 5) (640, 1)\n",
      "(512, 5) (512, 1)\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 71.54870221349925\n",
      "Epoch: 1, loss = 49.17244007852344\n",
      "Epoch: 2, loss = 36.97855626212227\n",
      "Epoch: 3, loss = 29.339286102188957\n",
      "Epoch: 4, loss = 24.232541322708137\n",
      "Epoch: 5, loss = 20.68429734971789\n",
      "Epoch: 6, loss = 18.15387633111742\n",
      "Epoch: 7, loss = 16.311692184872093\n",
      "Epoch: 8, loss = 14.944601919915938\n",
      "Epoch: 9, loss = 13.90612961186303\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 64.85873503155179\n",
      "Epoch: 1, loss = 44.3138001230028\n",
      "Epoch: 2, loss = 33.26135275099011\n",
      "Epoch: 3, loss = 26.439901696311104\n",
      "Epoch: 4, loss = 21.946460180812405\n",
      "Epoch: 5, loss = 18.87215868631999\n",
      "Epoch: 6, loss = 16.70988460381825\n",
      "Epoch: 7, loss = 15.152027236090767\n",
      "Epoch: 8, loss = 13.99976836310493\n",
      "Epoch: 9, loss = 13.122413966390813\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 63.66813084814283\n",
      "Epoch: 1, loss = 44.14810747570461\n",
      "Epoch: 2, loss = 33.30193616284265\n",
      "Epoch: 3, loss = 26.504902773433273\n",
      "Epoch: 4, loss = 21.999639312426257\n",
      "Epoch: 5, loss = 18.91578233242035\n",
      "Epoch: 6, loss = 16.76282768779331\n",
      "Epoch: 7, loss = 15.225374062856044\n",
      "Epoch: 8, loss = 14.099008030361597\n",
      "Epoch: 9, loss = 13.247221761279635\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 71.41616063647798\n",
      "Epoch: 1, loss = 49.14929866790772\n",
      "Epoch: 2, loss = 36.87429131401909\n",
      "Epoch: 3, loss = 29.221107681592304\n",
      "Epoch: 4, loss = 24.15229864915211\n",
      "Epoch: 5, loss = 20.67427836524116\n",
      "Epoch: 6, loss = 18.225362777709957\n",
      "Epoch: 7, loss = 16.46100244257185\n",
      "Epoch: 8, loss = 15.159090121587116\n",
      "Epoch: 9, loss = 14.170619103643634\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 74.44845989015364\n",
      "Epoch: 1, loss = 51.42219101058112\n",
      "Epoch: 2, loss = 38.66370627615189\n",
      "Epoch: 3, loss = 30.65896533595191\n",
      "Epoch: 4, loss = 25.298048668437538\n",
      "Epoch: 5, loss = 21.56916720337338\n",
      "Epoch: 6, loss = 18.91542796293895\n",
      "Epoch: 7, loss = 16.988972187042236\n",
      "Epoch: 8, loss = 15.561517741945055\n",
      "Epoch: 9, loss = 14.478812098503107\n",
      "(1152, 5) (1152, 1)\n",
      "test set size is (512, 5)\n",
      "(128, 5) (128, 1)\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 897.685188293457\n",
      "Epoch: 1, loss = 848.5712051391602\n",
      "Epoch: 2, loss = 804.253589630127\n",
      "Epoch: 3, loss = 763.6846466064453\n",
      "Epoch: 4, loss = 726.2823677062988\n",
      "Epoch: 5, loss = 691.5584869384766\n",
      "Epoch: 6, loss = 659.1344413757324\n",
      "Epoch: 7, loss = 628.6419715881348\n",
      "Epoch: 8, loss = 599.8716239929199\n",
      "Epoch: 9, loss = 572.6521377563477\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 972.3930740356445\n",
      "Epoch: 1, loss = 899.7449493408203\n",
      "Epoch: 2, loss = 835.8917846679688\n",
      "Epoch: 3, loss = 779.40576171875\n",
      "Epoch: 4, loss = 729.3008918762207\n",
      "Epoch: 5, loss = 684.5720558166504\n",
      "Epoch: 6, loss = 644.3593559265137\n",
      "Epoch: 7, loss = 607.9808044433594\n",
      "Epoch: 8, loss = 574.887020111084\n",
      "Epoch: 9, loss = 544.6257514953613\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 837.1575355529785\n",
      "Epoch: 1, loss = 793.044319152832\n",
      "Epoch: 2, loss = 752.7186813354492\n",
      "Epoch: 3, loss = 715.3726119995117\n",
      "Epoch: 4, loss = 680.4784660339355\n",
      "Epoch: 5, loss = 647.8640251159668\n",
      "Epoch: 6, loss = 617.2914085388184\n",
      "Epoch: 7, loss = 588.5320930480957\n",
      "Epoch: 8, loss = 561.4288597106934\n",
      "Epoch: 9, loss = 535.7296600341797\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 1028.651138305664\n",
      "Epoch: 1, loss = 960.0750274658203\n",
      "Epoch: 2, loss = 899.5022125244141\n",
      "Epoch: 3, loss = 845.526985168457\n",
      "Epoch: 4, loss = 797.3591690063477\n",
      "Epoch: 5, loss = 754.1536026000977\n",
      "Epoch: 6, loss = 715.0536499023438\n",
      "Epoch: 7, loss = 679.363208770752\n",
      "Epoch: 8, loss = 646.6425476074219\n",
      "Epoch: 9, loss = 616.4611358642578\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 1040.047607421875\n",
      "Epoch: 1, loss = 977.7626686096191\n",
      "Epoch: 2, loss = 921.2749824523926\n",
      "Epoch: 3, loss = 869.441047668457\n",
      "Epoch: 4, loss = 821.8450469970703\n",
      "Epoch: 5, loss = 778.1071014404297\n",
      "Epoch: 6, loss = 737.8054580688477\n",
      "Epoch: 7, loss = 700.5327453613281\n",
      "Epoch: 8, loss = 665.9112129211426\n",
      "Epoch: 9, loss = 633.6882514953613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 5) (1024, 1)\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 394.07933680216473\n",
      "Epoch: 1, loss = 243.64848592546252\n",
      "Epoch: 2, loss = 170.8536591000027\n",
      "Epoch: 3, loss = 128.40655220879452\n",
      "Epoch: 4, loss = 100.71756288740369\n",
      "Epoch: 5, loss = 81.28125492731729\n",
      "Epoch: 6, loss = 67.07351803779602\n",
      "Epoch: 7, loss = 56.381545199288276\n",
      "Epoch: 8, loss = 48.15647406048244\n",
      "Epoch: 9, loss = 41.711830986870645\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 350.83792071872296\n",
      "Epoch: 1, loss = 212.97111405266656\n",
      "Epoch: 2, loss = 150.11392688751226\n",
      "Epoch: 3, loss = 113.84812302059593\n",
      "Epoch: 4, loss = 90.21601284874808\n",
      "Epoch: 5, loss = 73.61814716127184\n",
      "Epoch: 6, loss = 61.38816661304895\n",
      "Epoch: 7, loss = 52.079265435536705\n",
      "Epoch: 8, loss = 44.846155802408845\n",
      "Epoch: 9, loss = 39.13890020052593\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 339.02530860900885\n",
      "Epoch: 1, loss = 211.39515442318378\n",
      "Epoch: 2, loss = 147.55048338572183\n",
      "Epoch: 3, loss = 110.91345771153769\n",
      "Epoch: 4, loss = 87.51840930514867\n",
      "Epoch: 5, loss = 71.34928046332463\n",
      "Epoch: 6, loss = 59.54790388213265\n",
      "Epoch: 7, loss = 50.626548714107926\n",
      "Epoch: 8, loss = 43.71325159072877\n",
      "Epoch: 9, loss = 38.25880114237467\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 401.7247390747069\n",
      "Epoch: 1, loss = 248.2383367750379\n",
      "Epoch: 2, loss = 173.01000595092776\n",
      "Epoch: 3, loss = 129.33053186204694\n",
      "Epoch: 4, loss = 100.90853823555841\n",
      "Epoch: 5, loss = 80.73259268866641\n",
      "Epoch: 6, loss = 65.8790773815579\n",
      "Epoch: 7, loss = 54.8042565981547\n",
      "Epoch: 8, loss = 46.425162129932\n",
      "Epoch: 9, loss = 40.00518088870577\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 391.55272822909876\n",
      "Epoch: 1, loss = 247.09403525458453\n",
      "Epoch: 2, loss = 173.11857022179495\n",
      "Epoch: 3, loss = 129.84023899502222\n",
      "Epoch: 4, loss = 102.00186634063719\n",
      "Epoch: 5, loss = 82.72476122114395\n",
      "Epoch: 6, loss = 68.63155312008328\n",
      "Epoch: 7, loss = 57.86191815800135\n",
      "Epoch: 8, loss = 49.27696651882596\n",
      "Epoch: 9, loss = 42.30798024601408\n",
      "(1152, 5) (1152, 1)\n",
      "(1024, 5) (1024, 1)\n",
      "\n",
      "Training model 0\n",
      "\n",
      "Epoch: 0, loss = 29.775435482754425\n",
      "Epoch: 1, loss = 18.234380743082838\n",
      "Epoch: 2, loss = 14.057410177062549\n",
      "Epoch: 3, loss = 12.059873584438773\n",
      "Epoch: 4, loss = 10.874445438385006\n",
      "Epoch: 5, loss = 10.047379160628592\n",
      "Epoch: 6, loss = 9.408555258722865\n",
      "Epoch: 7, loss = 8.886010320747602\n",
      "Epoch: 8, loss = 8.444208572892576\n",
      "Epoch: 9, loss = 8.062634678447944\n",
      "\n",
      "Training model 1\n",
      "\n",
      "Epoch: 0, loss = 28.29808650998508\n",
      "Epoch: 1, loss = 17.359541149700394\n",
      "Epoch: 2, loss = 13.58137198055492\n",
      "Epoch: 3, loss = 11.774057346231794\n",
      "Epoch: 4, loss = 10.659872226855336\n",
      "Epoch: 5, loss = 9.849597548737247\n",
      "Epoch: 6, loss = 9.209547428523795\n",
      "Epoch: 7, loss = 8.683338992735916\n",
      "Epoch: 8, loss = 8.24013838697882\n",
      "Epoch: 9, loss = 7.860623699777268\n",
      "\n",
      "Training model 2\n",
      "\n",
      "Epoch: 0, loss = 28.144762144369242\n",
      "Epoch: 1, loss = 17.614554317558518\n",
      "Epoch: 2, loss = 13.813526114996737\n",
      "Epoch: 3, loss = 11.985142532516932\n",
      "Epoch: 4, loss = 10.861335887628451\n",
      "Epoch: 5, loss = 10.044639149132896\n",
      "Epoch: 6, loss = 9.397561529103449\n",
      "Epoch: 7, loss = 8.861883254612193\n",
      "Epoch: 8, loss = 8.408131781746363\n",
      "Epoch: 9, loss = 8.017630307113423\n",
      "\n",
      "Training model 3\n",
      "\n",
      "Epoch: 0, loss = 29.101690699072446\n",
      "Epoch: 1, loss = 18.181716322898875\n",
      "Epoch: 2, loss = 14.260050931397606\n",
      "Epoch: 3, loss = 12.359084146864271\n",
      "Epoch: 4, loss = 11.179025716641368\n",
      "Epoch: 5, loss = 10.317799589213202\n",
      "Epoch: 6, loss = 9.635487005991092\n",
      "Epoch: 7, loss = 9.071941985803493\n",
      "Epoch: 8, loss = 8.59521257176119\n",
      "Epoch: 9, loss = 8.185855283456686\n",
      "\n",
      "Training model 4\n",
      "\n",
      "Epoch: 0, loss = 31.1102247097913\n",
      "Epoch: 1, loss = 18.959200245492607\n",
      "Epoch: 2, loss = 14.601863117778997\n",
      "Epoch: 3, loss = 12.534066890968994\n",
      "Epoch: 4, loss = 11.296249543919286\n",
      "Epoch: 5, loss = 10.41316485404968\n",
      "Epoch: 6, loss = 9.718178770121403\n",
      "Epoch: 7, loss = 9.142880913089304\n",
      "Epoch: 8, loss = 8.654043565778172\n",
      "Epoch: 9, loss = 8.232920271508837\n",
      "(2176, 5) (2176, 1)\n",
      "test set size is (512, 5)\n"
     ]
    }
   ],
   "source": [
    "errors_al = with_al(config_AL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "83b3a388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9321582132013233,\n",
       " 0.9262153489320486,\n",
       " 0.9141693532647568,\n",
       " 0.888057522228071,\n",
       " 0.831039806308556,\n",
       " 0.7336310506056534,\n",
       " 0.6033914633661223,\n",
       " 0.5604999867488258]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors_al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "749a86cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_err_plot(frac_err_baseline,frac_err_AL_ens):\n",
    "    \n",
    "    K_train_list = [8,16, 32, 64, 128, 256,512,1024]  # make it global\n",
    "    #K_train_list = [10,20,40,80,160,320,640,1280]\n",
    "    ninit = 128   #128\n",
    "    T=2     \n",
    "    N_train_list = ninit + T * np.array(K_train_list)\n",
    "    plt.style.use(\"classic\")\n",
    "    fig = plt.figure()\n",
    "    plt.plot(N_train_list,frac_err_baseline, \".-\", label=\"Baseline\")\n",
    "    plt.plot(N_train_list,frac_err_AL_ens, \".-\", label=\"Active learning, ensemble\")\n",
    "    plt.ylabel('Fractional error on test set')\n",
    "    plt.xlabel('Number of training points')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e2ba49c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "compare_err_plot(errors_baseline,errors_al)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3a65bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
